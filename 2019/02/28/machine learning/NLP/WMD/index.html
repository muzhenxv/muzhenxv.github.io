<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang="en">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">


























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=7.0.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.0.1">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.0.1">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.0.1">


  <link rel="mask-icon" href="/images/logo.svg?v=7.0.1" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '7.0.1',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false,"dimmer":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta name="description" content="文章出处本文为WMD_tutorial的翻译。 使用W2V和WMD发现文档的相似性WMD（Word Mover’s Distance）是机器学习中一个有前途的新工具，它允许我们提交查询并返回最相似的文档。例如，在博客OpenTable中，使用了WMD分析了餐厅评论。通过使用这种方法，他们能够从评论中挖掘出不同的方面。这本教程的第二部分，我们展示了如何使用gensim的WmdSimilarity去">
<meta name="keywords" content="machine learning,NLP,WMD">
<meta property="og:type" content="article">
<meta property="og:title" content="WMD">
<meta property="og:url" content="http://www.muzhen.tk/2019/02/28/machine learning/NLP/WMD/index.html">
<meta property="og:site_name" content="the Home of MuZhen">
<meta property="og:description" content="文章出处本文为WMD_tutorial的翻译。 使用W2V和WMD发现文档的相似性WMD（Word Mover’s Distance）是机器学习中一个有前途的新工具，它允许我们提交查询并返回最相似的文档。例如，在博客OpenTable中，使用了WMD分析了餐厅评论。通过使用这种方法，他们能够从评论中挖掘出不同的方面。这本教程的第二部分，我们展示了如何使用gensim的WmdSimilarity去">
<meta property="og:locale" content="en">
<meta property="og:image" content="http://i1.piimg.com/567571/651dd1b6e0d46b4b.png">
<meta property="og:image" content="http://i2.muimg.com/567571/479272b7085337df.png">
<meta property="og:updated_time" content="2019-02-28T12:45:06.537Z">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="WMD">
<meta name="twitter:description" content="文章出处本文为WMD_tutorial的翻译。 使用W2V和WMD发现文档的相似性WMD（Word Mover’s Distance）是机器学习中一个有前途的新工具，它允许我们提交查询并返回最相似的文档。例如，在博客OpenTable中，使用了WMD分析了餐厅评论。通过使用这种方法，他们能够从评论中挖掘出不同的方面。这本教程的第二部分，我们展示了如何使用gensim的WmdSimilarity去">
<meta name="twitter:image" content="http://i1.piimg.com/567571/651dd1b6e0d46b4b.png">






  <link rel="canonical" href="http://www.muzhen.tk/2019/02/28/machine learning/NLP/WMD/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>WMD | the Home of MuZhen</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left page-post-detail">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">the Home of MuZhen</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            

  <div id="posts" class="posts-expand">
    

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2019/02/28/machine learning/NLP/WMD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">WMD

              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-02-28 20:45:06" itemprop="dateCreated datePublished" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        <p><script type"text javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p>
<h1 id="文章出处"><a href="#文章出处" class="headerlink" title="文章出处"></a>文章出处</h1><p>本文为<a href="https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/WMD_tutorial.ipynb" target="_blank" rel="noopener">WMD_tutorial</a>的翻译。</p>
<h1 id="使用W2V和WMD发现文档的相似性"><a href="#使用W2V和WMD发现文档的相似性" class="headerlink" title="使用W2V和WMD发现文档的相似性"></a>使用W2V和WMD发现文档的相似性</h1><p>WMD（Word Mover’s Distance）是机器学习中一个有前途的新工具，它允许我们提交查询并返回最相似的文档。例如，在博客<a href="http://tech.opentable.com/2015/08/11/navigating-themes-in-restaurant-reviews-with-word-movers-distance/" target="_blank" rel="noopener">OpenTable</a>中，使用了WMD分析了餐厅评论。通过使用这种方法，他们能够从评论中挖掘出不同的方面。这本教程的第二部分，我们展示了如何使用gensim的WmdSimilarity去做类似与opentable所做的事情。在第一部分，我们说明了如何使用wmdistance去计算两个文档的WMD距离。如果你的目的是想使用WmdSimilarity，第一部分可以选读，但它是有意义的。</p>
<p>不管怎样，首先，我们浏览下关于wmd是什么的基础知识。</p>
<h1 id="WMD基础"><a href="#WMD基础" class="headerlink" title="WMD基础"></a>WMD基础</h1><p>wmd允许我们用一种有意义的方式去评估两个文档之间的距离，即使他们之间没有共同词汇。他使用w2v进行词向量嵌入。它的表现优于很多最先进的k近邻分类方法。</p>
<p>wmd被下面两个非常相似的句子所阐明（图例来源于<a href="http://vene.ro/blog/word-movers-distance-in-python.html" target="_blank" rel="noopener">Vlad Niculae’s blog</a>）。两个句子之间没有共同词汇，但是通过匹配相关词，wmd能够精确度量两个句子之间的相似性。这个方法也使用了文档的词袋表征（简单的说，文档的词频），在下面的图中被标记为d。该方法的直观理解是我们发现文档之间最小的”traveling distance”，换句话说将文档1的分布移向文档2的最有效方式。</p>
<p><img src="http://i1.piimg.com/567571/651dd1b6e0d46b4b.png" alt></p>
<p>这个方法已经在文章<a href="http://jmlr.org/proceedings/papers/v37/kusnerb15.pdf" target="_blank" rel="noopener">“From Word Embeddings To Document Distances” by Matt Kusner et al.</a>中被介绍。它被”Earth Mover’s Distance”激发灵感，并且利用了 “transportation problem”的一个解法。</p>
<p>在本教程中，我们将学习如何使用gensim的wmd函数。它由进行距离计算的wmdistance方法和基于相似查询的corpus计算的WmdSimilarity类构成。</p>
<blockquote>
<p>注意：<br>  如果你使用这个软件，请留意引用[1]，[2]和[3]。</p>
</blockquote>
<h1 id="运行notebook"><a href="#运行notebook" class="headerlink" title="运行notebook"></a>运行notebook</h1><p>你可以下载这个<a href="http://ipython.org/notebook.html" target="_blank" rel="noopener">iPython Notebook</a>，并在你自己电脑上运行它，如果你已经安装了gensim，pyemd，nltk，并下载了必要的数据。</p>
<p>这个notebook被运行在i7-4770cpu 3.40GHz (8 cores) 和 32 GB memory的ubuntu机器上。在这台机器上运行整个notebook需要话费3分钟。</p>
<h1 id="第一部分：计算词移距离（Word-Mover’s-Distance）"><a href="#第一部分：计算词移距离（Word-Mover’s-Distance）" class="headerlink" title="第一部分：计算词移距离（Word Mover’s Distance）"></a>第一部分：计算词移距离（Word Mover’s Distance）</h1><p>为了使用wmd，我们首先需要一些词嵌入。你需要在一些corpus上训练一个w2v模型（<a href="https://rare-technologies.com/word2vec-tutorial/" target="_blank" rel="noopener">教程</a>），但是我们将通过下载一些预训练的w2v嵌入模型来开始。下载 <a href="https://code.google.com/archive/p/word2vec/" target="_blank" rel="noopener">GoogleNews-vectors-negative300.bin.gz</a>（警告：1.5GB，第二部分并不需要这些文件）。训练你自己的嵌入模型是有益处的，但为了简化本教程，我们将首先使用预训练的嵌入。</p>
<p>让我们拿一些句子来计算它们之间的相似度。</p>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">from</span> <span class="built_in">time</span> import <span class="built_in">time</span></span><br><span class="line">start_nb = <span class="built_in">time</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize logging.</span></span><br><span class="line">import logging</span><br><span class="line">logging.basicConfig(<span class="built_in">format</span>=<span class="string">'%(asctime)s : %(levelname)s : %(message)s'</span>)</span><br><span class="line"></span><br><span class="line">sentence_obama = <span class="string">'Obama speaks to the media in Illinois'</span></span><br><span class="line">sentence_president = <span class="string">'The president greets the press in Chicago'</span></span><br><span class="line">sentence_obama = sentence_obama.<span class="built_in">lower</span>().<span class="built_in">split</span>()</span><br><span class="line">sentence_president = sentence_president.<span class="built_in">lower</span>().<span class="built_in">split</span>()</span><br></pre></td></tr></table></figure>
<p>这些句子有很相似的内容，因此wmd应该低。在我们计算wmd之前，我们想要移除停用词（”the”, “to”, etc.），因为它们对句子信息并没有很多贡献。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import and download stopwords from NLTK.</span></span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"><span class="keyword">from</span> nltk <span class="keyword">import</span> download</span><br><span class="line">download(<span class="string">'stopwords'</span>)  <span class="comment"># Download stopwords list.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Remove stopwords.</span></span><br><span class="line">stop_words = stopwords.words(<span class="string">'english'</span>)</span><br><span class="line">sentence_obama = [w <span class="keyword">for</span> w <span class="keyword">in</span> sentence_obama <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> stop_words]</span><br><span class="line">sentence_president = [w <span class="keyword">for</span> w <span class="keyword">in</span> sentence_president <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> stop_words]</span><br></pre></td></tr></table></figure>
<p>现在，正如先前提到的，我们将使用一些下载的预训练嵌入。我们加载这些进入gensim的w2v模型类中。注意我们这里选择的嵌入需要很多内存。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">start</span> = <span class="built_in">time</span>()</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> KeyedVectors</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">'/data/w2v_googlenews/GoogleNews-vectors-negative300.bin.gz'</span>):</span><br><span class="line">    <span class="keyword">raise</span> ValueError(<span class="string">"SKIP: You need to download the google news model"</span>)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">model</span> = KeyedVectors.load_word2vec_format(<span class="string">'/data/w2v_googlenews/GoogleNews-vectors-negative300.bin.gz'</span>, <span class="built_in">binary</span>=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Cell took %.2f seconds to run.'</span> % (<span class="built_in">time</span>() - <span class="keyword">start</span>))</span><br></pre></td></tr></table></figure>
<p>因此让我们使用wmdistance方法计算wmd。</p>
<figure class="highlight swift"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">distance</span> = model.wmdistance(sentence_obama, sentence_president)</span><br><span class="line"><span class="built_in">print</span> '<span class="built_in">distance</span> = %.4f' % <span class="built_in">distance</span></span><br></pre></td></tr></table></figure>
<p>让我们对完全不相关的句子做相同的操作。注意距离变大了。</p>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sentence_orange = <span class="string">'Oranges are my favorite fruit'</span></span><br><span class="line">sentence_orange = sentence_orange.<span class="built_in">lower</span>().<span class="built_in">split</span>()</span><br><span class="line">sentence_orange = [w <span class="keyword">for</span> w <span class="keyword">in</span> sentence_orange <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> stop_words]</span><br><span class="line"></span><br><span class="line">distance = model.wmdistance(sentence_obama, sentence_orange)</span><br><span class="line">print <span class="string">'distance = %.4f'</span> % distance</span><br></pre></td></tr></table></figure>
<h1 id="正则化w2v向量"><a href="#正则化w2v向量" class="headerlink" title="正则化w2v向量"></a>正则化w2v向量</h1><p>当使用wmdistance方法时，首先正则化w2v向量是有益的，因此它们都有相同的长度。为了实现正则化，简单的调用model.init_sims(replace=True)，gensim会为你实现它。</p>
<p>通常，使用<a href="https://en.wikipedia.org/wiki/Cosine_similarity" target="_blank" rel="noopener">余弦距离</a>度量两个w2v向量之间的距离。余弦距离度量的是两个向量之间的夹角。另一方面，wmd使用欧式距离。两个向量之间的欧式距离可能会因为她们之间的长度区别而变得很大，但是因为它们之间的夹角很小因此余弦距离很小。我们可以通过正则化向量减轻这个问题。</p>
<p>注意正则化向量会花费一些时间，特别是你有一个大的词汇表 和/或 大的向量集。</p>
<p>用法在下面的例子中被阐明。碰巧我们下载的向量已经被正则化了。因此，在这个例子中我们不会作出什么差别。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Normalizing word2vec vectors.</span></span><br><span class="line"><span class="keyword">start</span> = <span class="built_in">time</span>()</span><br><span class="line"></span><br><span class="line">model.init_sims(<span class="keyword">replace</span>=<span class="literal">True</span>)  <span class="comment"># Normalizes the vectors in the word2vec class.</span></span><br><span class="line"></span><br><span class="line">distance = model.wmdistance(sentence_obama, sentence_president)  <span class="comment"># Compute WMD as normal.</span></span><br><span class="line"></span><br><span class="line">print <span class="string">'Cell took %.2f seconds to run.'</span> %(<span class="built_in">time</span>() - <span class="keyword">start</span>)</span><br></pre></td></tr></table></figure>
<h1 id="第二部分：使用WmdSimilarity进行相似度查询"><a href="#第二部分：使用WmdSimilarity进行相似度查询" class="headerlink" title="第二部分：使用WmdSimilarity进行相似度查询"></a>第二部分：使用WmdSimilarity进行相似度查询</h1><p>你可以通过WmdSimilarity类，使用wmd得到一个查询对应的最相似的文档。它的交互过程相似于在gensim教程<a href="https://radimrehurek.com/gensim/tut3.html" target="_blank" rel="noopener">Similarity Queries</a>中所描绘的。</p>
<blockquote>
<p>重要注意<br> wmd是一种距离度量。WmdSimilarity中的相似度是简单的负距离。注意不要混淆距离和相似度。两个相似文档将有一个高相似得分和一个小距离;两个差异文档将有低的相似得分和一个大距离。</p>
</blockquote>
<h2 id="Yelp-data"><a href="#Yelp-data" class="headerlink" title="Yelp data"></a>Yelp data</h2><p>让我们使用一些真实世界数据来尝试下相似度查询。为此我们使用<a href="https://www.yelp.com/dataset_challenge" target="_blank" rel="noopener">yelp评论</a>。特别的，我们将使用单一餐馆也就是<a href="https://en.yelp.be/biz/mon-ami-gabi-las-vegas-2" target="_blank" rel="noopener">Mon Ami Gabi</a>的评论。</p>
<p>为了得到yelp数据，你需要使用名字和邮箱进行注册。数据是775MB。</p>
<p>这一次，我们将用我们自己的数据去训练W2V嵌入。一个餐馆的数据并不足以合适的训练出w2v，因此我们使用了6个餐馆的数据进行训练。但是仅仅在他们中的一个进行相似度查询试验。除了上面提到的Mon Ami Gabi，我们还将使用：</p>
<ul>
<li>Earl of Sandwich.</li>
<li>Wicked Spoon.</li>
<li>Serendipity 3.</li>
<li>Bacchanal Buffet.</li>
<li>The Buffet.</li>
</ul>
<p>我们选择的餐馆是yelp数据集中那些具有最高数量评论的餐馆。顺带一提，它们都在Las Vegas Boulevard中。我们用于训练w2v的corpus具有18957条文档（评论），而用于WmdSimilarity的corpus具有4137条文档。</p>
<p>下面代码是一个yelp评论的json文件被按行读取，文本被抽取，分词，停用词和标点符号被移除。</p>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Pre-processing a document.</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">from</span> nltk import word_tokenize</span><br><span class="line">download(<span class="string">'punkt'</span>)  <span class="comment"># Download data for tokenizer.</span></span><br><span class="line"></span><br><span class="line">def preprocess(doc):</span><br><span class="line">    doc = doc.<span class="built_in">lower</span>()  <span class="comment"># Lower the text.</span></span><br><span class="line">    doc = word_tokenize(doc)  <span class="comment"># Split into words.</span></span><br><span class="line">    doc = [w <span class="keyword">for</span> w <span class="keyword">in</span> doc <span class="keyword">if</span> <span class="keyword">not</span> w <span class="keyword">in</span> stop_words]  <span class="comment"># Remove stopwords.</span></span><br><span class="line">    doc = [w <span class="keyword">for</span> w <span class="keyword">in</span> doc <span class="keyword">if</span> w.isalpha()]  <span class="comment"># Remove numbers and punctuation.</span></span><br><span class="line">    <span class="literal">return</span> doc</span><br><span class="line"></span><br><span class="line"><span class="built_in">start</span> = <span class="built_in">time</span>()</span><br><span class="line"></span><br><span class="line">import json</span><br><span class="line"></span><br><span class="line"><span class="comment"># Business IDs of the restaurants.</span></span><br><span class="line">ids = [<span class="string">'4bEjOyTaDG24SY5TxsaUNQ'</span>, <span class="string">'2e2e7WgqU1BnpxmQL5jbfw'</span>, <span class="string">'zt1TpTuJ6y9n551sw9TaEg'</span>,</span><br><span class="line">      <span class="string">'Xhg93cMdemu5pAMkDoEdtQ'</span>, <span class="string">'sIyHTizqAiGu12XMLX3N3g'</span>, <span class="string">'YNQgak-ZLtYJQxlDwN-qIg'</span>]</span><br><span class="line"></span><br><span class="line">w2v_corpus = []  <span class="comment"># Documents to train word2vec on (all 6 restaurants).</span></span><br><span class="line">wmd_corpus = []  <span class="comment"># Documents to run queries against (only one restaurant).</span></span><br><span class="line">documents = []  <span class="comment"># wmd_corpus, with no pre-processing (so we can see the original documents).</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">'/data/yelp_academic_dataset_review.json'</span>) <span class="keyword">as</span> data_file:</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">line</span> <span class="keyword">in</span> data_file:</span><br><span class="line">        json_line = json.loads(<span class="built_in">line</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> json_line[<span class="string">'business_id'</span>] <span class="keyword">not</span> <span class="keyword">in</span> ids:</span><br><span class="line">            <span class="comment"># Not one of the 6 restaurants.</span></span><br><span class="line">            continue</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Pre-process document.</span></span><br><span class="line">        <span class="keyword">text</span> = json_line[<span class="string">'text'</span>]  <span class="comment"># Extract text from JSON object.</span></span><br><span class="line">        <span class="keyword">text</span> = preprocess(<span class="keyword">text</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Add to corpus for training Word2Vec.</span></span><br><span class="line">        w2v_corpus.append(<span class="keyword">text</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> json_line[<span class="string">'business_id'</span>] == ids[<span class="number">0</span>]:</span><br><span class="line">            <span class="comment"># Add to corpus for similarity queries.</span></span><br><span class="line">            wmd_corpus.append(<span class="keyword">text</span>)</span><br><span class="line">            documents.append(json_line[<span class="string">'text'</span>])</span><br><span class="line"></span><br><span class="line">print <span class="string">'Cell took %.2f seconds to run.'</span> %(<span class="built_in">time</span>() - <span class="built_in">start</span>)</span><br></pre></td></tr></table></figure>
<p>下面是一个文档长度的直方图，该图中也包含了平均文档长度。注意这些是经过预处理的文档，也就是说停用词已经被移除，标点符号已经被移除，等等。文档长度对wmd的运行时间有很大的影响，因此当和本次实验比较运行时间时，查询corpus的文档数量（大约4000）和文档长度（大约平均62个词）应该被考虑在内。</p>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">from</span> matplotlib import pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># Document lengths.</span></span><br><span class="line">lens = [<span class="built_in">len</span>(doc) <span class="keyword">for</span> doc <span class="keyword">in</span> wmd_corpus]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot.</span></span><br><span class="line">plt.rc(<span class="string">'figure'</span>, figsize=(<span class="number">8</span>,<span class="number">6</span>))</span><br><span class="line">plt.rc(<span class="string">'font'</span>, size=<span class="number">14</span>)</span><br><span class="line">plt.rc(<span class="string">'lines'</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">plt.rc(<span class="string">'axes'</span>, color_cycle=(<span class="string">'#377eb8'</span>,<span class="string">'#e41a1c'</span>,<span class="string">'#4daf4a'</span>,</span><br><span class="line">                            <span class="string">'#984ea3'</span>,<span class="string">'#ff7f00'</span>,<span class="string">'#ffff33'</span>))</span><br><span class="line"><span class="comment"># Histogram.</span></span><br><span class="line">plt.hist(lens, bins=<span class="number">20</span>)</span><br><span class="line">plt.hold(True)</span><br><span class="line"><span class="comment"># Average length.</span></span><br><span class="line">avg_len = <span class="built_in">sum</span>(lens) / float(<span class="built_in">len</span>(lens))</span><br><span class="line">plt.axvline(avg_len, color=<span class="string">'#e41a1c'</span>)</span><br><span class="line">plt.hold(False)</span><br><span class="line">plt.title(<span class="string">'Histogram of document lengths.'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Length'</span>)</span><br><span class="line">plt.<span class="keyword">text</span>(<span class="number">100</span>, <span class="number">800</span>, <span class="string">'mean = %.2f'</span> % avg_len)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://i2.muimg.com/567571/479272b7085337df.png" alt></p>
<p>现在，我们想要用corpus和w2v初始化相似类（提供了嵌入和wmdistance方法）。</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train Word2Vec on all the restaurants.</span></span><br><span class="line">model = Word2Vec(w2v_corpus, <span class="attribute">workers</span>=3, <span class="attribute">size</span>=100)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize WmdSimilarity.</span></span><br><span class="line"><span class="keyword">from</span> gensim.similarities import WmdSimilarity</span><br><span class="line">num_best = 10</span><br><span class="line">instance = WmdSimilarity(wmd_corpus, model, <span class="attribute">num_best</span>=10)</span><br></pre></td></tr></table></figure>
<p>num_best参数决定了查询返回的结果数量。现在让我们来做个查询。输出是corpus中文档相似度和索引的一个列表，按照相似度排序。</p>
<p>注意当num_best为None（也就是没有指定参数）时输出形式有些不同。在这种情况下，你得到一个涵盖corpus中每个文档的相似度数组。</p>
<p>下面的查询直接取自corpus中的一条评论。让我们看看是否有其他的评论相似于这一条。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">start</span> = <span class="built_in">time</span>()</span><br><span class="line"></span><br><span class="line">sent = <span class="string">'Very good, you should seat outdoor.'</span></span><br><span class="line"><span class="keyword">query</span> = preprocess(sent)</span><br><span class="line"></span><br><span class="line">sims = <span class="keyword">instance</span>[<span class="keyword">query</span>]  <span class="comment"># A query is simply a "look-up" in the similarity class.</span></span><br><span class="line"></span><br><span class="line">print <span class="string">'Cell took %.2f seconds to run.'</span> %(<span class="built_in">time</span>() - <span class="keyword">start</span>)</span><br></pre></td></tr></table></figure>
<p>查询和最相似的文档，以及它们的相似度，在下面被打印出来。我们看到被检索到的文档讨论着和查询一样的事情，尽管使用了不同的单词。查询谈论的是得到一个户外的座位，而结果谈论的是坐在外面，结果中的一个说的是餐馆有一个好的景观。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Print the query and the retrieved documents, together with their similarities.</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'Query:'</span></span><br><span class="line"><span class="keyword">print</span> sent</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_best):</span><br><span class="line">    <span class="keyword">print</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'sim = %.4f'</span> % sims[i][<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">print</span> documents[sims[i][<span class="number">0</span>]]</span><br></pre></td></tr></table></figure>
<p>让我们尝试一个不同的查询，同样直接取自corpus评论中的一个。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">start</span> = <span class="built_in">time</span>()</span><br><span class="line"></span><br><span class="line">sent = <span class="string">'I felt that the prices were extremely reasonable for the Strip'</span></span><br><span class="line"><span class="keyword">query</span> = preprocess(sent)</span><br><span class="line"></span><br><span class="line">sims = <span class="keyword">instance</span>[<span class="keyword">query</span>]  <span class="comment"># A query is simply a "look-up" in the similarity class.</span></span><br><span class="line"></span><br><span class="line">print <span class="string">'Query:'</span></span><br><span class="line">print sent</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="keyword">range</span>(num_best):</span><br><span class="line">    print</span><br><span class="line">    print <span class="string">'sim = %.4f'</span> % sims[i][<span class="number">1</span>]</span><br><span class="line">    print documents[sims[i][<span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line">print <span class="string">'\nCell took %.2f seconds to run.'</span> %(<span class="built_in">time</span>() - <span class="keyword">start</span>)</span><br></pre></td></tr></table></figure>
<p>这次，结果更加直接。检索到的文档基本包含了与查询相同的词。</p>
<p>WmdSimilarity默认正则化了词嵌入（使用init_sims()，正如前面解释的），但你可以改变这个行为通过调用WmdSimilarity,并设置normalize_w2v_and_replace=False。</p>
<figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print 'Notebook took %<span class="number">.2</span>f seconds <span class="keyword">to</span> <span class="built_in">run</span>.' %(<span class="built_in">time</span>() - start_nb)</span><br></pre></td></tr></table></figure>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ol>
<li>Ofir Pele and Michael Werman, A linear time histogram metric for improved SIFT matching, 2008.</li>
<li>Ofir Pele and Michael Werman, Fast and robust earth mover’s distances, 2009.</li>
<li>Matt Kusner et al. From Embeddings To Document Distances, 2015.</li>
<li>Thomas Mikolov et al. Efficient Estimation of Word Representations in Vector Space, 2013.</li>
</ol>

      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      
        <div class="post-tags">
          
            <a href="/tags/machine-learning/" rel="tag"># machine learning</a>
          
            <a href="/tags/NLP/" rel="tag"># NLP</a>
          
            <a href="/tags/WMD/" rel="tag"># WMD</a>
          
        </div>
      

      
      
      

      
        <div class="post-nav">
          <div class="post-nav-next post-nav-item">
            
              <a href="/2019/02/28/machine learning/NLP/materials/" rel="next" title="materials">
                <i class="fa fa-chevron-left"></i> materials
              </a>
            
          </div>

          <span class="post-nav-divider"></span>

          <div class="post-nav-prev post-nav-item">
            
              <a href="/2019/02/28/development/environment/install ubuntu/" rel="prev" title="install ubuntu">
                install ubuntu <i class="fa fa-chevron-right"></i>
              </a>
            
          </div>
        </div>
      

      
      
    </footer>
  </div>
  
  
  
  </article>


  </div>


          </div>
          

  



        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      
        <ul class="sidebar-nav motion-element">
          <li class="sidebar-nav-toc sidebar-nav-active" data-target="post-toc-wrap">
            Table of Contents
          </li>
          <li class="sidebar-nav-overview" data-target="site-overview-wrap">
            Overview
          </li>
        </ul>
      

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">muzhen</p>
              <div class="site-description motion-element" itemprop="description"></div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">369</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">38</span>
                    <span class="site-state-item-name">categories</span>
                  
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">114</span>
                    <span class="site-state-item-name">tags</span>
                  
                </div>
              
            </nav>
          

          

          

          

          
          

          
            
          
          

        </div>
      </div>

      
      <!--noindex-->
        <div class="post-toc-wrap motion-element sidebar-panel sidebar-panel-active">
          <div class="post-toc">

            
            
            
            

            
              <div class="post-toc-content"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#文章出处"><span class="nav-number">1.</span> <span class="nav-text">文章出处</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#使用W2V和WMD发现文档的相似性"><span class="nav-number">2.</span> <span class="nav-text">使用W2V和WMD发现文档的相似性</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#WMD基础"><span class="nav-number">3.</span> <span class="nav-text">WMD基础</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#运行notebook"><span class="nav-number">4.</span> <span class="nav-text">运行notebook</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第一部分：计算词移距离（Word-Mover’s-Distance）"><span class="nav-number">5.</span> <span class="nav-text">第一部分：计算词移距离（Word Mover’s Distance）</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#正则化w2v向量"><span class="nav-number">6.</span> <span class="nav-text">正则化w2v向量</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#第二部分：使用WmdSimilarity进行相似度查询"><span class="nav-number">7.</span> <span class="nav-text">第二部分：使用WmdSimilarity进行相似度查询</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Yelp-data"><span class="nav-number">7.1.</span> <span class="nav-text">Yelp data</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#References"><span class="nav-number">8.</span> <span class="nav-text">References</span></a></li></ol></div>
            

          </div>
        </div>
      <!--/noindex-->
      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">muzhen</span>

  

  
</div>


  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.0.1</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=7.0.1"></script>

  <script src="/js/src/motion.js?v=7.0.1"></script>



  
  


  <script src="/js/src/schemes/muse.js?v=7.0.1"></script>



  
  <script src="/js/src/scrollspy.js?v=7.0.1"></script>
<script src="/js/src/post-details.js?v=7.0.1"></script>



  


  <script src="/js/src/next-boot.js?v=7.0.1"></script>


  
  


  


  




  

  

  
  

  
  

  


  

  

  

  

  

  

  

  

  

  

</body>
</html>
