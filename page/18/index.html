<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang="en">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">


























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=7.0.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.0.1">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.0.1">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.0.1">


  <link rel="mask-icon" href="/images/logo.svg?v=7.0.1" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '7.0.1',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false,"dimmer":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta property="og:type" content="website">
<meta property="og:title" content="the Home of MuZhen">
<meta property="og:url" content="http://www.muzhen.tk/page/18/index.html">
<meta property="og:site_name" content="the Home of MuZhen">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="the Home of MuZhen">






  <link rel="canonical" href="http://www.muzhen.tk/page/18/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>the Home of MuZhen</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">the Home of MuZhen</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2019/02/28/development/datacenter/Detecting Large-Scale System Problems by Mining Console Logs/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/02/28/development/datacenter/Detecting Large-Scale System Problems by Mining Console Logs/" class="post-title-link" itemprop="url">Detecting Large-Scale System Problems by Mining Console Logs</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-02-28 20:45:06" itemprop="dateCreated datePublished" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/datacenter/" itemprop="url" rel="index"><span itemprop="name">datacenter</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p>
<h1 id="原文链接"><a href="#原文链接" class="headerlink" title="原文链接"></a>原文链接</h1><p><a href="http://iiis.tsinghua.edu.cn/~weixu/files/sosp09.pdf" target="_blank" rel="noopener">Detecting Large-Scale System Problems by Mining Console Logs</a></p>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>console log是被独立开发者所写的许多软件反馈信息的混合。惊奇的是，log很少能够帮助操作者侦测大规模数据中心服务器的问题。我们提出了一个一般方法论去通过这里丰富的信息源去自动侦测系统运行问题。<br>首先，我们结合源码分析和信息检索去解析log，以此来创造合成特征。然后，我们使用机器学习去分析这些特征来侦测系统问题。由于我们的方法具有较高的创造合成特征的能力，因此它可以分析出那些先前方法分析不出的问题。<br>我们同样展示了怎样提炼我们的分析结果到一个操作友好的单页决策树去列出一些对于所侦测问题至关重要的信息。我们使用Darkstar在线游戏服务和Hadoop文件系统数据集验证了我们的方法，它们以高精度和低假阳性率侦测出了许多真实问题。<br>在Hadoop案例中，我们能够在3min中内分析24百万条log。我们的方法可以工作于任意大小的log源数据上，不需要对服务软件进行修改，不需要人为输入，也不需要关于软件核心的知识。</p>
<h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h1><p>当一个由成百上千个运行着成百上千软件的电脑组成的大规模数据中心出现问题时，运维人员需要使用各种工具去侦测系统问题。<br>讽刺的是，有一个信息源，它囊括了几乎每种软件的细节信息，但却被经常忽略，它就是精简的console log。那些细节信息反映了软件原始开发者对于显著和不寻常活动的想法。</p>
<p>由于不同软件log形式不一致，而且由于软件的频繁修订和更新，导致log很混乱。我们的目标就是提供更好的工具去从log中提取价值。</p>
<p>由于log数据太庞大以至于不能人工检查，同时太不结构化以至于无法自动分析，因此运维人员往往通过搜索‘eroor’，’critical’之类关键词的方式来提取信息，但这已被表明对于侦测问题而言是不充分的。<br>而基于规则的处理过程是一个改进，但由于缺乏关于特殊软件成分的细节性知识，以及被各成分的交互所影响，依旧难以写出规则，去检出最相关的log。<br>代替要求用户去检测这一方式，我们提供了工具去自动发现有效的log信息。</p>
<p>由于不寻常的log信息通常表明了问题的根源，将之格式化机器学习中的异常检测问题是合理的。但是，用现有的方法并不能很好的解决这个问题，他是一个在不同种类log信息及其关系中进行异常检测的问题。<br>因此，相较于分析log中的字词，我们创造特征去精确捕获log之间各种各样的相关关系，然后通过这些特征去进行异常检测。创造这些特征需要利用源码信息去扩充log解析过程，这正是我们方法贡献的一部分。</p>
<p>我们研究很多应用于网络服务上的流行软件系统的log和源码，观察到console log比他看起来的更有结构性：‘schema’的定义隐含在log中，并可以从程序源码中发现。这个发现是我们log解析方法的关键，将可以导出细节和精确的特征。<br>我们相信开源软件开源代码的获取并不是我们方法的实践缺点。</p>
<p>我们的贡献是一个一般的四步法，它允许机器学习和信息检索技术被用于大海捞针，发现系统问题，而不需要人工输入。特别的，我们的方法包含了以下四个贡献：</p>
<ol>
<li>通过分析源码来发现log中隐含结构的技术;</li>
<li>log中信息的识别和特征的创造生成;</li>
<li>从大数据集中有效侦测异常的机器学习和信息检索方法论的证明;</li>
<li>异常侦测结果的一个合适、自动、用户友好的可视化构建方法。</li>
</ol>
<p>我们对log的分析可以深入细节层，可以并行化。</p>
<p>我们使用两个数据集做了验证。在Darkstar中，我们可以即时检测行为异常并提供异常原因的线索。在Hadoop中，我们可以探测运行时间异常这一经常被忽视的问题，并将结果可视化。</p>
<p>第二部分提供了我们方法的概要，第三部分从细节层面描述了我们的log解析技术，第四和第五部分阐明了特征创造和异常检测的结果，<br>第六部分评估了我们的方法并讨论了可视化技术，第七部分讨论了一些扩展和提出了改进log质量的建议，第八部分总结了相关工作，第九部分描述了一些结论。</p>
<h1 id="2-方法概览"><a href="#2-方法概览" class="headerlink" title="2. 方法概览"></a>2. 方法概览</h1><h2 id="2-1-信息被埋藏在log之中"><a href="#2-1-信息被埋藏在log之中" class="headerlink" title="2.1 信息被埋藏在log之中"></a>2.1 信息被埋藏在log之中</h2><p>重要信息被埋藏在大量log之中。为了自动分析log，我们需要创造高质量特征，实现log信息的数值表示，以便用于机器学习算法。以下三个关键观察导致了我们对这个问题的解决方法。</p>
<p><strong>源码是log的‘schema’。</strong> 尽管log可以以任意的文本格式出现，但实际上他们相当结构化，因为他们被系统中相对较小的log打印陈述规则集合所生成。</p>
<p>考虑图1中所展示的简单的log摘录和产生他们的源码。直觉上，使用源码信息去发现log的隐藏‘schema’是容易的。我们利用源码分析去发现log的固有结构。<br>我们方法的最显著优点是能够精确解析所有可能的log信息，即使是很少出现的log信息。另外，我们可以利用存在的方法去删除大部分启发式和猜测式的log解析。</p>
<p><img src="http://i1.piimg.com/567571/e2199d624f5440ef.png" alt></p>
<p><strong>通用的log结构导致有用的特征。</strong> 在这篇论文中，我们将log的常量部分称作<em>信息类型（message types）</em>，变量部分称作<em>信息变量（meassge variables）</em>。</p>
<p>我们仅仅将常量字符串标记为信息类型，完全忽视它们的语义。</p>
<p>信息变量也包含了至关重要的信息。我们识别了两个重要的信息变量：时间戳和各种各样的计数。</p>
<p><em>识别器（identifiers）</em>是用于识别客体的变量。而<em>状态变量（state vars）</em>是用于列举客体有的一系列可能状态的标签。我们可以基于频率判别一个给定的变量是识别器还是状态变量。表1给出了例子。直觉上，状态变量会有相对小的不同值，而识别器会有相对大的不同值（细节在第四部分）。</p>
<p><img src="http://i4.buimg.com/567571/3bca69ffecc22d0c.png" alt></p>
<p>信息类型和变量包含了重要的信息。但是，缺少工具去抽取这些结构。操作者要么忽视他们，要么手工花时间梳理他们。</p>
<p>我们的log解析方法允许我们使用结构化信息，例如信息类型和变量，去自动创造特征捕获log信息。据我们所知，这是工作是首次从log中抽取信息到这个粒度水平。</p>
<p><strong>信息强相关。</strong> 当log被合适的分组，组内信息具有强和稳定的相关性。例如，包含了特定文件名称的信息是高度相关的，因为它们很可能来自于系统中逻辑相关的操作步骤。</p>
<p>一组相关信息往往比起个体信息对问题具有更好的指示作用。 许多异常仅仅被不完全的信息片段所指出。例如，一个文件写入操作悄然失败（或许是因为开发者没有正确的处理报错机制），并没有一个单个的错误信息可以指出故障。但是，通过相同文件的相关信息，我们可以通过观察到相应的关闭文件信息缺失来侦测写入故障。先前的研究仅仅利用时间窗口进行log分组，并且侦测精度会遭受噪音影响。我们基于更为精确的信息去进行log分组，例如使用上面提到的信息变量。在这种情况下，相关性更加强大更具可读性，因此异常相关更容易被侦测。</p>
<h2 id="我们方法的工作流程"><a href="#我们方法的工作流程" class="headerlink" title="我们方法的工作流程"></a>我们方法的工作流程</h2><p><img src="http://i1.piimg.com/567571/696a19a03ea6c0f4.png" alt></p>
<p>图2展示了我们挖掘工作通用框架的四个步骤。</p>
<ol>
<li><p>log解析。<br>我们首先将log从非结构化文本转化为（信息类别，一系列信息变量）的数据结构。我们从源码中得到了所有可能的log信息模板字符串，并将之与log匹配，从而发现log的结构。我们的实验表明我们可以在真实系统中获得高精度的解析。<br>有系统使用了结构化追踪，例如BerkelyDB。既然这样，由于log已经被结构化了，我们可以跳过这个步骤，直接使用我们的特征创造和异常检测方法。注意这些结构化log仍然包含了识别器和状态变量。</p>
</li>
<li><p>特征创造。<br>下一步，我们通过对相关信息分组，并选择合适的变量，来构造特征向量。在这篇论文中，我们集中于构造状态比率向量和信息计数向量特征，这些在先前的研究中没有被探索。在我们对两个大规模真实统的实验中，这些特征都带来了很好的侦测结果。</p>
</li>
<li><p>异常检测。<br>然后，我们使用异常检测方法去挖掘特征向量，给每个特征向量打上正常或不正常的标签。我们发现基于主成分分析的异常侦测方法在这些特征上已经可以做的非常好。这是一个非监督学习算法，可以排除来自运维人员的先验需求，直接让所有的参数被自动选择或者轻松的进行调整。尽管我们使用了这一特殊的机器学习算法，它并不是我们方法的本质所在，利用不同抽取特征的不同算法可以轻易对我们的框架进行拓展。</p>
</li>
<li><p>可视化。<br>最后，为了让运维人员更好的理解侦测结果，我们利用决策树对结果做了可视化。相较于PCA侦测器，决策树以类似于活动处理规则的形式，提供了问题是如何被侦测到的更为细节的解释。</p>
</li>
</ol>
<h2 id="2-3-案例研究与数据收集"><a href="#2-3-案例研究与数据收集" class="headerlink" title="2.3 案例研究与数据收集"></a>2.3 案例研究与数据收集</h2><p>我们研究了22个被广泛部署的开源系统的源码和log。表2总结了成果。尽管这些系统本质上不同，它们在不同的时间被不同的开发者用不同的语言开发，其中20个系统还使用任意的log文本格式，我们基于源码的log解析应用到了全部20个系统上。有趣的是，</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2019/02/28/development/environment/install ubuntu/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/02/28/development/environment/install ubuntu/" class="post-title-link" itemprop="url">install ubuntu</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-02-28 20:45:06" itemprop="dateCreated datePublished" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/environment/" itemprop="url" rel="index"><span itemprop="name">environment</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="download-ubuntu-iso-and-make-boot-disk"><a href="#download-ubuntu-iso-and-make-boot-disk" class="headerlink" title="download ubuntu.iso and make boot disk"></a>download ubuntu.iso and make boot disk</h1><p><a href="https://mirrors.ustc.edu.cn/" target="_blank" rel="noopener">download</a></p>
<figure class="highlight delphi"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">// find the mount point of your u disk </span></span><br><span class="line">df</span><br><span class="line"><span class="comment">// cd to the location of your iso </span></span><br><span class="line">cd Downloads</span><br><span class="line"><span class="comment">// make it</span></span><br><span class="line">sudo dd <span class="keyword">if</span>=./<span class="keyword">name</span>.iso <span class="keyword">of</span>=/dev/sd?</span><br></pre></td></tr></table></figure>
<h1 id="install-terminator"><a href="#install-terminator" class="headerlink" title="install terminator"></a>install terminator</h1><p><a href="http://blog.csdn.net/liuxiaoheng1992/article/details/54409711" target="_blank" rel="noopener"> ubuntu14.04终端分屏terminator的安装使用与配置</a></p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="builtin-name">get</span> install terminator</span><br></pre></td></tr></table></figure>
<p>then you can change config to let your terminal more beautiful.</p>
<figure class="highlight arduino"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">cd ~/.<span class="built_in">config</span>/terminator/ </span><br><span class="line">sudo vim <span class="built_in">config</span></span><br></pre></td></tr></table></figure>
<p>if <code>~/.config/terminator/config</code> is not exists, you can open a terminal then right click to <code>perference-layouts-add-close</code>. the config file exists now.</p>
<p>there is a config templet below:</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">[global_config]</span><br><span class="line">[keybindings]</span><br><span class="line">[profiles]</span><br><span class="line">  [[default]]</span><br><span class="line">    use_system_font = <span class="literal">False</span> # 是否启用系统字体</span><br><span class="line">    login_shell = <span class="literal">True</span></span><br><span class="line">    background_darkness = 0.92 # 背景颜色</span><br><span class="line">    background_type = transparent</span><br><span class="line">    background_image = None</span><br><span class="line">    cursor_color = <span class="string">"#3036ec"</span> # 光标颜色</span><br><span class="line">    foreground_color = <span class="string">"#00ff00"</span></span><br><span class="line">    show_titlebar = <span class="literal">False</span> # 不显示标题栏，也就是 terminator 中那个默认的红色的标题栏</span><br><span class="line">    custom_command = tmux</span><br><span class="line">    font = Ubuntu Mono 15  # 字体设置，后面的数字表示字体大小</span><br><span class="line">[layouts]</span><br><span class="line">  [[default]]</span><br><span class="line">    [[[child1]]]</span><br><span class="line">     <span class="built_in"> type </span>= Terminal</span><br><span class="line">      parent = window0</span><br><span class="line">    [[[window0]]]</span><br><span class="line">     <span class="built_in"> type </span>= Window</span><br><span class="line">      parent = <span class="string">""</span></span><br><span class="line">[plugins]</span><br></pre></td></tr></table></figure>
<h1 id="install-vim-amp-gdebi"><a href="#install-vim-amp-gdebi" class="headerlink" title="install vim &amp; gdebi"></a>install vim &amp; gdebi</h1><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="builtin-name">get</span> install vim-gtk</span><br><span class="line">sudo apt-<span class="builtin-name">get</span> install gdebi</span><br></pre></td></tr></table></figure>
<h1 id="install-shadowsocks"><a href="#install-shadowsocks" class="headerlink" title="install shadowsocks"></a>install shadowsocks</h1><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get <span class="keyword">install</span> python-pip</span><br><span class="line">pip <span class="keyword">install</span> shadowsocks</span><br></pre></td></tr></table></figure>
<p>Create a config file /etc/shadowsocks.json. Example:<br><figure class="highlight json"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">&#123;</span><br><span class="line">    <span class="attr">"server"</span>:<span class="string">"my_server_ip"</span>,</span><br><span class="line">    <span class="attr">"server_port"</span>:<span class="number">8388</span>,</span><br><span class="line">    <span class="attr">"local_address"</span>: <span class="string">"127.0.0.1"</span>,</span><br><span class="line">    <span class="attr">"local_port"</span>:<span class="number">1080</span>,</span><br><span class="line">    <span class="attr">"password"</span>:<span class="string">"mypassword"</span>,</span><br><span class="line">    <span class="attr">"timeout"</span>:<span class="number">300</span>,</span><br><span class="line">    <span class="attr">"method"</span>:<span class="string">"aes-256-cfb"</span>,</span><br><span class="line">    <span class="attr">"fast_open"</span>: <span class="literal">false</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure></p>
<p>run shadowsocks:<br><figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sslocal -c <span class="regexp">/etc/</span>shadowsocks.json</span><br></pre></td></tr></table></figure></p>
<p>启动shadowsocks时，必须进入settings-network进行如下图设置：</p>
<p><img src="http://omdhuynsr.bkt.clouddn.com/17-8-5/29236859.jpg" alt></p>
<p>待安装好chrome后，安装switchyomega并如下图设置好代理后，将network proxy还原为none，就可以利用局部代理模式科学上网了。</p>
<p>先增一个ss代理，名字随便取，我下图中已经增加过了。只是演示如何增加。</p>
<p><img src="http://omdhuynsr.bkt.clouddn.com/17-8-5/42988679.jpg" alt></p>
<p><img src="http://omdhuynsr.bkt.clouddn.com/17-8-5/74327262.jpg" alt></p>
<p><img src="http://omdhuynsr.bkt.clouddn.com/17-8-5/39084595.jpg" alt></p>
<p><a href="https://github.com/shadowsocks/shadowsocks/tree/master" target="_blank" rel="noopener">install shadowsocks guide</a><br><a href="http://wxhp.org/shadowsocksr.html" target="_blank" rel="noopener">set swithchyomega proxy</a><br><em>rulelist</em>:<a href="https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt" target="_blank" rel="noopener">https://raw.githubusercontent.com/gfwlist/gfwlist/master/gfwlist.txt</a></p>
<h1 id="install-chrome"><a href="#install-chrome" class="headerlink" title="install chrome"></a>install chrome</h1><p>download .deb from <a href="https://www.google.com/chrome/browser/desktop/index.html?brand=CHBD&amp;gclid=CjwKCAjwzYDMBRA1EiwAwCv6Jtpe6PR1NYtzKPSVgnCLaWIbQCDJxZB_gyBBBndhfEu-E_BnUoOjaxoCFLUQAvD_BwE" target="_blank" rel="noopener">google chrome</a> then  <code>sudo gdebi chrome*.deb</code></p>
<p>When you install chrome at once,system maybe remind you to create keyring passward when you open chrome for the first time,you can set normally.<br>If you want to disable keyring,you can <code>seahorse</code> then right click <em>default keyring</em> to change passward to blank(first input old passward then blank new passward).</p>
<p>if you add ppa then <code>apt-get install</code>, you maybe find a lot of problems in that version.</p>
<p>在官网下载deb包安装后基本不会有下面问题出现，可以直接登录使用。但是貌似需要注销一次系统才能正常同步。</p>
<p>Maybe you will find a problem that external links are opened as blank tabs in new brower window in Chrome.The solution is below:</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo vim <span class="variable">$HOME</span>/.local/share/applications/google-chrome.desktop</span><br><span class="line">\\<span class="builtin-name">find</span> the line <span class="attribute">Exec</span>=/opt/google/chrome/chrome <span class="keyword">and</span> change <span class="keyword">to</span></span><br><span class="line"><span class="attribute">Exec</span>=/opt/google/chrome/chrome %U</span><br></pre></td></tr></table></figure>
<p><a href="http://askubuntu.com/questions/689449/external-links-are-opened-as-blank-tabs-in-new-browser-window-in-chrome" target="_blank" rel="noopener">External links are opened as blank tabs in new browser window in Chrome</a></p>
<h1 id="install-git"><a href="#install-git" class="headerlink" title="install git"></a>install git</h1><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install git</span><br><span class="line"></span><br><span class="line"><span class="comment">//配置个人信息</span></span><br><span class="line">git config --global user<span class="selector-class">.name</span> <span class="string">"Your Name Here"</span></span><br><span class="line">git config --global user<span class="selector-class">.email</span> your_email@example.com</span><br><span class="line">#查看已有的配置信息：</span><br><span class="line">git config --list</span><br></pre></td></tr></table></figure>
<p>if you use shadowsocks,you can use below codes to speed up:<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">git config --global http<span class="selector-class">.proxy</span> socks5:<span class="comment">//127.0.0.1:1080 </span></span><br><span class="line">git config --global https<span class="selector-class">.proxy</span> socks5:<span class="comment">//127.0.0.1:1080</span></span><br></pre></td></tr></table></figure></p>
<p>add ssh-key<br><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">\\check the exist of ssh-key</span><br><span class="line">cd ~/.ssh</span><br><span class="line">\\<span class="keyword">if</span> <span class="keyword">not</span> exist,generate ssh-key</span><br><span class="line">ssh-keygen -t rsa -C <span class="string">"your_email@youremail.com"</span> </span><br><span class="line">\\<span class="builtin-name">print</span> enter over time,then <span class="builtin-name">add</span> ssh-key <span class="keyword">to</span> your github</span><br><span class="line">\\copy key </span><br><span class="line">sudo vim ~/.ssh/id_rsa.pub</span><br><span class="line">\\load github by browser &gt;<span class="built_in"> Settings </span>&gt; SSH <span class="keyword">and</span> GPG keys &gt; <span class="builtin-name">add</span> key</span><br></pre></td></tr></table></figure></p>
<h1 id="install-vscode"><a href="#install-vscode" class="headerlink" title="install vscode"></a>install vscode</h1><p><a href="https://code.visualstudio.com/" target="_blank" rel="noopener">download</a></p>
<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">sudo</span> gdebi vscode<span class="regexp">*.deb</span></span><br></pre></td></tr></table></figure>
<h1 id="install-fcitx"><a href="#install-fcitx" class="headerlink" title="install fcitx"></a>install fcitx</h1><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="builtin-name">get</span> install fcitx-googlepinyin</span><br></pre></td></tr></table></figure>
<p>then <em>setting</em> &gt; <em>Language Support</em> &gt; ‘Keyboard input method system’ change to fcitx and add <em>chinese</em> by <em>Install/Remove Languages</em> &gt; log out &gt; add <em>google pinyin</em> by <em>configure</em> in input button which at the screen top.</p>
<h1 id="mount-ntfs"><a href="#mount-ntfs" class="headerlink" title="mount ntfs"></a>mount ntfs</h1><p>rename the ntfs disk</p>
<figure class="highlight awk"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo ntfslabel <span class="regexp">/dev/</span>sdxN my_label</span><br></pre></td></tr></table></figure>
<p>mount the ntfs disk</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo vim /etc/fstab</span><br><span class="line">\\ <span class="builtin-name">add</span> below texts <span class="keyword">in</span> it</span><br><span class="line"><span class="attribute">UUID</span>=04BA87A6BA8792BA /media/database ntfs rw,auto,users,exec,<span class="attribute">nls</span>=utf8,umask=003,gid=46,uid=1000    0   0</span><br></pre></td></tr></table></figure>
<p>uuid can be found by <code>sudo blkid</code></p>
<h1 id="install-zoom"><a href="#install-zoom" class="headerlink" title="install zoom"></a>install zoom</h1><p><a href="https://zoom.us/download?os=linux" target="_blank" rel="noopener">Doanload zoom client</a></p>
<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">sudo</span> gdebi zoom<span class="regexp">*.deb</span></span><br></pre></td></tr></table></figure>
<h1 id="install-okular"><a href="#install-okular" class="headerlink" title="install okular"></a>install okular</h1><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="builtin-name">get</span> install okular</span><br><span class="line"><span class="comment">#uninstall default pdf viewer</span></span><br><span class="line">sudo apt-<span class="builtin-name">get</span> <span class="builtin-name">remove</span> --purge evince</span><br></pre></td></tr></table></figure>
<h1 id="install-ksnapshot"><a href="#install-ksnapshot" class="headerlink" title="install ksnapshot"></a>install ksnapshot</h1><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="builtin-name">get</span> install ksnapshot</span><br></pre></td></tr></table></figure>
<h1 id="install-dropbox"><a href="#install-dropbox" class="headerlink" title="install dropbox"></a>install dropbox</h1><p><a href="https://www.dropbox.com/install-linux" target="_blank" rel="noopener">Download dropbox</a></p>
<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">sudo</span> gdebi dropbox<span class="regexp">*.deb</span></span><br><span class="line">dropbox start -i</span><br></pre></td></tr></table></figure>
<p>Maybe you need a pptp vpn to download complete package when you type <code>dropbox start -i</code>.</p>
<p>At least,I can’t normally download in the internet environment which built by shadowsocks and chrome proxy(SwithyOmega).</p>
<p>there is a method to install dropbox without downloading the complete package. <a href="http://blog.csdn.net/Jarlinn/article/details/38867455" target="_blank" rel="noopener">在Ubuntu14.04中，安装Dropbox后怎么在没有连接vpn的情况下启动Dropbox</a></p>
<p>step 1: </p>
<p><a href="https://www.dropbox.com/install-linux" target="_blank" rel="noopener">Download dropbox</a></p>
<figure class="highlight nginx"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="attribute">sudo</span> gdebi dropbox<span class="regexp">*.deb</span></span><br></pre></td></tr></table></figure>
<p>step 2:</p>
<p>download daemon <a href="http://www.dropbox.com/download/?plat=lnx.x86_64" target="_blank" rel="noopener">x86_64</a>. Or download from baiduyun(link: <a href="https://pan.baidu.com/s/1geBKJdD" target="_blank" rel="noopener">https://pan.baidu.com/s/1geBKJdD</a> pwd: dfww)</p>
<p>unzip daemon to root directory.</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">tar -xzf dropbox-lnx<span class="selector-class">.x86_64-2</span>.<span class="number">10.28</span><span class="selector-class">.tar</span><span class="selector-class">.gz</span> -C ~/</span><br></pre></td></tr></table></figure>
<p>step 3:</p>
<figure class="highlight jboss-cli"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cd</span> ~/</span><br><span class="line"><span class="string">.dropbox-dist/dropboxd</span></span><br></pre></td></tr></table></figure>
<p>then you can set the dropbox proxy.</p>
<p><img src="http://omdhuynsr.bkt.clouddn.com/17-8-5/38492276.jpg" alt></p>
<p>reference: <a href="http://blog.csdn.net/Jarlinn/article/details/38867455" target="_blank" rel="noopener">在Ubuntu14.04中，安装Dropbox后怎么在没有连接vpn的情况下启动Dropbox</a></p>
<h1 id="install-wps"><a href="#install-wps" class="headerlink" title="install wps"></a>install wps</h1><p>download latest wps and install. <a href="http://community.wps.cn/download/" target="_blank" rel="noopener">download wps</a></p>
<p><a href="http://www.cnblogs.com/liangml/p/5969404.html" target="_blank" rel="noopener">WPS for Linux（ubuntu）字体配置(字体缺失解决办法)</a></p>
<h1 id="install-gnome3"><a href="#install-gnome3" class="headerlink" title="install gnome3"></a>install gnome3</h1><p>when I install gnome3,some problems arise.I can’t address it yet!</p>
<h1 id="set-thunderbird"><a href="#set-thunderbird" class="headerlink" title="set thunderbird"></a>set thunderbird</h1><p>It’s easy.</p>
<h1 id="install-rar"><a href="#install-rar" class="headerlink" title="install rar"></a>install rar</h1><figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-<span class="builtin-name">get</span> install unrar</span><br></pre></td></tr></table></figure>
<h1 id="install-filezilla"><a href="#install-filezilla" class="headerlink" title="install filezilla"></a>install filezilla</h1><figure class="highlight smali"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo<span class="built_in"> add-apt-repository </span>ppa:n-muench/programs-ppa</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install filezilla</span><br></pre></td></tr></table></figure>
<h1 id="install-Xournal"><a href="#install-Xournal" class="headerlink" title="install Xournal"></a>install Xournal</h1><figure class="highlight smali"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo<span class="built_in"> add-apt-repository </span>ppa:nilarimogard/webupd8</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install xournal</span><br></pre></td></tr></table></figure>
<h1 id="install-wechart"><a href="#install-wechart" class="headerlink" title="install wechart"></a>install wechart</h1><p><a href="https://github.com/geeeeeeeeek/electronic-wechat/releases" target="_blank" rel="noopener">download wechart</a></p>
<p>unzip it and double click <em>electronic-wechat</em>.</p>
<h1 id="install-wineqq"><a href="#install-wineqq" class="headerlink" title="install wineqq"></a>install wineqq</h1><p>download <a href="http://www.ubuntukylin.com/application/show.php?lang=cn&amp;id=279" target="_blank" rel="noopener">winQQ国际版下载地址</a> and unzip it, then use below commands by order because of the dependency relationship:</p>
<figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo dpkg -<span class="selector-tag">i</span> ./fonts-wqy-microhei_0.<span class="number">2.0</span>-beta-<span class="number">2</span>_all.deb</span><br><span class="line">sudo dpkg -<span class="selector-tag">i</span> ./ttf-wqy-microhei_0.<span class="number">2.0</span>-beta-<span class="number">2</span>_all.deb</span><br><span class="line">sudo dpkg -<span class="selector-tag">i</span> ./wine-qqintl_0.<span class="number">1.3</span>-<span class="number">2</span>_i386.deb</span><br></pre></td></tr></table></figure>
<p>if you encounter problems, you can browse <a href="http://blog.csdn.net/fuchaosz/article/details/51919607" target="_blank" rel="noopener">Ubuntu 16.04 安装QQ解决方案</a></p>
<h1 id="install-download-tool"><a href="#install-download-tool" class="headerlink" title="install download tool"></a>install download tool</h1><ul>
<li>install Xware(迅雷)</li>
</ul>
<p><a href="http://forum.ubuntu.org.cn/viewtopic.php?t=461341" target="_blank" rel="noopener">xware deb 64位</a></p>
<figure class="highlight css"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">sudo</span> <span class="selector-tag">gdebi</span> <span class="selector-tag">xware-desktop_</span>??????<span class="selector-class">.deb</span></span><br></pre></td></tr></table></figure>
<p><a href="http://www.ubuntukylin.com/ukylin/forum.php?mod=viewthread&amp;tid=12324" target="_blank" rel="noopener">使用说明</a></p>
<h1 id="install-vlc"><a href="#install-vlc" class="headerlink" title="install vlc"></a>install vlc</h1><p><a href="http://www.linuxdiyf.com/linux/22727.html" target="_blank" rel="noopener">install vlc</a></p>
<figure class="highlight smali"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo<span class="built_in"> add-apt-repository </span>ppa:videolan/master-daily</span><br><span class="line">sudo apt-get update</span><br><span class="line">sudo apt-get install vlc</span><br></pre></td></tr></table></figure>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2019/02/28/machine learning/NLP/WMD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/02/28/machine learning/NLP/WMD/" class="post-title-link" itemprop="url">WMD</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-02-28 20:45:06" itemprop="dateCreated datePublished" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type"text javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p>
<h1 id="文章出处"><a href="#文章出处" class="headerlink" title="文章出处"></a>文章出处</h1><p>本文为<a href="https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/WMD_tutorial.ipynb" target="_blank" rel="noopener">WMD_tutorial</a>的翻译。</p>
<h1 id="使用W2V和WMD发现文档的相似性"><a href="#使用W2V和WMD发现文档的相似性" class="headerlink" title="使用W2V和WMD发现文档的相似性"></a>使用W2V和WMD发现文档的相似性</h1><p>WMD（Word Mover’s Distance）是机器学习中一个有前途的新工具，它允许我们提交查询并返回最相似的文档。例如，在博客<a href="http://tech.opentable.com/2015/08/11/navigating-themes-in-restaurant-reviews-with-word-movers-distance/" target="_blank" rel="noopener">OpenTable</a>中，使用了WMD分析了餐厅评论。通过使用这种方法，他们能够从评论中挖掘出不同的方面。这本教程的第二部分，我们展示了如何使用gensim的WmdSimilarity去做类似与opentable所做的事情。在第一部分，我们说明了如何使用wmdistance去计算两个文档的WMD距离。如果你的目的是想使用WmdSimilarity，第一部分可以选读，但它是有意义的。</p>
<p>不管怎样，首先，我们浏览下关于wmd是什么的基础知识。</p>
<h1 id="WMD基础"><a href="#WMD基础" class="headerlink" title="WMD基础"></a>WMD基础</h1><p>wmd允许我们用一种有意义的方式去评估两个文档之间的距离，即使他们之间没有共同词汇。他使用w2v进行词向量嵌入。它的表现优于很多最先进的k近邻分类方法。</p>
<p>wmd被下面两个非常相似的句子所阐明（图例来源于<a href="http://vene.ro/blog/word-movers-distance-in-python.html" target="_blank" rel="noopener">Vlad Niculae’s blog</a>）。两个句子之间没有共同词汇，但是通过匹配相关词，wmd能够精确度量两个句子之间的相似性。这个方法也使用了文档的词袋表征（简单的说，文档的词频），在下面的图中被标记为d。该方法的直观理解是我们发现文档之间最小的”traveling distance”，换句话说将文档1的分布移向文档2的最有效方式。</p>
<p><img src="http://i1.piimg.com/567571/651dd1b6e0d46b4b.png" alt></p>
<p>这个方法已经在文章<a href="http://jmlr.org/proceedings/papers/v37/kusnerb15.pdf" target="_blank" rel="noopener">“From Word Embeddings To Document Distances” by Matt Kusner et al.</a>中被介绍。它被”Earth Mover’s Distance”激发灵感，并且利用了 “transportation problem”的一个解法。</p>
<p>在本教程中，我们将学习如何使用gensim的wmd函数。它由进行距离计算的wmdistance方法和基于相似查询的corpus计算的WmdSimilarity类构成。</p>
<blockquote>
<p>注意：<br>  如果你使用这个软件，请留意引用[1]，[2]和[3]。</p>
</blockquote>
<h1 id="运行notebook"><a href="#运行notebook" class="headerlink" title="运行notebook"></a>运行notebook</h1><p>你可以下载这个<a href="http://ipython.org/notebook.html" target="_blank" rel="noopener">iPython Notebook</a>，并在你自己电脑上运行它，如果你已经安装了gensim，pyemd，nltk，并下载了必要的数据。</p>
<p>这个notebook被运行在i7-4770cpu 3.40GHz (8 cores) 和 32 GB memory的ubuntu机器上。在这台机器上运行整个notebook需要话费3分钟。</p>
<h1 id="第一部分：计算词移距离（Word-Mover’s-Distance）"><a href="#第一部分：计算词移距离（Word-Mover’s-Distance）" class="headerlink" title="第一部分：计算词移距离（Word Mover’s Distance）"></a>第一部分：计算词移距离（Word Mover’s Distance）</h1><p>为了使用wmd，我们首先需要一些词嵌入。你需要在一些corpus上训练一个w2v模型（<a href="https://rare-technologies.com/word2vec-tutorial/" target="_blank" rel="noopener">教程</a>），但是我们将通过下载一些预训练的w2v嵌入模型来开始。下载 <a href="https://code.google.com/archive/p/word2vec/" target="_blank" rel="noopener">GoogleNews-vectors-negative300.bin.gz</a>（警告：1.5GB，第二部分并不需要这些文件）。训练你自己的嵌入模型是有益处的，但为了简化本教程，我们将首先使用预训练的嵌入。</p>
<p>让我们拿一些句子来计算它们之间的相似度。</p>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">from</span> <span class="built_in">time</span> import <span class="built_in">time</span></span><br><span class="line">start_nb = <span class="built_in">time</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize logging.</span></span><br><span class="line">import logging</span><br><span class="line">logging.basicConfig(<span class="built_in">format</span>=<span class="string">'%(asctime)s : %(levelname)s : %(message)s'</span>)</span><br><span class="line"></span><br><span class="line">sentence_obama = <span class="string">'Obama speaks to the media in Illinois'</span></span><br><span class="line">sentence_president = <span class="string">'The president greets the press in Chicago'</span></span><br><span class="line">sentence_obama = sentence_obama.<span class="built_in">lower</span>().<span class="built_in">split</span>()</span><br><span class="line">sentence_president = sentence_president.<span class="built_in">lower</span>().<span class="built_in">split</span>()</span><br></pre></td></tr></table></figure>
<p>这些句子有很相似的内容，因此wmd应该低。在我们计算wmd之前，我们想要移除停用词（”the”, “to”, etc.），因为它们对句子信息并没有很多贡献。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import and download stopwords from NLTK.</span></span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"><span class="keyword">from</span> nltk <span class="keyword">import</span> download</span><br><span class="line">download(<span class="string">'stopwords'</span>)  <span class="comment"># Download stopwords list.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Remove stopwords.</span></span><br><span class="line">stop_words = stopwords.words(<span class="string">'english'</span>)</span><br><span class="line">sentence_obama = [w <span class="keyword">for</span> w <span class="keyword">in</span> sentence_obama <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> stop_words]</span><br><span class="line">sentence_president = [w <span class="keyword">for</span> w <span class="keyword">in</span> sentence_president <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> stop_words]</span><br></pre></td></tr></table></figure>
<p>现在，正如先前提到的，我们将使用一些下载的预训练嵌入。我们加载这些进入gensim的w2v模型类中。注意我们这里选择的嵌入需要很多内存。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">start</span> = <span class="built_in">time</span>()</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> KeyedVectors</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">'/data/w2v_googlenews/GoogleNews-vectors-negative300.bin.gz'</span>):</span><br><span class="line">    <span class="keyword">raise</span> ValueError(<span class="string">"SKIP: You need to download the google news model"</span>)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">model</span> = KeyedVectors.load_word2vec_format(<span class="string">'/data/w2v_googlenews/GoogleNews-vectors-negative300.bin.gz'</span>, <span class="built_in">binary</span>=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Cell took %.2f seconds to run.'</span> % (<span class="built_in">time</span>() - <span class="keyword">start</span>))</span><br></pre></td></tr></table></figure>
<p>因此让我们使用wmdistance方法计算wmd。</p>
<figure class="highlight swift"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">distance</span> = model.wmdistance(sentence_obama, sentence_president)</span><br><span class="line"><span class="built_in">print</span> '<span class="built_in">distance</span> = %.4f' % <span class="built_in">distance</span></span><br></pre></td></tr></table></figure>
<p>让我们对完全不相关的句子做相同的操作。注意距离变大了。</p>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sentence_orange = <span class="string">'Oranges are my favorite fruit'</span></span><br><span class="line">sentence_orange = sentence_orange.<span class="built_in">lower</span>().<span class="built_in">split</span>()</span><br><span class="line">sentence_orange = [w <span class="keyword">for</span> w <span class="keyword">in</span> sentence_orange <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> stop_words]</span><br><span class="line"></span><br><span class="line">distance = model.wmdistance(sentence_obama, sentence_orange)</span><br><span class="line">print <span class="string">'distance = %.4f'</span> % distance</span><br></pre></td></tr></table></figure>
<h1 id="正则化w2v向量"><a href="#正则化w2v向量" class="headerlink" title="正则化w2v向量"></a>正则化w2v向量</h1><p>当使用wmdistance方法时，首先正则化w2v向量是有益的，因此它们都有相同的长度。为了实现正则化，简单的调用model.init_sims(replace=True)，gensim会为你实现它。</p>
<p>通常，使用<a href="https://en.wikipedia.org/wiki/Cosine_similarity" target="_blank" rel="noopener">余弦距离</a>度量两个w2v向量之间的距离。余弦距离度量的是两个向量之间的夹角。另一方面，wmd使用欧式距离。两个向量之间的欧式距离可能会因为她们之间的长度区别而变得很大，但是因为它们之间的夹角很小因此余弦距离很小。我们可以通过正则化向量减轻这个问题。</p>
<p>注意正则化向量会花费一些时间，特别是你有一个大的词汇表 和/或 大的向量集。</p>
<p>用法在下面的例子中被阐明。碰巧我们下载的向量已经被正则化了。因此，在这个例子中我们不会作出什么差别。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Normalizing word2vec vectors.</span></span><br><span class="line"><span class="keyword">start</span> = <span class="built_in">time</span>()</span><br><span class="line"></span><br><span class="line">model.init_sims(<span class="keyword">replace</span>=<span class="literal">True</span>)  <span class="comment"># Normalizes the vectors in the word2vec class.</span></span><br><span class="line"></span><br><span class="line">distance = model.wmdistance(sentence_obama, sentence_president)  <span class="comment"># Compute WMD as normal.</span></span><br><span class="line"></span><br><span class="line">print <span class="string">'Cell took %.2f seconds to run.'</span> %(<span class="built_in">time</span>() - <span class="keyword">start</span>)</span><br></pre></td></tr></table></figure>
<h1 id="第二部分：使用WmdSimilarity进行相似度查询"><a href="#第二部分：使用WmdSimilarity进行相似度查询" class="headerlink" title="第二部分：使用WmdSimilarity进行相似度查询"></a>第二部分：使用WmdSimilarity进行相似度查询</h1><p>你可以通过WmdSimilarity类，使用wmd得到一个查询对应的最相似的文档。它的交互过程相似于在gensim教程<a href="https://radimrehurek.com/gensim/tut3.html" target="_blank" rel="noopener">Similarity Queries</a>中所描绘的。</p>
<blockquote>
<p>重要注意<br> wmd是一种距离度量。WmdSimilarity中的相似度是简单的负距离。注意不要混淆距离和相似度。两个相似文档将有一个高相似得分和一个小距离;两个差异文档将有低的相似得分和一个大距离。</p>
</blockquote>
<h2 id="Yelp-data"><a href="#Yelp-data" class="headerlink" title="Yelp data"></a>Yelp data</h2><p>让我们使用一些真实世界数据来尝试下相似度查询。为此我们使用<a href="https://www.yelp.com/dataset_challenge" target="_blank" rel="noopener">yelp评论</a>。特别的，我们将使用单一餐馆也就是<a href="https://en.yelp.be/biz/mon-ami-gabi-las-vegas-2" target="_blank" rel="noopener">Mon Ami Gabi</a>的评论。</p>
<p>为了得到yelp数据，你需要使用名字和邮箱进行注册。数据是775MB。</p>
<p>这一次，我们将用我们自己的数据去训练W2V嵌入。一个餐馆的数据并不足以合适的训练出w2v，因此我们使用了6个餐馆的数据进行训练。但是仅仅在他们中的一个进行相似度查询试验。除了上面提到的Mon Ami Gabi，我们还将使用：</p>
<ul>
<li>Earl of Sandwich.</li>
<li>Wicked Spoon.</li>
<li>Serendipity 3.</li>
<li>Bacchanal Buffet.</li>
<li>The Buffet.</li>
</ul>
<p>我们选择的餐馆是yelp数据集中那些具有最高数量评论的餐馆。顺带一提，它们都在Las Vegas Boulevard中。我们用于训练w2v的corpus具有18957条文档（评论），而用于WmdSimilarity的corpus具有4137条文档。</p>
<p>下面代码是一个yelp评论的json文件被按行读取，文本被抽取，分词，停用词和标点符号被移除。</p>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Pre-processing a document.</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">from</span> nltk import word_tokenize</span><br><span class="line">download(<span class="string">'punkt'</span>)  <span class="comment"># Download data for tokenizer.</span></span><br><span class="line"></span><br><span class="line">def preprocess(doc):</span><br><span class="line">    doc = doc.<span class="built_in">lower</span>()  <span class="comment"># Lower the text.</span></span><br><span class="line">    doc = word_tokenize(doc)  <span class="comment"># Split into words.</span></span><br><span class="line">    doc = [w <span class="keyword">for</span> w <span class="keyword">in</span> doc <span class="keyword">if</span> <span class="keyword">not</span> w <span class="keyword">in</span> stop_words]  <span class="comment"># Remove stopwords.</span></span><br><span class="line">    doc = [w <span class="keyword">for</span> w <span class="keyword">in</span> doc <span class="keyword">if</span> w.isalpha()]  <span class="comment"># Remove numbers and punctuation.</span></span><br><span class="line">    <span class="literal">return</span> doc</span><br><span class="line"></span><br><span class="line"><span class="built_in">start</span> = <span class="built_in">time</span>()</span><br><span class="line"></span><br><span class="line">import json</span><br><span class="line"></span><br><span class="line"><span class="comment"># Business IDs of the restaurants.</span></span><br><span class="line">ids = [<span class="string">'4bEjOyTaDG24SY5TxsaUNQ'</span>, <span class="string">'2e2e7WgqU1BnpxmQL5jbfw'</span>, <span class="string">'zt1TpTuJ6y9n551sw9TaEg'</span>,</span><br><span class="line">      <span class="string">'Xhg93cMdemu5pAMkDoEdtQ'</span>, <span class="string">'sIyHTizqAiGu12XMLX3N3g'</span>, <span class="string">'YNQgak-ZLtYJQxlDwN-qIg'</span>]</span><br><span class="line"></span><br><span class="line">w2v_corpus = []  <span class="comment"># Documents to train word2vec on (all 6 restaurants).</span></span><br><span class="line">wmd_corpus = []  <span class="comment"># Documents to run queries against (only one restaurant).</span></span><br><span class="line">documents = []  <span class="comment"># wmd_corpus, with no pre-processing (so we can see the original documents).</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">'/data/yelp_academic_dataset_review.json'</span>) <span class="keyword">as</span> data_file:</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">line</span> <span class="keyword">in</span> data_file:</span><br><span class="line">        json_line = json.loads(<span class="built_in">line</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> json_line[<span class="string">'business_id'</span>] <span class="keyword">not</span> <span class="keyword">in</span> ids:</span><br><span class="line">            <span class="comment"># Not one of the 6 restaurants.</span></span><br><span class="line">            continue</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Pre-process document.</span></span><br><span class="line">        <span class="keyword">text</span> = json_line[<span class="string">'text'</span>]  <span class="comment"># Extract text from JSON object.</span></span><br><span class="line">        <span class="keyword">text</span> = preprocess(<span class="keyword">text</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Add to corpus for training Word2Vec.</span></span><br><span class="line">        w2v_corpus.append(<span class="keyword">text</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> json_line[<span class="string">'business_id'</span>] == ids[<span class="number">0</span>]:</span><br><span class="line">            <span class="comment"># Add to corpus for similarity queries.</span></span><br><span class="line">            wmd_corpus.append(<span class="keyword">text</span>)</span><br><span class="line">            documents.append(json_line[<span class="string">'text'</span>])</span><br><span class="line"></span><br><span class="line">print <span class="string">'Cell took %.2f seconds to run.'</span> %(<span class="built_in">time</span>() - <span class="built_in">start</span>)</span><br></pre></td></tr></table></figure>
<p>下面是一个文档长度的直方图，该图中也包含了平均文档长度。注意这些是经过预处理的文档，也就是说停用词已经被移除，标点符号已经被移除，等等。文档长度对wmd的运行时间有很大的影响，因此当和本次实验比较运行时间时，查询corpus的文档数量（大约4000）和文档长度（大约平均62个词）应该被考虑在内。</p>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">from</span> matplotlib import pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># Document lengths.</span></span><br><span class="line">lens = [<span class="built_in">len</span>(doc) <span class="keyword">for</span> doc <span class="keyword">in</span> wmd_corpus]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot.</span></span><br><span class="line">plt.rc(<span class="string">'figure'</span>, figsize=(<span class="number">8</span>,<span class="number">6</span>))</span><br><span class="line">plt.rc(<span class="string">'font'</span>, size=<span class="number">14</span>)</span><br><span class="line">plt.rc(<span class="string">'lines'</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">plt.rc(<span class="string">'axes'</span>, color_cycle=(<span class="string">'#377eb8'</span>,<span class="string">'#e41a1c'</span>,<span class="string">'#4daf4a'</span>,</span><br><span class="line">                            <span class="string">'#984ea3'</span>,<span class="string">'#ff7f00'</span>,<span class="string">'#ffff33'</span>))</span><br><span class="line"><span class="comment"># Histogram.</span></span><br><span class="line">plt.hist(lens, bins=<span class="number">20</span>)</span><br><span class="line">plt.hold(True)</span><br><span class="line"><span class="comment"># Average length.</span></span><br><span class="line">avg_len = <span class="built_in">sum</span>(lens) / float(<span class="built_in">len</span>(lens))</span><br><span class="line">plt.axvline(avg_len, color=<span class="string">'#e41a1c'</span>)</span><br><span class="line">plt.hold(False)</span><br><span class="line">plt.title(<span class="string">'Histogram of document lengths.'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Length'</span>)</span><br><span class="line">plt.<span class="keyword">text</span>(<span class="number">100</span>, <span class="number">800</span>, <span class="string">'mean = %.2f'</span> % avg_len)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://i2.muimg.com/567571/479272b7085337df.png" alt></p>
<p>现在，我们想要用corpus和w2v初始化相似类（提供了嵌入和wmdistance方法）。</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train Word2Vec on all the restaurants.</span></span><br><span class="line">model = Word2Vec(w2v_corpus, <span class="attribute">workers</span>=3, <span class="attribute">size</span>=100)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize WmdSimilarity.</span></span><br><span class="line"><span class="keyword">from</span> gensim.similarities import WmdSimilarity</span><br><span class="line">num_best = 10</span><br><span class="line">instance = WmdSimilarity(wmd_corpus, model, <span class="attribute">num_best</span>=10)</span><br></pre></td></tr></table></figure>
<p>num_best参数决定了查询返回的结果数量。现在让我们来做个查询。输出是corpus中文档相似度和索引的一个列表，按照相似度排序。</p>
<p>注意当num_best为None（也就是没有指定参数）时输出形式有些不同。在这种情况下，你得到一个涵盖corpus中每个文档的相似度数组。</p>
<p>下面的查询直接取自corpus中的一条评论。让我们看看是否有其他的评论相似于这一条。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">start</span> = <span class="built_in">time</span>()</span><br><span class="line"></span><br><span class="line">sent = <span class="string">'Very good, you should seat outdoor.'</span></span><br><span class="line"><span class="keyword">query</span> = preprocess(sent)</span><br><span class="line"></span><br><span class="line">sims = <span class="keyword">instance</span>[<span class="keyword">query</span>]  <span class="comment"># A query is simply a "look-up" in the similarity class.</span></span><br><span class="line"></span><br><span class="line">print <span class="string">'Cell took %.2f seconds to run.'</span> %(<span class="built_in">time</span>() - <span class="keyword">start</span>)</span><br></pre></td></tr></table></figure>
<p>查询和最相似的文档，以及它们的相似度，在下面被打印出来。我们看到被检索到的文档讨论着和查询一样的事情，尽管使用了不同的单词。查询谈论的是得到一个户外的座位，而结果谈论的是坐在外面，结果中的一个说的是餐馆有一个好的景观。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Print the query and the retrieved documents, together with their similarities.</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'Query:'</span></span><br><span class="line"><span class="keyword">print</span> sent</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_best):</span><br><span class="line">    <span class="keyword">print</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'sim = %.4f'</span> % sims[i][<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">print</span> documents[sims[i][<span class="number">0</span>]]</span><br></pre></td></tr></table></figure>
<p>让我们尝试一个不同的查询，同样直接取自corpus评论中的一个。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">start</span> = <span class="built_in">time</span>()</span><br><span class="line"></span><br><span class="line">sent = <span class="string">'I felt that the prices were extremely reasonable for the Strip'</span></span><br><span class="line"><span class="keyword">query</span> = preprocess(sent)</span><br><span class="line"></span><br><span class="line">sims = <span class="keyword">instance</span>[<span class="keyword">query</span>]  <span class="comment"># A query is simply a "look-up" in the similarity class.</span></span><br><span class="line"></span><br><span class="line">print <span class="string">'Query:'</span></span><br><span class="line">print sent</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="keyword">range</span>(num_best):</span><br><span class="line">    print</span><br><span class="line">    print <span class="string">'sim = %.4f'</span> % sims[i][<span class="number">1</span>]</span><br><span class="line">    print documents[sims[i][<span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line">print <span class="string">'\nCell took %.2f seconds to run.'</span> %(<span class="built_in">time</span>() - <span class="keyword">start</span>)</span><br></pre></td></tr></table></figure>
<p>这次，结果更加直接。检索到的文档基本包含了与查询相同的词。</p>
<p>WmdSimilarity默认正则化了词嵌入（使用init_sims()，正如前面解释的），但你可以改变这个行为通过调用WmdSimilarity,并设置normalize_w2v_and_replace=False。</p>
<figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print 'Notebook took %<span class="number">.2</span>f seconds <span class="keyword">to</span> <span class="built_in">run</span>.' %(<span class="built_in">time</span>() - start_nb)</span><br></pre></td></tr></table></figure>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ol>
<li>Ofir Pele and Michael Werman, A linear time histogram metric for improved SIFT matching, 2008.</li>
<li>Ofir Pele and Michael Werman, Fast and robust earth mover’s distances, 2009.</li>
<li>Matt Kusner et al. From Embeddings To Document Distances, 2015.</li>
<li>Thomas Mikolov et al. Efficient Estimation of Word Representations in Vector Space, 2013.</li>
</ol>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2019/02/28/machine learning/NLP/materials/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/02/28/machine learning/NLP/materials/" class="post-title-link" itemprop="url">materials</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-02-28 20:45:06" itemprop="dateCreated datePublished" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="中文词向量"><a href="#中文词向量" class="headerlink" title="中文词向量"></a>中文词向量</h1><p><a href="http://www.cnblogs.com/Darwin2000/p/5786984.html" target="_blank" rel="noopener">开源共享一个训练好的中文词向量（语料是维基百科的内容，大概1G多一点）</a></p>
<h1 id="jieba-php版本汉语词性对照表"><a href="#jieba-php版本汉语词性对照表" class="headerlink" title="jieba php版本汉语词性对照表"></a>jieba php版本汉语词性对照表</h1><p><a href="https://www.ctolib.com/jieba-php.html" target="_blank" rel="noopener">jieba-php</a></p>
<figure class="highlight less"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br></pre></td><td class="code"><pre><span class="line"><span class="selector-tag">a</span> 形容词 (取英语形容词 adjective 的第 <span class="number">1</span> 个字母。)</span><br><span class="line">  <span class="selector-tag">ad</span> 副形词 (直接作状语的形容词，形容词代码 a 和副词代码 d 并在一起。)</span><br><span class="line">  <span class="selector-tag">ag</span> 形容词性语素 (形容词性语素，形容词代码为 a，语素代码 ｇ 前面置以 a。)</span><br><span class="line">  <span class="selector-tag">an</span> 名形词 (具有名词功能的形容词，形容词代码 a 和名词代码 n 并在一起。)</span><br><span class="line"><span class="selector-tag">b</span> 区别词 (取汉字「别」的声母。)</span><br><span class="line"><span class="selector-tag">c</span> 连词 (取英语连词 conjunction 的第 <span class="number">1</span> 个字母。)</span><br><span class="line"><span class="selector-tag">d</span> 副词 (取 adverb 的第 <span class="number">2</span> 个字母，因其第 <span class="number">1</span> 个字母已用于形容词。)</span><br><span class="line">  <span class="selector-tag">df</span> 副词*</span><br><span class="line">  <span class="selector-tag">dg</span> 副语素 (副词性语素，副词代码为 d，语素代码 ｇ 前面置以 d。)</span><br><span class="line"><span class="selector-tag">e</span> 叹词 (取英语叹词 exclamation 的第 <span class="number">1</span> 个字母。)</span><br><span class="line"><span class="selector-tag">eng</span> 外语</span><br><span class="line"><span class="selector-tag">f</span> 方位词 (取汉字「方」的声母。)</span><br><span class="line"><span class="selector-tag">g</span> 语素 (绝大多数语素都能作为合成词的「词根」，取汉字「根」的声母。)</span><br><span class="line"><span class="selector-tag">h</span> 前接成分 (取英语 head 的第 <span class="number">1</span> 个字母。)</span><br><span class="line"><span class="selector-tag">i</span> 成语 (取英语成语 idiom 的第 <span class="number">1</span> 个字母。)</span><br><span class="line"><span class="selector-tag">j</span> 简称略语 (取汉字「简」的声母。)</span><br><span class="line"><span class="selector-tag">k</span> 后接成分</span><br><span class="line"><span class="selector-tag">l</span> 习用语 (习用语尚未成为成语，有点「临时性」，取「临」的声母。)</span><br><span class="line"><span class="selector-tag">m</span> 数词 (取英语 numeral 的第 <span class="number">3</span> 个字母，n，u 已有他用。)</span><br><span class="line">  <span class="selector-tag">mg</span> 数语素</span><br><span class="line">  <span class="selector-tag">mq</span> 数词*</span><br><span class="line"><span class="selector-tag">n</span> 名词 (取英语名词 noun 的第 <span class="number">1</span> 个字母。)</span><br><span class="line">  <span class="selector-tag">ng</span> 名语素 (名词性语素，名词代码为 n，语素代码 ｇ 前面置以 n。)</span><br><span class="line">  <span class="selector-tag">nr</span> 人名 (名词代码n和「人(ren)」的声母并在一起。)</span><br><span class="line">  <span class="selector-tag">nrfg</span> 名词*</span><br><span class="line">  <span class="selector-tag">nrt</span> 名词*</span><br><span class="line">  <span class="selector-tag">ns</span> 地名 (名词代码 n 和处所词代码 s 并在一起。)</span><br><span class="line">  <span class="selector-tag">nt</span> 机构团体 (「团」的声母为 t，名词代码 n 和 t 并在一起。)</span><br><span class="line">  <span class="selector-tag">nz</span> 其他专名 (「专」的声母的第 <span class="number">1</span> 个字母为 z，名词代码 n 和 z 并在一起。)</span><br><span class="line"><span class="selector-tag">o</span> 拟声词 (取英语拟声词 onomatopoeia 的第 <span class="number">1</span> 个字母。)</span><br><span class="line"><span class="selector-tag">p</span> 介词 (取英语介词 prepositional 的第 <span class="number">1</span> 个字母。)</span><br><span class="line"><span class="selector-tag">q</span> 量词 (取英语 quantity 的第 <span class="number">1</span> 个字母。)</span><br><span class="line"><span class="selector-tag">r</span> 代词 (取英语代词 pronoun的 第 <span class="number">2</span> 个字母，因 p 已用于介词。)</span><br><span class="line">  <span class="selector-tag">rg</span> 代词语素</span><br><span class="line">  <span class="selector-tag">rr</span> 代词*</span><br><span class="line">  <span class="selector-tag">rz</span> 代词*</span><br><span class="line"><span class="selector-tag">s</span> 处所词 (取英语 space 的第 <span class="number">1</span> 个字母。)</span><br><span class="line"><span class="selector-tag">t</span> 时间词 (取英语 time 的第 <span class="number">1</span> 个字母。)</span><br><span class="line">  <span class="selector-tag">tg</span> 时语素 (时间词性语素，时间词代码为 t，在语素的代码 g 前面置以 t。)</span><br><span class="line"><span class="selector-tag">u</span> 助词 (取英语助词 auxiliary 的第 <span class="number">2</span> 个字母，因 a 已用于形容词。)</span><br><span class="line">  <span class="selector-tag">ud</span> 助词*</span><br><span class="line">  <span class="selector-tag">ug</span> 助词*</span><br><span class="line">  <span class="selector-tag">uj</span> 助词*</span><br><span class="line">  <span class="selector-tag">ul</span> 助词*</span><br><span class="line">  <span class="selector-tag">uv</span> 助词*</span><br><span class="line">  <span class="selector-tag">uz</span> 助词*</span><br><span class="line"><span class="selector-tag">v</span> 动词 (取英语动词 verb 的第一个字母。)</span><br><span class="line">  <span class="selector-tag">vd</span> 副动词 (直接作状语的动词，动词和副词的代码并在一起。)</span><br><span class="line">  <span class="selector-tag">vg</span> 动语素</span><br><span class="line">  <span class="selector-tag">vi</span> 动词*</span><br><span class="line">  <span class="selector-tag">vn</span> 名动词 (指具有名词功能的动词，动词和名词的代码并在一起。)</span><br><span class="line">  <span class="selector-tag">vq</span> 动词*</span><br><span class="line"><span class="selector-tag">w</span> 标点符号</span><br><span class="line"><span class="selector-tag">x</span> 非语素字 (非语素字只是一个符号，字母 x 通常用于代表未知数、符号。)</span><br><span class="line"><span class="selector-tag">y</span> 语气词 (取汉字「语」的声母。)</span><br><span class="line"><span class="selector-tag">z</span> 状态词 (取汉字「状」的声母的前一个字母。)</span><br><span class="line">  <span class="selector-tag">zg</span> 状态词*</span><br></pre></td></tr></table></figure>
<h1 id="中科院版汉语词性对照表"><a href="#中科院版汉语词性对照表" class="headerlink" title="中科院版汉语词性对照表"></a>中科院版汉语词性对照表</h1><p><a href="http://blog.csdn.net/kevin_darkelf/article/details/39520881" target="_blank" rel="noopener">汉语词性对照表[北大标准/中科院标准]</a></p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br></pre></td><td class="code"><pre><span class="line">词性编码</span><br><span class="line">词性名称</span><br><span class="line">注 解</span><br><span class="line">Ag</span><br><span class="line">形语素</span><br><span class="line">形容词性语素。形容词代码为 a，语素代码ｇ前面置以A。</span><br><span class="line">a</span><br><span class="line">形容词</span><br><span class="line">取英语形容词 adjective的第<span class="number">1</span>个字母。</span><br><span class="line">ad</span><br><span class="line">副形词</span><br><span class="line">直接作状语的形容词。形容词代码 a和副词代码d并在一起。</span><br><span class="line">an</span><br><span class="line">名形词</span><br><span class="line">具有名词功能的形容词。形容词代码 a和名词代码n并在一起。</span><br><span class="line">b</span><br><span class="line">区别词</span><br><span class="line">取汉字“别”的声母。</span><br><span class="line">c</span><br><span class="line">连词</span><br><span class="line">取英语连词 conjunction的第<span class="number">1</span>个字母。</span><br><span class="line">dg</span><br><span class="line">副语素</span><br><span class="line">副词性语素。副词代码为 d，语素代码ｇ前面置以D。</span><br><span class="line">d</span><br><span class="line">副词</span><br><span class="line">取 adverb的第<span class="number">2</span>个字母，因其第<span class="number">1</span>个字母已用于形容词。</span><br><span class="line">e</span><br><span class="line">叹词</span><br><span class="line">取英语叹词 exclamation的第<span class="number">1</span>个字母。</span><br><span class="line">f</span><br><span class="line">方位词</span><br><span class="line">取汉字“方”</span><br><span class="line">g</span><br><span class="line">语素</span><br><span class="line">绝大多数语素都能作为合成词的“词根”，取汉字“根”的声母。</span><br><span class="line">h</span><br><span class="line">前接成分</span><br><span class="line">取英语 head的第<span class="number">1</span>个字母。</span><br><span class="line">i</span><br><span class="line">成语</span><br><span class="line">取英语成语 idiom的第<span class="number">1</span>个字母。</span><br><span class="line">j</span><br><span class="line">简称略语</span><br><span class="line">取汉字“简”的声母。</span><br><span class="line">k</span><br><span class="line">后接成分</span><br><span class="line"> </span><br><span class="line">l</span><br><span class="line">习用语</span><br><span class="line">习用语尚未成为成语，有点“临时性”，取“临”的声母。</span><br><span class="line">m</span><br><span class="line">数词</span><br><span class="line">取英语 numeral的第<span class="number">3</span>个字母，n，u已有他用。</span><br><span class="line">Ng</span><br><span class="line">名语素</span><br><span class="line">名词性语素。名词代码为 n，语素代码ｇ前面置以N。</span><br><span class="line">n</span><br><span class="line">名词</span><br><span class="line">取英语名词 noun的第<span class="number">1</span>个字母。</span><br><span class="line">nr</span><br><span class="line">人名</span><br><span class="line">名词代码 n和“人(ren)”的声母并在一起。</span><br><span class="line">ns</span><br><span class="line">地名</span><br><span class="line">名词代码 n和处所词代码s并在一起。</span><br><span class="line">nt</span><br><span class="line">机构团体</span><br><span class="line">“团”的声母为 t，名词代码n和t并在一起。</span><br><span class="line">nz</span><br><span class="line">其他专名</span><br><span class="line">“专”的声母的第 <span class="number">1</span>个字母为z，名词代码n和z并在一起。</span><br><span class="line">o</span><br><span class="line">拟声词</span><br><span class="line">取英语拟声词 onomatopoeia的第<span class="number">1</span>个字母。</span><br><span class="line">p</span><br><span class="line">介词</span><br><span class="line">取英语介词 prepositional的第<span class="number">1</span>个字母。</span><br><span class="line">q</span><br><span class="line">量词</span><br><span class="line">取英语 quantity的第<span class="number">1</span>个字母。</span><br><span class="line">r</span><br><span class="line">代词</span><br><span class="line">取英语代词 pronoun的第<span class="number">2</span>个字母,因p已用于介词。</span><br><span class="line">s</span><br><span class="line">处所词</span><br><span class="line">取英语 space的第<span class="number">1</span>个字母。</span><br><span class="line">tg</span><br><span class="line">时语素</span><br><span class="line">时间词性语素。时间词代码为 t,在语素的代码g前面置以T。</span><br><span class="line">t</span><br><span class="line">时间词</span><br><span class="line">取英语 time的第<span class="number">1</span>个字母。</span><br><span class="line">u</span><br><span class="line">助词</span><br><span class="line">取英语助词 auxiliary</span><br><span class="line">vg</span><br><span class="line">动语素</span><br><span class="line">动词性语素。动词代码为 v。在语素的代码g前面置以V。</span><br><span class="line">v</span><br><span class="line">动词</span><br><span class="line">取英语动词 verb的第一个字母。</span><br><span class="line">vd</span><br><span class="line">副动词</span><br><span class="line">直接作状语的动词。动词和副词的代码并在一起。</span><br><span class="line">vn</span><br><span class="line">名动词</span><br><span class="line">指具有名词功能的动词。动词和名词的代码并在一起。</span><br><span class="line">w</span><br><span class="line">标点符号</span><br><span class="line"> </span><br><span class="line">x</span><br><span class="line">非语素字</span><br><span class="line">非语素字只是一个符号，字母 x通常用于代表未知数、符号。</span><br><span class="line">y</span><br><span class="line">语气词</span><br><span class="line">取汉字“语”的声母。</span><br><span class="line">z</span><br><span class="line">状态词</span><br><span class="line">取汉字“状”的声母的前一个字母。</span><br><span class="line">un</span><br><span class="line">未知词</span><br><span class="line">不可识别词及用户自定义词组。取英文Unkonwn首两个字母。(非北大标准，CSW分词中定义)</span><br></pre></td></tr></table></figure>
<h1 id="中文文本分类语料"><a href="#中文文本分类语料" class="headerlink" title="中文文本分类语料"></a>中文文本分类语料</h1><p><a href="http://www.newsmth.net/nForum/#!article/NLP/13408?p=1" target="_blank" rel="noopener">中文文本分类语料</a></p>
<figure class="highlight groovy"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">文本分类作为一项基础的研究，技术上已经很成熟了。下面提供一些网上能下载到的中文的好语料，供研究人员学习使用。 </span><br><span class="line">  </span><br><span class="line"><span class="number">1.</span>中科院自动化所的中英文新闻语料库    <span class="string">http:</span><span class="comment">//www.datatang.com/data/13484 </span></span><br><span class="line">中文新闻分类语料库从凤凰、新浪、网易、腾讯等版面搜集。英语新闻分类语料库为Reuters<span class="number">-21578</span>的ModApte版本。 </span><br><span class="line">  </span><br><span class="line"><span class="number">2.</span>搜狗的中文新闻语料库    <span class="string">http:</span><span class="comment">//www.sogou.com/labs/dl/c.html </span></span><br><span class="line">包括搜狐的大量新闻语料与对应的分类信息。有不同大小的版本可以下载。 </span><br><span class="line">  </span><br><span class="line"><span class="number">3.</span>李荣陆老师的中文语料库    <span class="string">http:</span><span class="comment">//www.datatang.com/data/11968 </span></span><br><span class="line">压缩后有<span class="number">240</span>M大小 </span><br><span class="line">  </span><br><span class="line"><span class="number">4.</span>谭松波老师的中文文本分类语料    <span class="string">http:</span><span class="comment">//www.datatang.com/data/11970 </span></span><br><span class="line">不仅包含大的分类，例如经济、运动等等，每个大类下面还包含具体的小类，例如运动包含篮球、足球等等。能够作为层次分类的语料库，非常实用。 </span><br><span class="line">  </span><br><span class="line"><span class="number">5.</span>网易分类文本数据    <span class="string">http:</span><span class="comment">//www.datatang.com/data/11965 </span></span><br><span class="line">包含运动、汽车等六大类的<span class="number">4000</span>条文本数据。 </span><br><span class="line">  </span><br><span class="line"><span class="number">6.</span>中文文本分类语料    <span class="string">http:</span><span class="comment">//www.datatang.com/data/11963 </span></span><br><span class="line">包含Arts、Literature等类别的语料文本。</span><br></pre></td></tr></table></figure>
<h1 id="中文情感分析资源"><a href="#中文情感分析资源" class="headerlink" title="中文情感分析资源"></a>中文情感分析资源</h1><p><a href="http://blog.sina.com.cn/s/blog_8af1069601019flb.html" target="_blank" rel="noopener">中英文情感分析资源</a></p>
<figure class="highlight haml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line">情感词典</span><br><span class="line">1.知网的情感词典</span><br><span class="line">-<span class="ruby"> <span class="symbol">http:</span>/<span class="regexp">/www.keenage.com/html</span><span class="regexp">/c_bulletin_2007.htm</span></span></span><br><span class="line"><span class="ruby">由知网发布的词典，包括中文情感词典和英文情感词典</span></span><br><span class="line"><span class="ruby">（以下需要论坛积分）</span></span><br><span class="line"><span class="ruby"><span class="number">2</span>.台湾大学的情感极性词典</span></span><br><span class="line"><span class="ruby">- <span class="symbol">http:</span>/<span class="regexp">/www.datatang.com/data</span><span class="regexp">/11837</span></span></span><br><span class="line"><span class="ruby">包括<span class="number">2810</span>个正极性词语和<span class="number">8276</span>个负极性词语。准确度很高</span></span><br><span class="line"><span class="ruby"></span></span><br><span class="line"><span class="ruby">情感分析语料</span></span><br><span class="line"><span class="ruby"><span class="number">3</span>.酒店评论语料</span></span><br><span class="line"><span class="ruby">- <span class="symbol">http:</span>/<span class="regexp">/www.datatang.com/data</span><span class="regexp">/11936 </span></span></span><br><span class="line"><span class="ruby">谭松波整理的一个较大规模的酒店评论语料。</span></span><br><span class="line"><span class="ruby">语料规模为<span class="number">10000</span>篇。语料从携程网上自动采集，并经过整理而成。</span></span><br><span class="line"><span class="ruby"><span class="number">4</span>.豆瓣网影评情感测试语料</span></span><br><span class="line"><span class="ruby">- <span class="symbol">http:</span>/<span class="regexp">/www.datatang.com/data</span><span class="regexp">/13539 </span></span></span><br><span class="line"><span class="ruby">来自豆瓣网对电影《ICE AGE3》的评论，评分标准均按照<span class="number">5</span> stars评分在网页中有标注。语料至<span class="number">527</span>页。每页<span class="number">20</span>条短评。共计<span class="number">11323</span>条评论</span></span><br><span class="line"><span class="ruby"><span class="number">5</span>.酒店、电脑与书籍的评论语料</span></span><br><span class="line"><span class="ruby">- <span class="symbol">http:</span>/<span class="regexp">/www.datatang.com/data</span><span class="regexp">/11937</span></span></span><br><span class="line"><span class="ruby">数据量不太大，也有一些重复的数据</span></span><br><span class="line"><span class="ruby"><span class="number">6</span>.评论网页数据集</span></span><br><span class="line"><span class="ruby">- <span class="symbol">http:</span>/<span class="regexp">/www.datatang.com/data</span><span class="regexp">/12044</span></span></span><br><span class="line"><span class="ruby">数据量不小，包括的电影和评论都不少</span></span><br></pre></td></tr></table></figure>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2019/02/28/machine learning/ensemble/adaboost/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/02/28/machine learning/ensemble/adaboost/" class="post-title-link" itemprop="url">adaboost</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-02-28 20:45:06" itemprop="dateCreated datePublished" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/ensemble/" itemprop="url" rel="index"><span itemprop="name">ensemble</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type"text javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p>
<h1 id="adaboost原理"><a href="#adaboost原理" class="headerlink" title="adaboost原理"></a>adaboost原理</h1><p>adaboost通过集成弱分类器来最终实现一个强分类器。</p>
<p>以下的讨论局限于二分类问题。并且注意正类标记为1,负类标记为-1。</p>
<p>弱分类器：优于随机水平。事实上，这很容易达到，因为对于低于随机水平的，只需要反转预测的label就可以高于随机水平了。</p>
<p>adaboost是一个加法模型，最终模型形如：<br>$$F(x) = \sum_{t=1}^T \alpha_t h_t(x)$$<br>$$H(x) = sign[F(x)]$$<br>其中，T是总迭代次数。$\alpha$是模型的权重，也就是需要求解的模型参数之一。另一部分参数就是各个基分类器。</p>
<p>我们的优化目标自然是最小化误差：<br>$$\arg \min Err(H;D) = \arg \min \frac{1}{m} \sum_{i=1}^m I(h(x_i) \neq y_i)$$<br>其中，m是样本数。</p>
<h1 id="权重调整"><a href="#权重调整" class="headerlink" title="权重调整"></a>权重调整</h1><p>adaboost通过调整误分类样本权重重新建模，来改进模型效果。<br>初始权重为$\frac{1}{m}$。</p>
<p>通过如下公式进行权重调整：<br>$$D_{t+1}(i) = \frac{1}{Z_t} D_t(i) \exp[-\alpha_t y_i h_t(x_i)]$$<br>其中，$Z_t$是规范化因子，保证权重大于等于0,和为1。$D_t(i)$是t轮迭代的样本权重。显然，当预测正确时，$y_i h_t(x_i) = 1$，新权重会减小;预测错误时，$y_i h_t(x_i) = -1$，新权重会增大。</p>
<p>因此，可以得到递推公式：<br>\begin{split} D_{t+1}(i) &amp;&amp;= \frac{1}{Z_t} D_t(i) \exp[-\alpha_t y_i h_t(x_i)] \newline<br>&amp;&amp;= \frac{1}{Z_t Z_{t-1}} D_{t-1}(i) \exp[-y_i(\alpha_t h_t(x_i) + \alpha_{t-1} h_{t-1}(x_i))] \newline<br>&amp;&amp;= \frac{1}{Z_t … Z_{1}} D_{1}(i) \exp[-y_i(\alpha_t h_t(x_i) + …+\alpha_{1} h_{1}(x_i))] \newline<br>&amp;&amp;= \frac{1}{Z_t … Z_{1}} D_{1}(i) \exp[-y_i F(x_i)] \end{split}</p>
<p>左右两边同时求和有：<br>\begin{split} 1 = \sum_{i=1}^m D_{t+1}(i) = \frac{1}{Z_t … Z_{1}} \frac{1}{m} \sum_{i=1}^m \exp[-y_i F(x_i)] \end{split}<br>\begin{split} Z = Z_t … Z_{1} = \frac{1}{m} \sum_{i=1}^m \exp[-y_i F(x_i)] \end{split}</p>
<h1 id="adaboost误差上界"><a href="#adaboost误差上界" class="headerlink" title="adaboost误差上界"></a>adaboost误差上界</h1><p>adaboost通过优化误差上界来优化模型效果。</p>
<p>下面证明：<br>$$ Err(H) = \frac{1}{m} \sum_{i=1}^m I(h(x_i) \neq y_i) \leq Z = \frac{1}{m} \sum_{i=1}^m \exp[-y_i F(x_i)]$$</p>
<p>证明如下：</p>
<p>对于求和的每一项有：<br>\begin{split} \text{If } H(x_i) \neq y_i \text{ then the LHS } = 1 \leq \text{RHS } = e^{+|F(x_i)|} \newline<br>\text{If } H(x_i) = y_i \text{ then the LHS } = 0 \leq \text{RHS } = e^{-|F(x_i)|} \end{split}<br>显然求和后不等式依旧成立。故而得证。</p>
<p>因此，我们的问题转化为优化Z。同时，我们采用step-wise的方法去进行优化，也就是逐步优化$Z_1,Z_2,Z_3,…,Z_t$。</p>
<h1 id="权重系数求解"><a href="#权重系数求解" class="headerlink" title="权重系数求解"></a>权重系数求解</h1><p>对$Z_t$最小化，令导数为0。</p>
<p>\begin{split} Z_t(\alpha_t,h_t) &amp;&amp;= \sum_{x_i \in A} D_t(x_i) exp[-\alpha_t] + \sum_{x_i \in \bar{A}} D_t(x_i) exp[-\alpha_t] \newline<br>\frac{d Z_t(\alpha_t,h_t)}{d \alpha_t} &amp;&amp;= \sum_{x_i \in A} -D_t(x_i) exp[-\alpha_t] + \sum_{x_i \in \bar{A}} D_t(x_i) exp[-\alpha_t] = 0 \newline<br>\sum_{x_i \in A} D_t(x_i) &amp;&amp;= \sum_{x_i \in \bar{A}} D_t(x_i) exp[2 \alpha_t] \end{split}</p>
<p>我们定义加权误差：<br>\begin{split} \epsilon_t(h) = \sum_{i=1}^m D_t(x_i)I(h(x_i) \neq y_i) = \sum_{x_i \in \bar{A}} D_t(x_i) \end{split}</p>
<p>因此有：<br>\begin{split} \alpha_t = \frac{1}{2} \ln \frac{1-\epsilon_t(h_t)}{\epsilon_t(h_t)} \end{split}</p>
<p><strong>由于每个分类器都要求是弱分类器。也就是要求每个基分类器的 <em>加权误差率</em> 小于0.5。</strong><br>因此，$\alpha_t$显然都大于0。</p>
<h1 id="误差分析"><a href="#误差分析" class="headerlink" title="误差分析"></a>误差分析</h1><p>将解出的权重系数带回：</p>
<p>\begin{split} Z_t(\alpha_t,h_t) &amp;&amp;= \sum_{x_i \in A} D_t(x_i) exp[-\alpha_t] + \sum_{x_i \in \bar{A}} D_t(x_i) exp[-\alpha_t] \newline<br>&amp;&amp;= (1-\epsilon_t(h_t))\sqrt{\frac{\epsilon_t(h_t)}{1-\epsilon_t(h_t)}} + \epsilon_t(h_t)\sqrt{\frac{1-\epsilon_t(h_t)}{\epsilon_t(h_t)}} \newline<br>&amp;&amp;= 2\sqrt{\epsilon_t(h_t) (1-\epsilon_t(h_t))}\end{split}</p>
<p>令:<br>\begin{split} \gamma_t = \frac{1}{2} - \epsilon_t(h_t) , \gamma_t \in (0,\frac{1}{2}]\end{split}</p>
<p>则有：</p>
<p>\begin{split} Z_t(\alpha_t,h_t) &amp;&amp;= 2\sqrt{\epsilon_t(h_t) (1-\epsilon_t(h_t))} \newline<br>&amp;&amp;= \sqrt{1-4\gamma_t^2} \newline<br>&amp;&amp; \leq \exp[-2 \gamma_t^2]\end{split}</p>
<p>因此：</p>
<p>\begin{split} Err(H) \leq Z \leq \exp[-2\sum_{t=1}^T \gamma_t^2] \end{split}</p>
<p>随着迭代次数增加，误差上界以指数级减小。</p>
<h1 id="过拟合问题"><a href="#过拟合问题" class="headerlink" title="过拟合问题"></a>过拟合问题</h1><p>adaboost在使用中常常不容易过拟合。也就是说验证集可以随着训练集精度提升而提升，并不存在一个下降的拐点。</p>
<p>一些相关的理论解释：</p>
<blockquote>
<p>作者：知乎用户<br>链接：<a href="https://www.zhihu.com/question/41047671/answer/127832345" target="_blank" rel="noopener">https://www.zhihu.com/question/41047671/answer/127832345</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。<br>AdaBoost提出的论文对AdaBoost的泛化界进行了分析，使用了通常的学习器泛化界：<br>泛化错误（泛化错误可理解为测试错误） &lt; 训练错误 + 学习算法容量相关项     (1)<br>由(1)式可见，当训练错误不变时，应当选择简单的学习模型，从而减少学习算法容量。然而在实验中已经观察到，在一些情况，训练错误已经是0了，继续训练还能进一步减小泛化错误，这里的继续训练意味着增加学习算法容量，理应导致泛化错误上升才对。因此(1)直接套上AdaBoost是解释不通的。更有JMLR 2008的论文发现，更多的实验观察与Boosting的理论及其统计解释都不符合，例如使用多层的决策树竟然比使用一层的简单决策树更好。理论不能解释实验肯定是理论的问题，于是人们觉得，(1)式不够好，可能把更加细微的因素忽略了。<br>于是AdaBoost的作者针对这一情况，提出了新的学习器泛化界：<br>泛化错误 &lt; 训练Margin项 + 学习算法容量相关项     (2)<br>(1)中的训练错误是指一个数据样本分类对了就是0，错了就是1，而(2)里的“训练Margin”不仅看分类对错，还要看对的信心有多少，例如对于一个正类+1，分类器A输出+0.1，B输出+2，虽然都分类正确了，但B的信心更多。这样(2)就比(1)更加细致的刻画了学习器的表现。实验一看，果然AdaBoost在训练错误为0后，继续训练不能再减少训练错误了，确能够进一步减少训练Margin，也就是信心更足了。<br>看似解决了AdaBoost不容易过拟合的问题，然而好景不长，统计大牛Leo Breiman（bagging，random forest出自他手）来了个比 (2) 更紧的界（更紧就是更接近真实）：<br>泛化错误 &lt; 训练Margin的最小值 + 学习算法容量相关项     (3)<br>（3）的容量相关项目更小，但这不是关键，关键是根据这一理论，就应该去看训练Margin的最小值，也就是分类器在所有样本上信心最不足的那个。然而根据这一理论，Breiman设计了另一种Boosting算法arc-gv，最小训练Margin更小了，但实验效果比AdaBoost差了很多。于是乎Breiman的结论是，这个用训练Margin来刻画泛化错误整个就是不对的。大家都傻眼了，AdaBoost不容易过拟合的问题无解了。<br>7年之后。。。AdaBoost作者之一的工作发现，Breiman的实验竟然有问题：没有很好的控制决策树的复杂性，也就是说，AdaBoost和arc-gv关于“学习算法容量相关项”的值并不一样，虽然arc-gv的最小训练Margin更小，但后面一项更大啊，因此泛化错误就更大了。于是重做实验，都用一层决策树，这样后面一项都一样了，一看AdaBoost更好了，也就是说原来的Margin理论并没有错误，松了口气。该论文获ICML’06最佳论文奖。<br>但是这篇最佳论文奖并没有终结问题，当都用一层决策树时，AdaBoost的最小训练Margin比arc-gv还要小，也就是说，并没有否定(3)式。然而在实验中，最小化这个最小Margin的效果并不好。这篇论文也指出，可能要看Margin的分布，也就是算法在所有样本上的信心，而不是最差的那个样本上的信心。但这只是“可能”，理论研究者最求的是更紧的界。接下来都是我国研究者的贡献了，北京大学王立威等人的到了比(3)更紧的界，其中“训练Margin的最小值” 被替代为 “训练Margin的某一个值”，这某一个要解一个均衡式，但不管怎么说，最小Margin被替代掉了。南京大学的高尉和周志华教授推导出了“第k Margin界”，（3）和 “训练Margin的某一个值” 都是其k赋特定值的特例，并由此得到了基于“算法在所有样本上的信心”的界，比（3）式更好。<br>以上内容可见：<a href="https://wenku.baidu.com/view/8efc9b880975f46527d3e1cb.html" target="_blank" rel="noopener">CCL2014_keynote-周志华</a><br>Margin理论讨论的主要是学习算法在训练样本上的信心，学习算法的容量是不是随着训练轮数的增加而增加呢，其实并不一定，近来有工作表明，有差异的学习器的组合，能够起到正则化的作用，也就是减少学习算法容量（<a href="http://lamda.nju.edu.cn/yuy/GetFile.aspx?File=papers/ecml12-divprune.pdf" target="_blank" rel="noopener">Diversity regularized ensemble pruning. ECML’12</a>; <a href="https://arxiv.org/abs/1511.07110" target="_blank" rel="noopener">On the Generalization Error Bounds of Neural Networks under Diversity-Inducing Mutual Angular Regularization</a>）。在许多variance-bias 分解实验中也观察到，AdaBoost不仅是减少了bias，同时也减少了variance，variance的减少往往与算法容量减少有关。<br>总之这一方面还值得进一步探索，新原理的发型，有可能导致新型高效学习算法的发明，意义重大。</p>
</blockquote>
<h1 id="references"><a href="#references" class="headerlink" title="references"></a>references</h1><p><a href="https://www.cse.buffalo.edu/~jcorso/t/CSE555/files/lecture_boosting.pdf" target="_blank" rel="noopener">boosting and adaboost</a></p>
<p><a href="https://www.zhihu.com/question/41047671" target="_blank" rel="noopener">adaboost为什么不容易过拟合呢？</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2019/02/28/machine learning/optimization/优化方法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/02/28/machine learning/optimization/优化方法/" class="post-title-link" itemprop="url">优化方法</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-02-28 20:45:06" itemprop="dateCreated datePublished" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/optimization/" itemprop="url" rel="index"><span itemprop="name">optimization</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p>
<h1 id="梯度下降法"><a href="#梯度下降法" class="headerlink" title="梯度下降法"></a>梯度下降法</h1><h2 id="SGD-batch-GD-mini-batch-GD"><a href="#SGD-batch-GD-mini-batch-GD" class="headerlink" title="SGD/batch GD/mini-batch GD"></a>SGD/batch GD/mini-batch GD</h2><p>SGD: 每次对一个样本点计算梯度进行更新<br>batch GD： 每次都所有样本点计算梯度进行更新<br>mini-batch GD: 每次对一小批样本点计算梯度进行更新</p>
<h2 id="momentum"><a href="#momentum" class="headerlink" title="momentum"></a>momentum</h2><p>\begin{split} v_t &amp;&amp;= \gamma v_{t-1} + \eta \cdot \nabla_\theta J(\theta)  \newline<br>\theta &amp;&amp;= \theta - v_t \end{split}</p>
<p><img src="http://omdhuynsr.bkt.clouddn.com/17-5-5/39961276-file_1493979742645_3f6b.png" alt><br><img src="http://omdhuynsr.bkt.clouddn.com/17-5-5/49809367-file_1493979972098_105f0.png" alt>  </p>
<p>峡谷区域： 某些方向比另一些方向上陡峭的多，常见于局部极值点<br>在这种情况下，会出现一个方向更新缓慢（如图中x轴），一个方向更新剧烈震荡（如y轴）。<br>使用momentum后，x轴方向可以叠加历史效应使得变化便快，而y轴方向则由于前后梯度方向相反，会发生抑制，不再剧烈震荡。可以参考右图理解。</p>
<h2 id="NAG"><a href="#NAG" class="headerlink" title="NAG"></a>NAG</h2><p>NAG是对momentum的改进。<br>\begin{split} v_t &amp;&amp;= \gamma v_{t-1} + \eta \cdot \nabla J(\theta - \gamma v_{t-1})  \newline<br>\theta &amp;&amp;= \theta - v_t \end{split}<br>可以看到，差异在于当前梯度的计算上。<br>正常情况下就是算当前参数位置的梯度。但是，NAG算的却是近似未来位置的梯度。因为我们知道，参数的更新有一部分就是根据momentum作出的。这里提前将参数位置提前做了momentum这一部分变化，并计算对应梯度。<br>为什么这样更好呢？</p>
<p>NAG用到了近似二阶导信息，详见 <a href="http://www.360doc.com/content/16/1010/08/36492363_597225745.shtml" target="_blank" rel="noopener">比Momentum更快：揭开Nesterov Accelerated Gradient的真面目</a></p>
<h2 id="Adagrad"><a href="#Adagrad" class="headerlink" title="Adagrad"></a>Adagrad</h2><p>Adagrad可以对每个参数自适应不同的学习速率，对稀疏特征，得到大的学习更系，对非稀疏特征，得到较小的学习更新，因此该算法适合处理稀疏特征数据。  </p>
<p>其参数更新方程如下：<br>\begin{split} &amp;&amp;g_{t,i} = \nabla_{\theta_i} (\theta_i) \newline<br>&amp;&amp;\theta_{t+1,i} = \theta_{t,i} - \frac{\eta}{\sqrt{G_{t,ii} + \epsilon}} \cdot g_{t,i}\newline<br>&amp;&amp;G_t \text{为一个对角矩阵，对角元素 } e_{ii} \text{ 为过去到当前第 i 个参数 } \theta_i \text{ 的梯度平方和} \newline<br>&amp;&amp;\epsilon \text{是一个平滑参数，为了使分母不为0} \newline<br>&amp;&amp;\text{如果分母不开根号，算法性能会很糟糕} \end{split}</p>
<p>需要注意，前期很可能先起到一个梯度放大的作用！而非梯度从一开始就不断的衰减！  </p>
<p>Adagrad 存在学习速率衰减过快的问题。因此，深度学习中可能会造成训练提前结束。</p>
<h2 id="RMSProp"><a href="#RMSProp" class="headerlink" title="RMSProp"></a>RMSProp</h2><p>\begin{split} &amp;&amp;g = \nabla J(\theta) \newline<br>&amp;&amp;r = \rho r + (1- \rho) \cdot (g \odot g) \newline<br>&amp;&amp; \theta = \theta - \frac{\eta}{\epsilon + \sqrt{r}} \odot g \newline<br>&amp;&amp; \text{注： 此处为向量写法， } \odot \text{为点乘} \end{split}</p>
<p>RMSProp在Adagrad基础上引入了动量思想,并由于分母不再是以前所有时刻的梯度平方和，减小了衰减速度。</p>
<p>同样的，也可以进一步引入NAG方法，只需要在梯度计算上做改进，同时引入动量。</p>
<p>\begin{split} &amp;&amp;\theta’ = \theta - \alpha v \newline<br>&amp;&amp;g = \nabla J(\theta’) \newline<br>&amp;&amp;r = \rho r + (1- \rho) \cdot (g \odot g) \newline<br>&amp;&amp;v = \alpha v + \frac{\eta}{\epsilon + \sqrt{r}} \odot g \newline<br>&amp;&amp; \theta = \theta - v \newline<br>&amp;&amp; \text{注： 此处为向量写法， } \odot \text{为点乘} \end{split}</p>
<h2 id="Adadelta"><a href="#Adadelta" class="headerlink" title="Adadelta"></a>Adadelta</h2><p>Adagrad存在三个问题：</p>
<ol>
<li>其学习率单调递减，训练后期学习率非常小</li>
<li>其需要手工设置一个全局的初始学习率</li>
<li>更新参数时，左右两边单位不同一</li>
</ol>
<p>针对第一个问题，RMSprop可以解决。</p>
<p>针对第三个问题，我们来讨论一下。</p>
<p>对于一般的sgd方法：<br>\begin{split} unit(\Delta \theta)  \propto unit(\nabla J(\theta)) \propto  \frac{\partial{J}}{\partial{\theta}} \propto \frac{1}{unit(\theta)} \end{split}</p>
<p>类似的，在Adagrad中：<br>\begin{split} unit(\Delta \theta)  \propto 1 \end{split}</p>
<p>在牛顿法中：<br>\begin{split} unit(\Delta \theta)  \propto H^{-1} \nabla J(\theta) \propto  \frac{\partial{J}/ \partial{\theta}}{\partial^2{J}/ \partial{\theta^2}} \propto unit(\theta) \end{split}</p>
<p>可以看到，牛顿法的更新单位是一致的。</p>
<p>因此，可以仿牛顿法进行构造：</p>
<p>\begin{split} g_t &amp;&amp;= \nabla J(\theta_t) \newline<br>\Delta \theta_t &amp;&amp;= - \frac{\sqrt{E[\Delta \theta^2]_{t-1}}}{\sqrt{E[g^2]_t}+\epsilon} g_t \newline<br>E[g^2]_t &amp;&amp;= \rho E[g^2]_{t-1} + (1 - \rho) g_t^2 \newline<br>E[\Delta \theta^2]_t &amp;&amp;= \rho E[\Delta \theta^2]_{t-1} + (1 - \rho) \Delta \theta^2 \newline<br>\theta_t &amp;&amp;= \theta_{t-1} + \Delta \theta_t<br>\end{split}</p>
<p>如此一来，第三个问题就得到了解决，并且顺带解决了第二个问题。<br>为何如此仿造是合理的，参<a href="http://www.cnblogs.com/neopenx/p/4768388.html" target="_blank" rel="noopener">自适应学习率调整：AdaDelta</a>  </p>
<p>总结一下：<br>Adagrad进行了学习率衰减，RMSProp优化了Adagrad衰减过快的问题，Adadelta进一步利用了近似二阶导数信息优化了RMSProp。</p>
<h2 id="Adam"><a href="#Adam" class="headerlink" title="Adam"></a>Adam</h2><p>\begin{split} g &amp;&amp;= \nabla J(\theta) \newline<br>s &amp;&amp;= \rho_1 s + (1 - \rho_1) g \newline<br>r &amp;&amp;= \rho_2 r + (1 - \rho_2) g^2 \newline<br>\hat{s} &amp;&amp;= \frac{s}{1 - \rho_1} \newline<br>\hat{r} &amp;&amp;= \frac{r}{1 - \rho_2} \newline<br>\Delta \theta &amp;&amp;= - \eta \frac{\hat{s}}{\sqrt{\hat{r}}+\epsilon} \newline<br>\theta &amp;&amp;= \theta + \Delta \theta \end{split}</p>
<h1 id="二阶方法"><a href="#二阶方法" class="headerlink" title="二阶方法"></a>二阶方法</h1><h2 id="牛顿法"><a href="#牛顿法" class="headerlink" title="牛顿法"></a>牛顿法</h2><p>\begin{split} &amp;&amp;f(x) \approx f(x_k) + f’(x_k)(x-x_k) + \frac{1}{2} f’’(x_k)(x-x_k)^2 = \varphi (x)\newline<br> \text{在x处取极值时满足： } &amp;&amp;\varphi’(x) = 0 \newline<br> \text{推出： } &amp;&amp;f’(x_k) + f’’(x_k)(x-x_k) = 0 \newline<br> &amp;&amp;x = x_k - \frac{f’(x_k)}{f’’(x_k)} \newline<br> &amp;&amp; x_{k+1} = x_k - \frac{f’(x_k)}{f’’(x_k)}, k =0,1,… \end{split}</p>
<p>多维情况下：<br>\begin{split} &amp;&amp;f(x) \approx f(x_k) + \nabla f(x_k)(x-x_k) + \frac{1}{2}(x-x_k)^T \cdot \nabla^2 f(x_k) \cdot (x-x_k) = \varphi (x)\newline<br> \text{在x处取极值时满足： } &amp;&amp;\nabla \varphi(x) = 0 \newline<br> \text{推出： } &amp;&amp;g_k + H_k \cdot (x-x_k) = 0 \newline<br> &amp;&amp;x = x_k - H_k^{-1} \cdot g_k \newline<br> &amp;&amp; x_{k+1} = x_k -  H_k^{-1} \cdot g_k, k =0,1,… \newline<br> &amp;&amp; g = \nabla, H = \nabla^2 \end{split}</p>
<p>牛顿法利用二阶泰勒展开自动选择更新方向。<br>当目标是二次函数时，由于二次泰勒展开函数与原目标函数不是近似而是完全相同的二次式，Hessian矩阵退化成一个常数矩阵，从任一初始点出发，只需要一步迭代即可达到最小值点。对于非二次函数，若函数的二次性态较强，或迭代点以进入极小值的领域，则其收敛速度也是很快的。<br>但对于非二次型目标函数，有时会使函数值上升，不能保证函数值稳定下降，甚至可能发散。因为在极大值处也可以一阶导数为0。<br>以及可能会陷入局部极小值中无法跳出。</p>
<p>牛顿法存在连个主要缺点：</p>
<ol>
<li>对目标函数有较严格的要求。函数必须具有连续的一、二阶偏导数，Hessian矩阵必须正定;</li>
<li>计算复杂。计算量、存储量均很大，且均以维数N的平方比增加。</li>
</ol>
<h2 id="阻尼牛顿法"><a href="#阻尼牛顿法" class="headerlink" title="阻尼牛顿法"></a>阻尼牛顿法</h2><p>阻尼牛顿法在牛顿法的基础上引入了步长：<br>\begin{split} &amp;&amp;d_k = - H_k^{-1} \cdot g_k \newline<br>&amp;&amp;\lambda_k = \arg \min_{\lambda \in R} f(x_k + \lambda d_k) \newline<br>&amp;&amp;x_{k+1} = x_k + \lambda_k d_k \newline\end{split}</p>
<p>前面说到 $f(x_{k+1}) &gt; f(x_k))$ 的情况可能发生，因此，我们可以引入步长因子，进行一维搜索，使得更新后得到的 $f(x_{k+1})$ 必然不会变大，从而一直沿着正确的方向前进。</p>
<p>一维搜索可以使用进退法寻找到单谷所在区间，然后使用黄金分割法、评分法等在区间内找寻最小值。（理想情况下是单谷，但即使不是单谷，也并不会影响更新方向）</p>
<h2 id="拟牛顿法"><a href="#拟牛顿法" class="headerlink" title="拟牛顿法"></a>拟牛顿法</h2><p>这个方法的基本思想是：不用二阶偏导数而构造出可以近似Hessian矩阵（或其逆）的正定对称矩阵，在“拟牛顿”的条件下优化目标函数。<br>为明确起见，用B表示对H的近似，用D表示对H的逆的近似。</p>
<p>对H作出近似，需要满足拟牛顿条件，下面推导之。</p>
<p>\begin{split} &amp;&amp;f(x) \approx f(x_{k+1}) +  \nabla f(x_{k+1}) \cdot (x - x_{k+1}) + \frac{1}{2} \cdot (x - x_{k+1})^T \cdot \nabla^2 f(x_{k+1}) \cdot (x - x_{k+1}) \newline<br>\text{两边同时对x求导: } &amp;&amp;\nabla f(x) \approx \nabla f(x_{k+1}) + H_{k+1} \cdot (x - x_{k+1}) \newline<br>\text{令} x = x_k \text{整理得: }  &amp;&amp;g_{k+1} - g_k \approx H_{k+1} \cdot (x_{k+1} - x_k) \newline<br>\text{引入记号: } &amp;&amp;s_k = x_{k+1} - x_k,y_k = g_{k+1} - g_k \newline<br>\text{有: } &amp;&amp;y_k \approx H_{k+1} \cdot s_k \newline<br>\text{或: } &amp;&amp;s_k \approx H^{-1}_{k+1} \cdot y_k \end{split}</p>
<p>这就是拟牛顿条件，他对迭代过程中的 $H_{k+1}$ 作出了约束。因此，作为近似的 $B_{k+1}$ 和 $D_{k+1}$ 可以将以下公式作为指导。<br>\begin{split} &amp;&amp;y_k = B_{k+1} \cdot s_k \newline<br>&amp;&amp;s_k = D_{k+1} \cdot y_k \end{split}</p>
<p>根据H的构造函数不同，可以分为不同的拟牛顿法。下面介绍。</p>
<h2 id="DFP法"><a href="#DFP法" class="headerlink" title="DFP法"></a>DFP法</h2><p>该算法的核心是：通过迭代的方法，对H的逆做近似：<br>\begin{split} D_{k+1} = D_k + \Delta D_k, k = 0,1,2,……, D_0 = I \end{split}</p>
<p>显然，关键在于 $\Delta D_k$ 的构造，很容易猜想其可能会和 $s_k,y_k,D_k$ 发生关联。这里，我们采用待定法，即首先将其待定为某种形式，然后结合拟牛顿条件来推导。</p>
<p>可以将其待定为：<br>\begin{split} \Delta D_k = \alpha \mathbf{uu}^T +\beta \mathbf{vv}^T \end{split}<br>从形式上看，至少保证了其对称性。</p>
<p>将之代入拟牛顿条件：<br>\begin{split} \mathbf{s}_k &amp;&amp;= D_k \mathbf{y}_k + \alpha \mathbf{uu}^T \mathbf{y}_k + \beta \mathbf{vv}^T \mathbf{y}_k \newline<br>&amp;&amp;= D_k \mathbf{y}_k + (\alpha \mathbf{u}^T \mathbf{y}_k) \mathbf{u} + (\beta \mathbf{v}^T \mathbf{y}_k) \mathbf{v} \newline<br>\text{不妨令: } &amp;&amp;\alpha \mathbf{u}^T \mathbf{y}_k = 1, \beta \mathbf{v}^T \mathbf{y}_k = -1 \newline<br>\text{得: } &amp;&amp;\alpha = \frac{1}{\mathbf{u}^T \mathbf{y}_k}, \beta = - \frac{1}{\mathbf{v}^T \mathbf{y}_k} \newline \newline<br>&amp;&amp;\mathbf{u} - \mathbf{v} = \mathbf{s}_k - D_k \mathbf{y}_k \newline<br>\text{为使上式成立，不妨直接令: } &amp;&amp;\mathbf{u} = \mathbf{s}_k, \mathbf{v} = D_k \mathbf{y}_k \newline<br>\text{代入有: } &amp;&amp;\alpha = \frac{1}{\mathbf{s}_k^T \mathbf{y}_k}, \beta = - \frac{1}{\mathbf{y}_k^T D_k \mathbf{Y}_k} \text{ 其中第二个等式用到了对称性} D_k^T = D_k \newline<br>&amp;&amp;\Delta D_k = \frac{\mathbf{s}_k\mathbf{s}_k^T}{\mathbf{s}_k^T \mathbf{y}_k} - \frac{D_k \mathbf{y}_k \mathbf{y}_k^T D_k}{\mathbf{y}_k^T D_k \mathbf{y}_k} \end{split}</p>
<p>由此可以利用近似的H运用牛顿法。</p>
<h2 id="BFGS法"><a href="#BFGS法" class="headerlink" title="BFGS法"></a>BFGS法</h2><p>BFGS则是通过迭代的方法，直接对H做近似。其形式和思路和DFP基本相同。最后再去求近似H的逆运用牛顿法即可。</p>
<p>另外，除了前面提到的一维搜索来计算步长之外，还有其他搜索方法。Powell证明了带Wolfe搜索的BFGS算法的全局收敛性和超线性收敛性。</p>
<p>具体见 <a href="http://blog.csdn.net/itplus/article/details/21897443" target="_blank" rel="noopener">牛顿法与拟牛顿法学习笔记（四）BFGS 算法</a></p>
<h2 id="L-BFGS法"><a href="#L-BFGS法" class="headerlink" title="L-BFGS法"></a>L-BFGS法</h2><p>考虑一个N阶矩阵，当N=10万时，用double（8字节）来存储该矩阵将需要74.5GB内存。其内存开销非常大！</p>
<p>因此，L-BFGS 对 BFGS 进行了近似，可以减小内存开销。其基本思想是： 不再存储完整的矩阵 $D_k$ ,而是存储计算过程中的向量序列 ${s_i},{y_i}$ ，需要矩阵 $D_k$ 时，利用向量序列的计算来代替。而且，向量序列不是所有的都存，而是固定存最新的m个。每次计算 $D_k$ 时，只利用最新的m个来进行近似计算。</p>
<p>具体见<a href="http://blog.csdn.net/itplus/article/details/21897715" target="_blank" rel="noopener">牛顿法与拟牛顿法学习笔记（五）L-BFGS 算法</a></p>
<h1 id="其他策略"><a href="#其他策略" class="headerlink" title="其他策略"></a>其他策略</h1><h2 id="Shuffling-and-Curriculum-Learning"><a href="#Shuffling-and-Curriculum-Learning" class="headerlink" title="Shuffling and Curriculum Learning"></a>Shuffling and Curriculum Learning</h2><p>为了使得学习过程更加无偏，可以打乱训练样本。  </p>
<p>另一方面，可以将训练集按照某个有意义的顺序排列一次来提高模型性能和SGD收敛性，如何建立有意义的排列成为Curriculum Learning。</p>
<h2 id="Batch-normalization"><a href="#Batch-normalization" class="headerlink" title="Batch normalization"></a>Batch normalization</h2><p>\begin{split} \mu &amp;&amp;= \frac{1}{m} \sum_{i=1}^m x_i \newline<br>\sigma &amp;&amp;= \frac{1}{m} \sum_{i=1}^m (x_i - \mu)^2 \newline<br>\hat{x_i} &amp;&amp;= \frac{x_i - \mu}{\sqrt{\sigma^2 + \epsilon}} \newline<br>y_i &amp;&amp;= \gamma \hat{x_i} + \beta \equiv BN_{\gamma,\beta}(x_i) \end{split}</p>
<p>BN方法对需要层的输出做z标准化，然后再训练参数进行还原，通过这种方式来保存原输出数据的分布特征。  </p>
<p>在原paper中作者建议BN放在激活函数之前，但也有实验表明放在激活值后更好。<br>[TBC]为什么经过这样的变化之后会更好？并解决梯度消失问题？Batch Normalization: Accelerating Deep Network Training by  Reducing Internal Covariate Shift</p>
<h2 id="LRN"><a href="#LRN" class="headerlink" title="LRN"></a>LRN</h2><p>[TBC]</p>
<h2 id="Early-stopping"><a href="#Early-stopping" class="headerlink" title="Early stopping"></a>Early stopping</h2><p>当验证集效果不再显著提升时，提前结束训练。</p>
<h2 id="Gradient-noise"><a href="#Gradient-noise" class="headerlink" title="Gradient noise"></a>Gradient noise</h2><p>对于计算出的梯度增加一个高斯分布的随机误差，即：<br>\begin{split} g_{t,i} &amp;&amp;= g_{t,i} + N(0,\sigma_t^2) \newline<br>\sigma_t^2 &amp;&amp;= \frac{\eta}{(1+t)^\gamma} \end{split}</p>
<p>对梯度增加误差可以增加鲁棒性，即使初始参数值选择不好。<br>并且适合对深层次网络进行你个训练。原因在于增加随机噪声会有更多的可能性跳过局部极值点并去寻找一个更好的局部极值点，这种可能性在深层次网络中更常见。</p>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><p><a href="http://sebastianruder.com/optimizing-gradient-descent/" target="_blank" rel="noopener">An overview of gradient descent optimization algorithms</a><br><a href="http://blog.csdn.net/heyongluoyao8/article/details/52478715" target="_blank" rel="noopener">梯度下降优化算法综述</a><br><a href="https://zhuanlan.zhihu.com/p/21486826" target="_blank" rel="noopener">路遥知马力——Momentum</a><br><a href="http://www.360doc.com/content/16/1010/08/36492363_597225745.shtml" target="_blank" rel="noopener">比Momentum更快：揭开Nesterov Accelerated Gradient的真面目</a><br><a href="http://blog.csdn.net/u014595019/article/details/52989301" target="_blank" rel="noopener">深度学习笔记：优化方法总结(BGD,SGD,Momentum,AdaGrad,RMSProp,Adam)</a><br><a href="http://climin.readthedocs.io/en/latest/rmsprop.html" target="_blank" rel="noopener">rmsprop</a><br><a href="http://www.cnblogs.com/neopenx/p/4768388.html" target="_blank" rel="noopener">自适应学习率调整：AdaDelta</a><br><a href="http://blog.csdn.net/luo123n/article/details/48239963" target="_blank" rel="noopener">各种优化方法总结比较（sgd/momentum/Nesterov/adagrad/adadelta）</a><br><a href="http://blog.csdn.net/elaine_bao/article/details/50890491" target="_blank" rel="noopener">解读Batch Normalization</a><br><a href="http://blog.csdn.net/itplus/article/details/21896453" target="_blank" rel="noopener">牛顿法与拟牛顿法学习笔记（一）牛顿法</a><br><a href="http://dataunion.org/20714.html" target="_blank" rel="noopener"><strong><em>寻找最优参数解：最速下降法，牛顿下降法，阻尼牛顿法，拟牛顿法DFP/BFGS</em></strong></a><br><a href="https://wenku.baidu.com/view/22138d22bcd126fff7050b99.html" target="_blank" rel="noopener">一维搜索</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2019/02/28/pansee/book review/《书中之书讲演录》读书杂感系列·第五讲至第十二讲/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/02/28/pansee/book review/《书中之书讲演录》读书杂感系列·第五讲至第十二讲/" class="post-title-link" itemprop="url">《书中之书讲演录》读书杂感系列·第五讲至第十二讲</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-02-28 20:45:06" itemprop="dateCreated datePublished" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/book-review/" itemprop="url" rel="index"><span itemprop="name">book review</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>第五讲至第十二讲中多包涵知识性论述，故而此部分我以笔记为主，杂以思感。</p>
<h1 id="第五讲：打开《圣经》之匙：约与赎"><a href="#第五讲：打开《圣经》之匙：约与赎" class="headerlink" title="第五讲：打开《圣经》之匙：约与赎"></a>第五讲：打开《圣经》之匙：约与赎</h1><p>从圣约与救赎的视角对《圣经》的整体性解读：<br>生命之约  上帝与第一个亚当（人类始祖） 背约（吞食禁果） 骄傲、自我中心、罪、死亡<br>恩典之约（旧约（以色列民）、新约（信心为本者），永恒的救赎之约）  上帝与末后的亚当（耶稣基督） 道成肉身（代表资格）  因信称义  </p>
<p>“上帝允许亚当有背叛和堕落的自由，因为亚当若没有堕落的自由，那他对上帝的顺服和爱也是假的，因为没有经过他自己的选择。”（书60页）  </p>
<p>“他最根本的问题是骄傲和自我为中心，人类最大的罪就是骄傲，而骄傲是原罪的最切实的体现。”（书61页）  </p>
<h1 id="第六讲：从《创世纪》到“十诫”"><a href="#第六讲：从《创世纪》到“十诫”" class="headerlink" title="第六讲：从《创世纪》到“十诫”"></a>第六讲：从《创世纪》到“十诫”</h1><p>“人其实都知道有神，即使他没有神他也一定要制造神来崇拜，他是不可能忍受真正无神的生活。”（书81页）</p>
<p>前文已经谈到了虚无主义问题。对一个非虚无主义者，活着就必须要遵循一定的准则与价值。而此准则若是由人本身来制定，就缺失了绝对权威。故而，必须有一个外在的神来定下一个道德律。一个无神的生活，一个缺乏准则的生活，人将在其中迷失，流浪。</p>
<p>对于虚无主义者，当你们在陈述世界及人的存在的无价值无目的时何不反问一句，虚无何以证明其自身的虚无呢？也就是一个无穷后退的逻辑。当然，最终可以归结为一个虚无的虚无，也就是不可再论的恒常的空，但这空本身也是一个绝对准则，而非是一般的所谓虚无。</p>
<p>至于何以选择上帝而不选择空，这已经不是理性所能论述的。不妨回到事实上来，事实告诉世人有过那么一个死而复活的耶稣基督，有过一个行诸般神迹的耶稣基督，有过这么一个非人力所能及的耶稣基督，既然有这样明朗的事实，何以不去选择信仰上帝呢？</p>
<p>或许有人会反问，难道释迦摩尼不是真实存在的么，佛不是真实存在的一种境界么？是的，在我一开始的思考中，存在这样一个误区，无意之中将神的地位拔高了，故而才能在上帝和佛之间选择出上帝。承认耶稣的死而复活不过是确证了对上帝的信仰是对人终极问题的一个合理解答罢了。那么佛和上帝的地位是同等的么？不妨进行一下上帝和佛的对比：</p>
<ol>
<li>基督教通过耶稣复活确立上帝的合理性，而耶稣复活则是在一定预设下可以接受证实和证伪的；释迦摩尼是否达到那样的境界既不可知（在他人眼中的达到与否根本不具有意义，唯其自身知道）也无知的必要，佛的境界是否能够达到必须归结到自身的体认。</li>
<li>对上帝的信仰可以立时达到，具有简单性（排除美学的和实用的，波普尔证伪主义意义上的）的特点；佛的境界耗时耗力，甚至于苦参一生而只余苦。</li>
</ol>
<p>可以看出，信仰上帝类似于一条合理的捷径，而修佛则是前途难测的攀登。上帝所包含的信息与真实较之佛的境界更大，更确实。但对于一个负隅顽抗者而言，这样的量的区别怕是还不足以让其承认上帝的独一性。我想，问题的关键根本不在于在<br>上帝和佛之间做出选择，因为不管选择哪个都已经承认了两者具有同等的地位，至少是抹杀了上帝的威权。问题的核心在于，必须证明上帝是高于佛的，上帝是独一的，根本不该存在选择问题。否则那样一个上帝也不过是人自造的上帝罢了。<br>上帝和佛是矛盾的么？还是说，佛只是上帝智慧的一个映射？如果上帝包容佛，那么上帝就是独一的了，也是我该信的了！这样的对比有教内言教的嫌疑，怕是没有太大意义。</p>
<p>没有死而复活的耶稣的上帝和佛已然是对等的了，加入了耶稣死而复活这一筹码的上帝确实更具有诱惑力。但是，上帝代表的是目的性，佛代表的是无目的性，人在考虑的时候存在肯定力量与否定力量的不对等，但是原初的事实到底如何呢？这实在无法用人的视角去解读。当我带着迫切的寻求归属的目的去查考上帝时，势必带有自身的偏见，从而无法见到真实的上帝。</p>
<p>还是回到事实吧！一个混沌原初的人，一个不带主观偏见的人，当他看到了一系列悖逆规则的事，当他看到了一个死而复活的耶稣基督，但仅仅是一个叫做耶稣的人死而复活还不够，必须是一个作为传道者和规则建立者的耶稣基督的死而复活，如此来，他有什么理由不相信基督是真实存在的规则的创造者呢？！！！而佛从头到尾也仅仅是揭露规则罢了。</p>
<p>为什么一定需要一个规则的创造者呢？为什么一定要有规则呢？当然不必须有，但又为什么不能有呢？问题的核心不在于规则，应该做的是“回归事实”。</p>
<p>或许又要质疑了，何以见得事实便是真的事实（这一事实是本质意义上的，不是事件发生意义上的）呢？肉眼所见都可能不过是虚假的表象而已。依旧是那样的思路，是不是本质的事实并不重要，因为事实可能是虚假，也可能是真实。因此只需要“回归事实”（这里的事实既非本质意义上的也非事件发生意义上的）就好了！</p>
<p>但必然又有人要质疑了，那样的“回归事实”有多大的可靠性呢？我自己在这个问题上都未必能坚定的如此解释：这样的“回归事实”就好像佛家所谓真空妙有的空的境界一样，是只可意会不可言传的。但至少我目前是如此理解我所谓的“回归事实”的。</p>
<p>另外，我以为，信仰可以和怀疑并行，在虔诚信仰的同时也可保有理性。如此一来，当有证据证明上帝是个骗局，也就可以清醒地脱身，方不为盲信。</p>
<h1 id="第七讲：“诗歌智慧书”中的大智慧"><a href="#第七讲：“诗歌智慧书”中的大智慧" class="headerlink" title="第七讲：“诗歌智慧书”中的大智慧"></a>第七讲：“诗歌智慧书”中的大智慧</h1><h2 id="《约伯记》"><a href="#《约伯记》" class="headerlink" title="《约伯记》"></a>《约伯记》</h2><p>“这种无缘无故就使得信仰回到了一种单纯而非功利的关系。”（书95页）</p>
<p>“这就是因着痛苦学习顺服进入了完全的道路，好回归到非功利的信仰，与上帝建立一种真纯关系，所以苦难就成了一种考验和一种试炼。”（书95页）</p>
<p>“基督教强调的是对苦难的一种承担，所以信仰呢，不是解释苦难，而是承担苦难。”（书96页）</p>
<h2 id="《诗篇》"><a href="#《诗篇》" class="headerlink" title="《诗篇》"></a>《诗篇》</h2><p>神性哀伤传统与欢歌传统</p>
<p>诗人对苦难的担当</p>
<h2 id="《箴言》"><a href="#《箴言》" class="headerlink" title="《箴言》"></a>《箴言》</h2><p>“所以真正的学习者，他一定是一个谦卑的人，而不是借着学习来满足自我的人，他是一个借着学习来敬畏造物主的人，他的内心和头脑是开放的，而非封闭的。”（书101页）</p>
<p>“如果想不清楚，我相信我们对于自由的所有探讨一定会被任性给吞吃掉，我们所有的自由变成了我们的一种臆想，我想干什么就干什么就成了所谓自由的本质。”（书103页）</p>
<h2 id="《传道书》"><a href="#《传道书》" class="headerlink" title="《传道书》"></a>《传道书》</h2><p>“因为自由意味着我想干什么就干什么，于是他就陷入了自由的不自由。”（书103页）</p>
<p>“知识是灰色的，而生活之树长青！”（书105页）</p>
<h1 id="第八讲：问世间，情是何物"><a href="#第八讲：问世间，情是何物" class="headerlink" title="第八讲：问世间，情是何物"></a>第八讲：问世间，情是何物</h1><p>此讲主要论及两个问题，两性关系与爱情。</p>
<p>基督教的爱：没有条件的，完全接纳的爱。爱先于知识</p>
<p>爱是克服我的自我为中心。</p>
<p>谈恋爱：呼唤与应答的模式</p>
<p>一夫一妻制，男女的地位平等，秩序差别。男人需要尊重，女人需要爱。</p>
<p>以上是此讲所具体论及的内容，个人不能完全认同，具体分析参见对弗洛姆《爱的艺术》的书评，此处不再赘述。</p>
<h1 id="第九讲：从“历史书”、“先知书”到“福音书”"><a href="#第九讲：从“历史书”、“先知书”到“福音书”" class="headerlink" title="第九讲：从“历史书”、“先知书”到“福音书”"></a>第九讲：从“历史书”、“先知书”到“福音书”</h1><p>历史背景：罗马和平时期</p>
<p>耶稣时代的文化传统：</p>
<p>犹太文化（对律法的重视和对末世论的普遍期待）</p>
<p>希罗背景（两个危机：真理在理念界还是现象界、理性跟真理的矛盾）</p>
<h1 id="第十讲：“福音书”研读"><a href="#第十讲：“福音书”研读" class="headerlink" title="第十讲：“福音书”研读"></a>第十讲：“福音书”研读</h1><p>“信心的对象比信心本身更重要。”（书148页）</p>
<p>用一个诡辩的逻辑，人既然能活下去，总是有信心的，哪怕一个宣称没有信心的人，在他的潜意识中也藏匿着一个信心。所以，重要的不是有没有信心，有没有绝对准则，而是那绝对准则是什么。</p>
<p>“听了道之后还要有领受，跟生命有融合，而且结实。”（书155页）</p>
<p>“在世而不靠世”</p>
<h1 id="第十一讲：《使徒行传》与“新约书信”"><a href="#第十一讲：《使徒行传》与“新约书信”" class="headerlink" title="第十一讲：《使徒行传》与“新约书信”"></a>第十一讲：《使徒行传》与“新约书信”</h1><p>“如果你想更深爱上帝，那就要更深爱人。这不是一种关门修炼的方式，而是敞开自我，在与真理和人们交往中形成一个全新自我的观念。”（书158页）</p>
<p>耶稣升天后基督教会的发展过程。</p>
<p>“因信称义”</p>
<p>“人性受到了罪全面的玷污和影响，人不可能自我拯救，所以必须要依靠外在的拯救者——耶稣基督的牺牲救赎。”“但对于基督教来说，行善是得救的结果。”（书166页）</p>
<p>“对很多人来说，不是该怎样生活的问题，而是为什么生活的问题，后者解决了，前者也就迎刃而解。”（书173页）</p>
<h1 id="第十二讲：真有世界末日吗？"><a href="#第十二讲：真有世界末日吗？" class="headerlink" title="第十二讲：真有世界末日吗？"></a>第十二讲：真有世界末日吗？</h1><p>“天堂只是信仰的结果，而不是信仰的动机和目的。”（书188页）</p>
<p>末日，短暂，永恒。人类对美好生活和永恒境界的向往。</p>
<p>但这里需要反问一下，何以见得永恒就是好的呢？故而，哪怕永恒并不真正可以成为信仰的动力。而不过是信仰的结果而已。信仰源自上帝本身。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2019/02/28/pansee/book review/健全的社会·爱的艺术/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/02/28/pansee/book review/健全的社会·爱的艺术/" class="post-title-link" itemprop="url">健全的社会·爱的艺术</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-02-28 20:45:06" itemprop="dateCreated datePublished" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/book-review/" itemprop="url" rel="index"><span itemprop="name">book review</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>此文本该是对《爱的艺术》的书评，但关于爱这一超越方式的推导在《健全的社会》前半部分有更详尽以及从社会视角入手的描述，故而两书合论。而《健全的社会》后半部分内容此文不再涉及，将放在与《逃避自由》一书的合论中。</p>
<p>正常与反常的分界，文明与疯癫的分界是什么呢？一般情况下总是以大多数人的状况也即是社会作为参考系来考察一个人的状况。那么，首先面对的问题就是，社会是什么？社会与个人的关系是怎样的？这里不讨论关于社会属性的问题。直接跳过看第二个问题，与社会相符就是正常，不符就是反常么？</p>
<p>“数百万人都有同样的恶习，这并不能把恶习变成美德；数百万人都犯了同样的错误，这并不能把错误变成真理；数百万人都患有同样的精神疾病，这并不能使这些人变成健全的人。”（《健全的社会》10页）弗洛姆通过自杀、他杀、酒精中毒等数据敏锐的查知了“社会作为一个整体可能缺乏健全的精神……文化本身可能存在的不适应情况”（《健全的社会》3页）。</p>
<p>既然如此，原本依凭的社会参考系出了问题，新的参考系在哪里呢？弗洛姆由此提出了人本主义精神分析。“标准人本主义的观点是基于这样一种假设：就像其他问题一样，人的存在问题的解决办法也有正确的错误的，令人满意的和令人不满意的。如果依照人性的特征和规律发展至成熟，他的精神就会是健康的。精神疾病的发生即由于没能实现这种发展。”（《健全的社会》10页）</p>
<p>“真正的问题是从人性的无数表现形式——包括正常的和病态的形式中——推断出整个人类共有的根本的东西。”“人创造了他自己。但是，正像他只能按照自然物质的性质来改造和改变自然界一样，他也只能按人的本性来改造和改变他自己”（《健全的社会》9页）这里有一个问题，在人未成人之前，人的本性从何而来？人创造了自己，那么这一创造的源动力是属于人的，但这源动力又是被创造出来的（因为此时人尚未成为人），这样至少在语言表述上也就有了一些循环论证的问题。当然，这个问题本身并不重要，更近于文字游戏。我真正是想通过这个讨论推出这样的问题，弗洛姆这里谈到的那个人性中“根本的东西”最终很有可能会归于上帝赋予或者外界环境的塑造，而在一定程度上让真正属人的，或者是自由意志成为后天的东西。当然，也可以说人是在和自然的相互作用下生成，虽然不能撇离自然因素，但也不能沦为被动的环境决定论。关于人性的问题与社会问题一样，在此文中不去讨论，仅仅提及。（顺带一提，“人创造了自己”，可以理解为“人”是共性，“自己”是个性，当然，依旧无法避免上述问题。）</p>
<p>“文化为大多数人提供了行为模式，使它们能够既带着缺陷生活又不会患病。”（《健全的社会》12页）“人性的要求和社会的要求会相互冲突，因而整个社会是会生病的。”（《健全的社会》14页）“一个健全的社会是一个符合人的需要的社会——这里所说的需要并不一定就是人认为他所需要的，因为即使是最病态的目的，也可能被人在主观上认为是最需要的；这里所说的是人类客观的需要，我们可以通过对人的研究明确这些需要。”上面列出了弗洛姆关于社会病理的一些论断，此文不过多涉及。直接来看弗洛姆是怎样分析人的客观需要的。</p>
<p>人本主义精神分析的关键乃是人类状况。从生命有了自我意识开始，“当人——人类的整体以及人类的个体——诞生之时，他就被从一个像本能一样确定的环境中抛入了一个不确定的、无常的和开放的环境中”（《健全的社会》18页）。由此，人类的诞生是一个否定性的事件，是与自然的部分脱离，是“和谐”的消亡。</p>
<p>弗氏由此说“人必须在倒退与前进之间，在退回到动物的生存状态与达到人类的存在状态之间，做出一个选择，人人都逃不脱这种选择。任何倒退的尝试都是痛苦的，都必定导致苦难与精神疾病，导致生理上的或精神上的死亡（精神错乱）。”（《健全的社会》21页）首先，是不是真的要做出选择而不能够原地滞留是个问题。其次，倒退是否痛苦就人这一物种初生之时而言是不可知的，用现代的观点去评判它显然是有问题的，未免先入为主而不够客观。尽管弗氏说“人类个体和种属的历史证明了，进步的倾向比后退的倾向更强有力”（《健全的社会》20页），但语焉不详，这一证明过程也待考。</p>
<p>什么是根源于人类存在的需要和感情呢？弗氏总结了五点：</p>
<h2 id="一、与他者关联和自恋"><a href="#一、与他者关联和自恋" class="headerlink" title="一、与他者关联和自恋"></a>一、与他者关联和自恋</h2><p>关于这点下文结合《爱的艺术》详谈。</p>
<h2 id="二、超越——创造性与破坏性"><a href="#二、超越——创造性与破坏性" class="headerlink" title="二、超越——创造性与破坏性"></a>二、超越——创造性与破坏性</h2><p>“人类状况的另一个方面是人作为被造物却又必须超越这种生存状态。”（《健全的社会》28页）显然，这个结论的得出是基于上面所谈到的倒退与前进之间的选择问题。弗氏认为创造性和破坏性都可以解决这一问题，但破坏性是创造性的替代物，是次一等的解决方法。对此，又关乎到与他者关联的问题，容后详述。</p>
<h2 id="三、根性——友爱与乱伦"><a href="#三、根性——友爱与乱伦" class="headerlink" title="三、根性——友爱与乱伦"></a>三、根性——友爱与乱伦</h2><p>容后详述。</p>
<h2 id="四、身份感——个性与从众行为"><a href="#四、身份感——个性与从众行为" class="headerlink" title="四、身份感——个性与从众行为"></a>四、身份感——个性与从众行为</h2><p>对自我的认知。独立的个人身份感和群体身份感。</p>
<p>“人们宁愿冒生命危险，放弃自己的爱，舍弃自己的自由，牺牲自己的思想，为的就是成为群体中的一员，与群体协调一致，并由此获得一种（即使是虚妄的）身份感。”（《健全的社会》51页）</p>
<h2 id="五、对位坐标系与信仰体系的需要——理性与非理性"><a href="#五、对位坐标系与信仰体系的需要——理性与非理性" class="headerlink" title="五、对位坐标系与信仰体系的需要——理性与非理性"></a>五、对位坐标系与信仰体系的需要——理性与非理性</h2><p>弗氏认为人需要一个坐标系。但人真的需要一个坐标系么？这个反问有两层含义。其一是可以没有坐标系，诸如虚无，相对。其二是坐标系可以随时变化而不固定。</p>
<p>下面主要结合《爱的艺术》来谈。</p>
<p>弗氏是这样推演的，人在和自然脱离之后就处于一个孤寂的状态，这有导致了恐惧，故而人最大的需要就是摆脱孤寂。并批判了共生有机体、纵欲带来的个性丧失，创造性劳动是人与物的统一，最后得出结论说在爱中实现人与人的统一。</p>
<p>整个论述有这样几个问题。</p>
<ol>
<li>孤寂带来恐惧，这是想当然，在逻辑上是不必然的。</li>
<li>人有意识，但是不要妄自在意识前面加上自我两字，更不要妄自推高个人，拔高个性。弗氏恰恰是默认了自我、独立的重要性，才能做出那样的批判。</li>
<li>解决孤寂为什么就必须是人与人的统一而不能是人与物的统一呢？对创造性劳动的批判也是语焉不详。</li>
</ol>
<p>对于弗氏谈到的几种爱的形式，这里就不再抓细枝末节。只细谈一下博爱和性爱。</p>
<p>“博爱的基础是认识到我们所有的人都是平等的。”（《爱的艺术》58页）但是，为什么所有的人都是平等的呢？弗氏说到，“同人共有的核心相比，人与人之间在才能、智力和知识上的差别微不足道。”（《爱的艺术》58页）那么，这个人共有的核心是什么呢？再者，就算人有一个共有的核心又如何呢？有其是在今天这个人类社会不再严重受到自然威胁的情况下，难道才能、智力和知识上明显高于人者有必要因为这共有的核心就去谈平等、博爱么？</p>
<p>博爱的基础应是谦卑！是认识到人的局限性，认识到自我的局限性。一个自以为了解真理，自以为触摸到了最高道德律令的人恰恰是最不懂得博爱的！因为这样的人根本意识不到自身的局限性，内心也就被强烈的自我意识和偏执所占据。在这一方面，耶稣背负十字架是在舍己，而佛学的施舍说的更为透彻。为什么佛家明明知道善恶乃是虚妄，还要谈慈悲呢？佛家的慈悲不是道德观念，而是一种施舍，是抛去我执的一种修行方法。只有放下我执，才可能无分别心，才可能博爱。</p>
<p>那么，一个真正懂得爱的人应该如何行呢？不妨看看新约·哥林多前书8:9-13：“只是你们要谨慎，恐怕你们这自由竟成了那软弱人的绊脚石。若有人见你这有知识的在偶像的庙里坐席，这人的良心若是软弱，岂不放胆去吃那祭偶像之物吗？因此，基督为他死的那软弱弟兄，也就因你的知识沉沦了。你们这样得罪弟兄们，伤了他们软弱的良心，就是得罪基督。所以，食物若叫我弟兄跌倒，我就永远不吃肉，免得叫我弟兄跌倒了。”</p>
<p>一个博爱的人应该是谦卑的，是认识到自身的局限和不足的，是如和风细雨一样的慈悲的，凡是过度强调道德和正义的都有走向专制主义的潜在威胁。</p>
<p>再来看看性爱。我本来是反对一夫一妻制的，也是反对固定配偶的。真正的爱情应当是自由的，哪怕滥交，群交。我也不觉得应该把爱情和责任捆在一起来说，这是两个概念。或者说，我从根本上是要消解性爱这一特殊形式，包括其他有专属性对象的爱情形式，而只需要保留博爱。至于性关系的发生不过是双方看得顺眼有共同话题乐意发生而已。但这个看法应该是有问题的。对于这种特定对象化的爱情形式，我尚未考虑清楚，悬置不表。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2019/02/28/pansee/book review/故事与讲故事——《小说的艺术》《天真的和感伤的小说家》/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/02/28/pansee/book review/故事与讲故事——《小说的艺术》《天真的和感伤的小说家》/" class="post-title-link" itemprop="url">故事与讲故事——《小说的艺术》《天真的和感伤的小说家》</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-02-28 20:45:06" itemprop="dateCreated datePublished" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/book-review/" itemprop="url" rel="index"><span itemprop="name">book review</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="昆德拉《小说的艺术》、帕慕克《天真的和感伤的小说家》读书笔记"><a href="#昆德拉《小说的艺术》、帕慕克《天真的和感伤的小说家》读书笔记" class="headerlink" title="昆德拉《小说的艺术》、帕慕克《天真的和感伤的小说家》读书笔记"></a>昆德拉《小说的艺术》、帕慕克《天真的和感伤的小说家》读书笔记</h1><h2 id="一、故事与小说的差异"><a href="#一、故事与小说的差异" class="headerlink" title="一、故事与小说的差异"></a>一、故事与小说的差异</h2><p>1、 作品主体上：</p>
<ul>
<li><p>“作者已死”</p>
</li>
<li><p>作者与读者的博弈</p>
</li>
<li><p>作者背景，现实背景的重要性？</p>
</li>
</ul>
<p>2、 作品内容上：</p>
<ul>
<li><p>叙述<br>卢卡奇“历史本体论”<br>帕慕克“博物馆”比喻<br>现实世界是可以被小说家所理解并呈现的么（天真小说家的特质）？<br>环境描写的必要性？</p>
</li>
<li><p>创作<br>昆德拉“小说的历史是一部发现史”</p>
</li>
</ul>
<p>3、作品形式上：</p>
<ul>
<li><p>卢卡奇“心灵与形式”</p>
</li>
<li><p>技法、结构（如意识流、复调）的意义？</p>
</li>
<li><p>帕慕克：小说与其他艺术的区别在于以文字呈现图像</p>
</li>
</ul>
<h2 id="二、小说本体论"><a href="#二、小说本体论" class="headerlink" title="二、小说本体论"></a>二、小说本体论</h2><p>1、帕慕克：人物、情节、时间是节点</p>
<p>2、小说的可理解基于人类共同的感知基础</p>
<p>3、小说的本体不是人物性格或经历，而是人物所经历的世界本身</p>
<ul>
<li><p>世界的复杂性</p>
</li>
<li><p>世界的可能性</p>
</li>
<li><p>幽默与讽刺</p>
</li>
</ul>
<p>4、昆德拉：小说是对存在的探寻</p>
<h2 id="三、小说价值论"><a href="#三、小说价值论" class="headerlink" title="三、小说价值论"></a>三、小说价值论</h2><p>1、价值源于独一无二？</p>
<ul>
<li><p>独一性</p>
</li>
<li><p>首创性</p>
</li>
<li><p>不饱和性</p>
</li>
</ul>
<p>2、昆德拉：远离道德批判</p>
<p>3、昆德拉：探寻与超越性</p>
<p>4、帕慕克：隐秘的中心</p>
<h1 id="小说的艺术·天真的和感伤的小说家摘录"><a href="#小说的艺术·天真的和感伤的小说家摘录" class="headerlink" title="小说的艺术·天真的和感伤的小说家摘录"></a>小说的艺术·天真的和感伤的小说家摘录</h1><p>从豪门望族的悲欢离合到庸常人家的姑嫂勃豀，从异国的王子哈姆雷特到末庄那个瘦骨伶仃的阿Q，他们的故事发生在遥远的地方。无数日常的个人体验冷暖自知，他人又有什么理由置喙——“干卿何事”？一本小说或者一部电影仅仅是窥视他人世界的钥匙孔吗？如果无非是睁眼伊始的无穷无尽的日常细节，文学又有什么必要重复一遍？如果琐碎的日常生活和文学长廊上的各色人等不存在某种普遍意义，文学的声望怎么可能维持到今天？这时，一个潜在的认识模式逐渐成型：文学之所以赢得肯定，即是赢得某种普遍意义的认可。</p>
<p>这种认识模式的背后存在一个隐蔽的否定：如果普遍意义匮乏，日常生活可能成为文学的累赘。沉溺于凡俗的日子，抒发一些小小的感伤和哀怜，目光如豆，心智平庸，种种令人仰望的事物已经远遁，黄钟大吕一般的宏论高见成为绝响。这种文学是不是出了什么问题？至少在卢卡奇眼里，这即是自然主义将文学拖入的泥潭。卢卡奇无法体会日常生活的杂乱表象带给左拉的无穷乐趣——为了精确无误的描写，左拉愿意蹲在现场核实每一个螺丝钉。《叙述与描写》这篇著名论文之中，卢卡奇认为自然主义的重大危险在于，放纵各种生活细节无节制地“疯长”——他引用了左拉对于自己的一个生动形容：“真实细节的肥大症”。分析了左拉小说《娜娜》的一个赛马段落之后，卢卡奇发现，一套场景和细节由于作家的精雕细琢而就地膨胀起来。这些超重的细节很可能压垮情节，甚至抛下情节而形成独立王国：“这种精妙的描写在小说本身中只是一种‘穿插’。赛马这件事同整个情节只有很松懈的联系，而且很容易从中抽出来”；“细节不再是具体情节的体现者。它们得到了一种离开情节、离开行动着的人物的命运而独立的意义。”1当然，对于卢卡奇说来，维持一个没有破绽的、匀称完整的故事情节仅仅是初步意图。卢卡奇真正期待的是，这种故事情节可以承担历史大叙事的象征或者缩影。这即是叙述的意义。描写是静态的细节堆积，叙述表现了动态的历史运动。诚如詹姆逊所言：“叙事本身的可能性，只在那样一些历史时刻存在；在这些历史时刻，人类生活可能根据具体的、个人的遭遇和戏剧性事件来领悟，其中某种基本的一般真理，可以通过个别故事或者个别情节来讲述。”2换言之，叙事的目的即是，击穿日常生活的混沌表象。文学召集的种种细节必须编织在情节乃至历史——前者显然是后者的缩微——的轨迹之中，显示出运行的方向和机制。征用日常生活决不是喋喋不休地复述一种拖拉、繁杂的平庸状态。如果无法在历史的高度注册，那么，一切个人的偶然事件毫无价值，甚至是令人厌倦和可耻的。文学形式负责删除日常生活的多余内容，弹压各种不驯的声音，进而将大量多余的细节当作历史排泄出来的垃圾抛弃。</p>
<p>幽默的发明</p>
<p>格朗古歇（GRANDGOUSIER ）太太有孕，吃太多的大肠，多到了别人只好给她吃收敛药的地步；胎儿太壮实，使胎盘叶松弛，卡冈杜埃 （GARGANTUA）滑进一条动脉，爬上去，从他妈妈的耳朵里出来了。从前几句开始，这本书就把它的牌打了出来：这里所讲的不是正经事；也就是说：在这里，人们并不声明什么真理（科学的或虚构的） ；人们不保证要去描写在实际中就是那样的事实。<br>幸运的拉伯雷（RABELAIS）时代；小说的蝴蝶腾空飞舞，随身带走蚕蛹的碎片。当庞大固埃（PANTAGRUEL）和他的巨人的外表还属于神奇故事的过去时代，巴努什（PAANURGE）则已从小说尚未人知的未来来到。一种新艺术诞生之非凡时刻给了拉伯雷的书以难以置信的财富；一切都已经在那里了：似真与似假，隐喻，讽刺，巨人们与正常人们，轶事，思索，真正的和虚构的旅行，智慧的争吵，纯粹卖弄口舌的离题。今天的小说家，十九世纪的继承者，对早期小说家这个绝妙混杂的宇宙以及他们身居其中的快乐的自由不由生起含有羡慕之情的怀旧。</p>
<p>拉伯雷其书才开始几页，就让庞大固埃从他妈妈的耳朵掉到了人间的地板上；与此相同，萨尔曼·拉什迪（SALMANRUSHDIE）的《撒旦诗篇》中的两个主人公在一架飞机于飞行中爆炸后，边聊边唱，摔了下去，行为可笑而难以置信。同时，“在上面，后面，下面，在真空里”，飘浮着活动背座椅、纸杯、氧气罩和一些乘客，有一位吉布列尔·法利什达 （GIBREELFARISHTA），“在空中，用蝶泳、蛙泳的姿式漫游，而后像球一样滚动，将手臂和腿伸展在近乎黎明的近乎无限之中” ，另一位撒拉丁·尚沙（SA－LADINCHAMCHA），像“一个微妙的影子……头朝下栽了下去，身着灰色制服，每一只纽扣都扣得很好，双臂贴在身体两侧……一顶西瓜帽扣在头上”。以这样的场面，小说开始了。因为，和拉伯雷一样，拉什迪知道：小说家与读者间的契约应该从一开始就建立；这本来很清楚：我们在这里的讲述不是认真的，即使它涉及到再可怕没有的事情。</p>
<p>不认真与可怕的结成婚姻，请看“第四卷” （QUARTLIVRE）的一个场面：庞大固埃的船在海上遇到了一只载有羊贩子的船；其中一个商人看到巴努什裤子没有前开挡，眼镜系在帽子上，便自以为可以卖弄一下自己，把巴努什当作戴绿帽子的人对待。巴努什立即报复：他向商人买了一只羊，并将羊扔进海里；其他的羊习惯了跟随头羊，也都跟着跳进水里。羊贩子们着了慌，又是拽羊皮，又是拉羊角，结果自己也被拖进水里。巴努什手握一支船桨，不是要搭救这些人，而是要阻止他们爬上船；他用动人的口才劝告他们，给他们指明这个世道的苦难，另一种生活的好处和幸福，并声明死者比生存者要幸福。而且他祝愿他们，如果继续活在人类当中并不使他们不高兴的话，他们就会像詹纳斯（JONAS）那样，碰上几条鲨鱼。那些人都淹死之后，让（JEAN）大哥向巴努什祝贺，唯一责怪他的是：给那商人付了钱，无谓地浪费了钱。巴努什道：“以上帝的名义，我获得了值五万法郎还要多的消遣。”</p>
<p>场面是非真实，而且不可能的；它至少有一点道德吧？拉伯雷是不是在揭露商人的斤斤计较，对这些人的惩罚是不是应该使我们高兴？或是他想让我们对巴努什的残酷产生愤慨？或是作为坚决反教会权力的人，他在嘲笑巴努什预言的那些宗教的陈词滥调？请猜一猜！每一个答案都是一个给傻瓜的陷阱。</p>
<p>奥塔维欧·帕兹 （OCTAVIOPAZ）说：“荷马 （HOMERE）和维吉尔 （VIRGILE）都不知道幽默；亚里士多德好像对它有预感，但是幽默，只是到了塞万提斯（CERAVANTES）才具有了形式。”幽默，帕兹接着说，是现代精神的伟大发明。具有根本意义的思想：幽默不是人远古以来的实践；它是一个发明，与小说的诞生相关联。因而幽默，它不是笑、嘲讽、讥讽，而是一个特殊种类的可笑，帕兹说它（这是理解幽默本质的钥匙）“使所有被它接触到的变为模棱两可”。对于巴努什一边任羊贩子淹死一边赞颂来世生活的场面，不懂得开心的人们永远不会懂得任何小说的艺术。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2019/02/28/pansee/essay/关于历史、事实、宗教/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/02/28/pansee/essay/关于历史、事实、宗教/" class="post-title-link" itemprop="url">关于历史、事实、宗教</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-02-28 20:45:06" itemprop="dateCreated datePublished" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/essay/" itemprop="url" rel="index"><span itemprop="name">essay</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>关于历史、事实、宗教</p>
<p>今次读书会所谈到的《贵妃的红汗》《天工开物·栩栩如真》都涉及历史。如何看待过往的历史呢？特别是中国有长期的循环历史特点。关于历史部分我将在历史类书评中详讨，这里就不再赘述。更贴近现实的，这里涉及如何看待自身过往经历、错误的问题。也就可以谈谈施舍的问题。这一部分内容将放在下文论宗教部分。</p>
<p>同时，会上引出了对HISTORY（第一历史）、History（第二历史）的讨论。其实，HISTORY和History的关系有些类似于柏拉图理型世界和现实世界的关系，更进一步的，这里存在一个认识论和本体论上的问题。不妨回顾一下休谟的怀疑主义。同时，它存在一个经由人的过滤而带来的主观性和纯然客观性的分野问题。我在这里直接引用我在《科学发现的逻辑》书评中所谈到的内容：</p>
<p>在马克思主义哲学中，本体论问题在于物质与意识何者为第一性的问题，并据此区分唯物主义与唯心主义。窃以为，本体论问题的解答无法离开人的认识过程。自大陆理性主义与英国经验主义的争论以来，休谟以怀疑主义的视角进行了逻辑归纳法的批判，并表明了本体的不可知性。止于此，绝对化的谈论唯物和唯心已经没有太大的价值，毕竟任何的唯物客观终归要经由人这一过程，这也就势必无法确定性的剔除主观性。这也造成了对形而上学的价值的怀疑，及其与经验科学的差别分野。（《科学发现的逻辑》书评）</p>
<p>这里牵涉到逻辑归纳法的批判和理性的局限性问题。这里引用我在其他书评中所谈到的内容：</p>
<p>开篇进行了归纳法批判，首先是单称陈述推导出全称陈述的不合理性，其次是妄图将归纳法建基于经验之上势必导致无穷后退，并表明了归纳法无法保证理论的正确性。止于此，经验科学的可证实性特征遭到挑战。我们可以很轻松的列出一个反例来否决理论，却无法通过罗列与归纳来证实理论。当然，此处的理论是一个关乎普遍概念的理论，若是局限于某一可数范围，自然可以穷尽之证明之，此类缺乏普遍性，也不被称之为理论。以此表明了以归纳方法作为经验科学的特征所存在的问题。（《科学发现的逻辑》书评）</p>
<p>最后，是关于真理的问题。理性不等于真理！人的理性是有其局限性的。休谟与波普尔对于归纳法的批判已经说明了这个问题。过分的强调理性而忽略非理性过程或者把非理性过程置于次要地位都是没有依凭的，正如福柯在《疯癫与文明》中对于理性的批判。而灵性修养则是非理性过程中一个重要部分，这也正是希伯来精神与中国部分传统文化所重视的方面。至于绝对真理，或者用一个不至于引起误解的说法，绝对准则是否存在，这又牵涉到后现代性的解构主义的问题。容后详述。（《书中之书讲演录》书评）</p>
<p>这里又牵涉到绝对与相对，虚无与解构的问题。不妨先谈一下人性和道德问题，以便提供一个新的视角给那些过于相信“光明意识”者。直接引用如下：</p>
<p>一直视求诸于神的“外在超越”实在是一种怯懦与缺乏勇气的行为，而更崇尚“两刃相交”的直面与魄力。故而初识尼采被被其所论酒神精神所折服。在人生的永恒悲剧长流之上用轻盈的足舞蹈。这是直视生死离合的大勇气，是不屈的精神抗争与追求。中国传统文化所谓“小人求诸人，君子求诸己”也是如此。</p>
<p>但是，在牵涉到终极价值的问题上，这样的思想是否能够给出一个满意的解答呢？我一直以来拒绝承认自己是一个虚无主义者，尽管我的很多想法带有虚无主义的特质。我经常用禅宗“真空妙有”来谈空与虚无的区别。甚至于曾经几度以为自己已然深明禅意，只是随之而来的现实却让我发现空口谈玄甚是容易，实践行动甚是困难。故而有所谓“悟道前砍柴挑水,悟道后砍柴挑水”，如何在悟道之后把握住何者当为何者不当为，将解构之后的分崩离析重新合成一个坚实的根底与方向呢？</p>
<p>“空”也好，“自然”也好，都容易陷入绝对主观主义的圈子里。到了最后，也就是无可无不可的绝对自由状态。当然，不是说这样的观念就不对了，恰恰相反，我以为这样的“空”是能够解释终极问题的一个可接受答案。关于这点，先悬置不表，且先论“光明意识”与“泛道德化”。</p>
<p>“‘光明意识’是一种深刻觉悟到的‘道德意识’，这种主张认为哪怕再顽劣的本性，仍然可以用道德努力去完成自我圣化的这一过程。”（书33页）对于这一观念，齐老师提到，“‘内向超越’论和‘泛道德化’的最大问题还不在此，而是出在其人性预设上，对人性过于乐观的预设使‘内向超越’和‘泛道德化’论者过于忽略了人性的幽暗面”（书33页），“在批评人家‘外在超越’过于悲观之前，我们先要反思自己‘内在超越’是否过于乐观了”（书35页），“万一陷入‘真诚的自欺’并‘自欺’到自以为‘真诚’怎么办”（书35页）。</p>
<p>这里提到一个人性问题，《第四讲 人性善，还是人性恶》也专门谈到了这一问题。除了“光明意识”外，还有“幽暗意识”，“所谓幽暗意识是发自对人性中与生俱来的阴暗面和人类社会中根深蒂固的黑暗势力的正视和警惕”。（书34页，张灏《幽暗意识与民主传统》）对于人性问题，这里不再赘述。仅仅指明，我喜欢如此评价李安导演《少年派的奇幻漂流》：In me the tiger sniffs the rose.（<in me,past,present,future meet>by Siegfried Sasson）</in></p>
<p>相较之人性问题，我更关注的是道德问题，确切的说，何以证明道德本身是合乎道德的？除了宗教视角之外，可能更多的是从人与人的关系，从伦理、社会视角来界定道德。那么，最终也就要回归到人的社会属性上去。但是，至少我很难接受简单的用人的社会属性去解答人从何而来、人的价值与意义的终极问题，而对更古远的历史，宇宙的生成等问题弃置不表或排除出“人的问题”之外。其次，通过人来解释人，各种准则也就不可避免的带来了人的主观性与随意性，甚至于认为“此亦一是非，彼亦一是非”。“于是，‘内向超越’论的‘内向’就有吞噬掉‘超越’的危险。”（书35页）更极端的作法便是消解“人”的概念，消解意义与价值，视这世界为偶发的随机的，本无什么价值与意义可言，也就无可无不可，这样一来人即可以选择强力意志也可以选择谦逊道德，不过是凭着个人的意愿而已，也就近于尼采的“人神”精神。故而，在我看来，这是可以解决人的归属的一种可能。（《书中之书讲演录》书评）</p>
<p>我是这样理解相对这个观念的。相对主义有两层水平。一种是视外物为相对的，这里其实存在一个人的劣根性，就是看待外在事物的时候非常超然，可以自如的说这个东西可能存在可能不存在，这件事可以做可以不做，视立场不同而定。</p>
<p>还有一种则是把相对主义印到内心之中。这件事从这个角度该做那个角度不该做，我到底是做还是不做呢？如此一来，也就陷入了一种无立场徘徊彳亍状态。诸位，你们不妨自问一下，你们真的是相对主义么？你们在行为做事的时候难道不是依凭着一个显而易见的绝对准则么？你们真的是处于流浪无根的状态中么？</p>
<p>需要说明的是，这样的流浪状态和佛的境界是不同的。佛是窥视了这样的相对本质，又从其中走出来，也就是出世然后入世。我由于长期陷入这样一种流浪状态中，在佛学之中虽是明白些许，但又无法落实到生活实践中去，故而开始接触基督教。当然，这绝不是为了逃避，如果那是一种该然的状态，我愿意欣然领受。那么，是什么原因令我选择了基督教而放弃了那样一种流浪状态呢？这里引用如下：</p>
<p>“人其实都知道有神，即使他没有神他也一定要制造神来崇拜，他是不可能忍受真正无神的生活。”（书81页）</p>
<p>前文已经谈到了虚无主义问题。对一个非虚无主义者，活着就必须要遵循一定的准则与价值。而此准则若是由人本身来制定，就缺失了绝对权威。故而，必须有一个外在的神来定下一个道德律。一个无神的生活，一个缺乏准则的生活，人将在其中迷失，流浪。</p>
<p>对于虚无主义者，当你们在陈述世界及人的存在的无价值无目的时何不反问一句，虚无何以证明其自身的虚无呢？也就是一个无穷后退的逻辑。当然，最终可以归结为一个虚无的虚无，也就是不可再论的恒常的空，但这空本身也是一个绝对准则，而非是一般的所谓虚无。</p>
<p>至于何以选择上帝而不选择空，这已经不是理性所能论述的。不妨回到事实上来，事实告诉世人有过那么一个死而复活的耶稣基督，有过一个行诸般神迹的耶稣基督，有过这么一个非人力所能及的耶稣基督，既然有这样明朗的事实，何以不去选择信仰上帝呢？</p>
<p>或许有人会反问，难道释迦摩尼不是真实存在的么，佛不是真实存在的一种境界么？是的，在我一开始的思考中，存在这样一个误区，无意之中将神的地位拔高了，故而才能在上帝和佛之间选择出上帝。承认耶稣的死而复活不过是确证了对上帝的信仰是对人终极问题的一个合理解答罢了。那么佛和上帝的地位是同等的么？不妨进行一下上帝和佛的对比：</p>
<p>基督教通过耶稣复活确立上帝的合理性，而耶稣复活则是在一定预设下可以接受证实和证伪的；释迦摩尼是否达到那样的境界既不可知（在他人眼中的达到与否根本不具有意义，唯其自身知道）也无知的必要，佛的境界是否能够达到必须归结到自身的体认。</p>
<p>对上帝的信仰可以立时达到，具有简单性（排除美学的和实用的，波普尔证伪主义意义上的）的特点；佛的境界耗时耗力，甚至于苦参一生而只余苦。</p>
<p>可以看出，信仰上帝类似于一条合理的捷径，而修佛则是前途难测的攀登。上帝所包含的信息与真实较之佛的境界更大，更确实。但对于一个负隅顽抗者而言，这样的量的区别怕是还不足以让其承认上帝的独一性。我想，问题的关键根本不在于在上帝和佛之间做出选择，因为不管选择哪个都已经承认了两者具有同等的地位，至少是抹杀了上帝的威权。问题的核心在于，必须证明上帝是高于佛的，上帝是独一的，根本不该存在选择问题。否则那样一个上帝也不过是人自造的上帝罢了。</p>
<p>上帝和佛是矛盾的么？还是说，佛只是上帝智慧的一个映射？如果上帝包容佛，那么上帝就是独一的了，也是我该信的了！这样的对比有教内言教的嫌疑，怕是没有太大意义。</p>
<p>没有死而复活的耶稣的上帝和佛已然是对等的了，加入了耶稣死而复活这一筹码的上帝确实更具有诱惑力。但是，上帝代表的是目的性，佛代表的是无目的性，人在考虑的时候存在肯定力量与否定力量的不对等，但是原初的事实到底如何呢？这实在无法用人的视角去解读。当我带着迫切的寻求归属的目的去查考上帝时，势必带有自身的偏见，从而无法见到真实的上帝。</p>
<p>还是回到事实吧！一个混沌原初的人，一个不带主观偏见的人，当他看到了一系列悖逆规则的事，当他看到了一个死而复活的耶稣基督，但仅仅是一个叫做耶稣的人死而复活还不够，必须是一个作为传道者和规则建立者的耶稣基督的死而复活，如此来，他有什么理由不相信基督是真实存在的规则的创造者呢？！！！而佛从头到尾也仅仅是揭露规则罢了。</p>
<p>为什么一定需要一个规则的创造者呢？为什么一定要有规则呢？当然不必须有，但又为什么不能有呢？问题的核心不在于规则，应该做的是“回归事实”。</p>
<p>或许又要质疑了，何以见得事实便是真的事实（这一事实是本质意义上的，不是事件发生意义上的）呢？肉眼所见都可能不过是虚假的表象而已。依旧是那样的思路，是不是本质的事实并不重要，因为事实可能是虚假，也可能是真实。因此只需要“回归事实”（这里的事实既非本质意义上的也非事件发生意义上的）就好了！</p>
<p>但必然又有人要质疑了，那样的“回归事实”有多大的可靠性呢？我自己在这个问题上都未必能坚定的如此解释：这样的“回归事实”就好像佛家所谓真空妙有的空的境界一样，是只可意会不可言传的。但至少我目前是如此理解我所谓的“回归事实”的。</p>
<p>另外，我以为，信仰可以和怀疑并行，在虔诚信仰的同时也可保有理性。如此一来，当有证据证明上帝是个骗局，也就可以清醒地脱身，方不为盲信。（《书中之书讲演录》书评）</p>
<p>上面谈到回归事实时做了三种区分，一种是本质意义上的即柏拉图理型概念，一种是事件发生意义上的即HISTORY概念，一种是回归事实的事实，只可意会不可言传大有圣灵与我同证的意思。</p>
<p>前文《书中之书讲演录》书评的节段也回答了如下两个问题：</p>
<p>其一，“人神”精神（近于尼采的而非中国传统的）与“神人”精神之间的选择问题。</p>
<p>其二，世界上不仅有基督教，还有伊斯兰教，犹太教，诸多中小宗教，那么在这些“神”之间应该选择哪一个呢？纵然把各个宗教研究透彻，也不过是证明各宗教之间哪个更合理而已。如果再突然的通过考古挖掘发现更为古老的宗教与神祇又当如何？若是如此，岂非从生至死都在无归属的研究之中？更何况，不可能要求每个信众都是伟大的神学家，对神的领悟也不与理性直接相关。（《书中之书讲演录》书评）</p>
<p>最后，我再谈一下舍己的问题。其实，这个问题和博爱的问题又有关联。具体内容我也就将放在弗洛姆《爱的艺术》书评中去论，此处不再赘述。</p>
<p>今次的通讯有些偷懒，尚望诸君多多担待。关于我以上书评节录有兴趣看完整版的直接去我QQ空间就好。</p>
<p>2013年12月16日星期一    文/鷇音</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2019/02/28/pansee/film review/南京，南京——观《南京，南京》有感/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/02/28/pansee/film review/南京，南京——观《南京，南京》有感/" class="post-title-link" itemprop="url">南京，南京——观《南京，南京》有感</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-02-28 20:45:06" itemprop="dateCreated datePublished" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/film-review/" itemprop="url" rel="index"><span itemprop="name">film review</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>南京，南京</p>
<p>——观《南京，南京》有感</p>
<p>在灰暗的氛围下看灰暗的《南京，南京》，看出灰暗的思绪与灰暗的心情。</p>
<p>从头至尾，我似乎只从影片中看出了灰暗。</p>
<p>没有当时的世界背景，没有当时日本侵华的背景，也没有当时中国的时代与社会背景。</p>
<p>没有日军进行南京大屠杀的完整而具体的描画，没有屠杀前后南京的实际情况，也没有屠杀之后日本的反应。</p>
<p>一段斩头去尾的历史不叫历史，一段连过程都不清晰的史实不叫史实。</p>
<p>如果说《南京，南京》的看点在于以角川——一个参加了南京大屠杀的日本军人为视角进行描画的话，我也认为这是此片的最失败之处。</p>
<p>主人公角川，给我的印象是一个迫于形势，却又在心中埋藏着爱与仁慈的被士兵者。</p>
<p>看着血流成河，尸骨成山，角川在良心的谴责中崩溃，为了逃避这样的残酷现实，为了将自己双手上的血渍洗去，他选择了用生命换取救赎。</p>
<p>我却不禁要发问，是什么使得角川被士兵，是什么使得角川被染血，是什么使得这样一个残酷的现实被出现？</p>
<p>用一个日本军人作主体，明明是最便于分析当时日本的社会情况，明明是最便于透视当时一个日本军人的心理状态和精神信仰，明明是最便于展露当时进行南京大屠杀的日本军人的内心活动和屠杀原因，导演却对这些视而不见，或者说将这些要素埋藏得好深好深，使之被置于迷雾下的灰暗里。</p>
<p>看完影片，我认为导演是考虑到这些的，可却有意将这些次要化，而突出一个日本军人的良心救赎，这又是为什么？</p>
<p>为了迎合观众，迎合主流，为了迎合利益！</p>
<p>既然是为了利益，那我也就不难理解日本为何侵华了。</p>
<p>在日本东京，以荀子“游必就士”命名的战争博物馆——游就馆里的十一个展室里，炫耀着日本明治之后在他国土地上的辉煌战绩，供奉着各式杀人武器，同时以“资料短缺，生存空间狭隘”为由为一场场侵略战争作解释，东条英机等人的照片高悬墙上。</p>
<p>每个人的生命只有一次，没有人愿意用可贵的生命来体验战争的刺激。当没有等同的利益乃至于更高层的利益时，战争与流血绝不会发生。同样的，没有必要的利益，南京大屠杀也不会悲哀地发生。</p>
<p>惨剧的发生，源于人类对利益的趋之若鹜。滚滚长江东逝水，鲜血可以被稀释乃至于分解，尸骨可以被腐蚀乃至于归于黄土，时间的长河中，利益却是可以永恒存在的。</p>
<p>我可以清晰的记得，蒙古的铁骑开拓下了一片又一片的沃土，却难以想起中亚的寸草不生，近两亿人的头颅被悬挂在马脖子上。我可以清晰的记得，太平天国的奋起反抗，立志创出一片新天地，却难以想起其所过之城，人民的惨象。我可以清晰的记得，一代名儒曾国藩的种种丰功伟绩，却难以想起南京城五十万市民遭屠的史实。</p>
<p>一幕又一幕的惨剧，一次又一次的反思，换来的不过是一起又一起的悲哀。这样的反思，到底反思出了什么呢？</p>
<p>靖国神社，依旧伫立。相当一部分日本人乃至于首相，都在参拜靖国神社。如果我是日本人，我同样会去参拜靖国神社。</p>
<p>甲级战犯？胜者王侯败者寇，历史同样要被权力和利益左右。成吉思汗，不是杀人魔王，而是一代天骄。难道说东条英机与成吉思汗有什么不同？他们同样在侵略，他们同样在屠杀，他们同样在追求利益。</p>
<p>今天的世界，我们倡导和平，那是因为可以用战争以外的方式追求所需的利益。不管是个人与个人之间，团体与团体之间，还是国家与国家之间，明争暗斗何曾止歇，利益之争何曾消停？相同的目的，不同的方法与过程，就能够避免惨剧的发生？</p>
<p>既然利益是人类永恒的主题，那么将自己置身于利益圈之外，从所谓人性的角度去批判这些所谓战犯，去批判这些追求利益的失败者，岂不是彻头彻尾的伪善？！</p>
<p>遑论这些“战犯”还是在为国家利益，为了心中的信仰在拼搏。所以对于一个日本国民来说，这些“战犯”乃是在为国家做贡献，乃是在健全自己的个体人生意义，就应当是英雄与神灵，就应当值得参拜！</p>
<p>那么我们能做些什么呢？还是白自己练的铁石心肠，冷血无情，从而方便自己去追求利益？</p>
<p>勃兰特在最寒冷的日子里向死于德国纳粹之手的犹太人下跪，普金在卡廷为当年的遭难者下跪，可是这样的行为能挽回什么？如果说跪完之后继续去不加节制的追求利益，那他们也不过是披着羊皮的狼罢了！</p>
<p>君子爱财，取之有道。纵然人有原罪，也同样可以杜绝本罪的发生。人之为人，在于神性与兽性的兼有并融合性。君子与小人的区别，不在于一个趋于义，一个趋于利，而在于君子是神性的趋于利，小人是兽性的趋于利。</p>
<p>和谐社会，所谓和谐，不在于存天理，灭人欲，不在于摒弃兽性，只留神性，而在于神性与兽性的制衡，善与恶的平均。</p>
<p>诚如海德格尔所言：善时恶的善。在时间的长河里，还有一样东西是永恒存在的，那便是对于利益的理性批判。</p>
<p>东条英机之所以会惹来千古骂名，首先在于它是一个失败者，本源在于他的所作所为僭越了一个人类共同的尺度。</p>
<p>其实人是残忍而自私的。我们在餐桌上大鱼大肉，同时大声怒骂着那些吃食死婴的“禽兽”。不管是哪一种族，永远都不会对其他种族施以仁慈，而所谓的怜悯亦不过是鳄鱼的眼泪。而人类今日的残忍与自私在于可以为了自己的利益而去枉顾他人的生命，竟是连同类亦可相残，无怪乎梁启超要感叹：但闻虎吃人，不闻虎吃虎。而平静社会下的相残要比动荡与战火中的更让人痛心。</p>
<p>当火焰熊熊的燃烧，一个美丽的女人如斯凋零，当事者却是满口的国家利益至上，只去感到惋惜，而不曾有一丝悔恨。这不得不说是一个情盲，乃至于一群情盲的悲剧。（《城管局长钟昌林如是说：唐福珍自焚是一个法盲的悲剧》。《南方周末》第一千三百六十四期）</p>
<p>人非草木，孰能无情。人与动物的区别便是在于情之有无。正是因为有情，人方能为五行之秀，实天地之心。不敢想象，如果一个的心中只充盈着利益，丧失了人的本性后，又该是什么？魔，实为人魔！</p>
<p>当我们每日吃食着地沟油的饭菜，提心吊胆着血铅是否超标，青嫩的蔬菜又要担心有毒，于是在这毒素蔓延中，人被失去了一颗鲜活的红心，失去了对天地，对自然，对生命的敬畏。</p>
<p>回顾汶川大地震与玉树地震的不同，它们的区别绝不仅仅在于死亡人数与震级上的差异，一个是血泪布撒，一个是平和而略带忧伤。原来死亡从不曾是终结，不过是另一个开始。不是在生者长戚戚，反过去担心死者将会幸福么。有了信仰，不死的灵魂得以成为实在。</p>
<p>也正因此，如果我是一个日本人，我会去参拜靖国神社，仅仅是去表示一种缅怀，一种敬意，对他们为了信仰而能付出一切的敬意。</p>
<p>可惜的是，恶魔的信仰会让他们也成为恶魔。军国主义绝不应当再起，尽管知道，这仅仅是一种奢望。只希望，能让心中有些许敬畏，不至让曾经的惨剧再度发生。</p>
<p>可谈天不如人意。躲猫猫死，撞墙死，喝水死，死的人数少了。死的悲惨却是更增。无意去指责些什么，知道每个人都是在摸着石头过河，每个国家也是在摸着石头过河。只希望，官员们能够去敬畏权力，唯有如此，在使用权力时方会如履薄冰。</p>
<p>从来都认为，读历史是为了更好地去生活于现实。与其再去纠缠于曾经的悲哀与纠葛，倒不如去做些实在而有意义的小事。而所谓反思，也只在具体而微的事中方能体现，一如丰田召回门。</p>
<p>仅仅呓语，随行为之。</p>
<p>2010年5月10日星期一上午 多云</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2019/02/28/pansee/film review/告白/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/02/28/pansee/film review/告白/" class="post-title-link" itemprop="url">告白</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-02-28 20:45:06" itemprop="dateCreated datePublished" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/film-review/" itemprop="url" rel="index"><span itemprop="name">film review</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>告白</p>
<p>2012年1月5日星期四——2012年1月7日星期六</p>
<p>应该讲，这部电影有其成功的一方面，只是个人认为它的部分成功是建立在它的最终失败之上的。</p>
<p>告白——森田悠子</p>
<p>开场便是老师森田悠子的告白。和艾滋病人樱宫正义结合，为了孩子不受异样的眼光而独自抚养女儿爱美的悠子，不可避免的将主要的精力放在孩子身上，而忽略了作为班导对班上一群13岁学生应有的爱和付出。因为有女学生陷害男老师的事件发生，悠子对于进入派出所的班上男生也只是摆脱其他男老师去接，实在是伤透了孩子们的心。</p>
<p>如何在工作和家庭之中求得平衡，尤其是在这两方面产生矛盾的时候？不妨将这个问题放大一点，如何在个人与社会之间求得平衡，尤其是在个人价值与社会价值产生矛盾的时候？儒教（不是儒家！）喜欢讲，先天下之忧而忧，后天下之乐而乐。但是也正因为儒教的礼，让得人们反感，它不仅仅是一套等级森严的礼法制度，它更在压力个人价值的实现，或者说是对个人价值的漠视！其实，时至如今，中国人依旧没有摆脱这样的思想。为了国，为了社会，个人是可以牺牲的，个人是应该牺牲的，个人是自愿牺牲的。只是这样一来，个人到底是社会的个人还是个人的个人，社会到底是社会的社会，还是个人的社会？</p>
<p>另一方面，儒家讲忠恕之道。推己及人曰忠，己所不欲勿施于人曰恕。如果学生能多为老师想想，老师也能多体谅学生，又何至于发生那样的惨剧。只是，必须注意的是，一群不满14岁的学生们，是难以有如此的思想与宽容的，那么森田悠子的过错就是显而易见的了。其实，通过班长北原美月的一句告白，“悠子老师走后，班上洋溢着开朗的笑容”，便可以预见到悠子在这部电影中所扮演的角色了。</p>
<p>便是在老师与学生的矛盾中，修哉和小直杀害了爱美。紧接着的，便是悠子一连串的报复？</p>
<p>因为未满14岁，根据少年法，修哉和小直并不会受到伤害，正如用化学药剂毒杀全家人的露娜希一样，最终将逍遥法外。也正因此，悠子在两人的牛奶中加入了樱田的鲜血，并言称生命的珍贵，要两人用以后的日子来慢慢体悟与理解。</p>
<p>不妨谈谈艾滋病。时下的中国，对于艾滋病这一话题也有了更多的关注。就是昨天，还看到一则电视广告，是通过两个艾滋病小孩来呼吁给予艾滋病人以关怀。让我不解的是，艾滋病人是畸形的么？为什么广告中那两个小孩要笑的那样畸形，希望以此来博取正常人的同情心？难道这样的广告，不是对艾滋病人的侮辱？需知道，艾滋病人与正常人的生活并没有多大差别。正常人也并不比艾滋病人高贵。艾滋病人需要的不是正常人的怜悯，而是尊重和认同。脱下虚伪的外衣吧，口口声声喊着关怀艾滋病人，心底里不过是充满了自身的优越感以及对于艾滋病人的鄙夷罢了！</p>
<p>对待一个人，最可贵的是尊重，而不是同情，因为只有这样，才能让他人有尊严的活着。对待一个人，最可贵的是宽容，而不是理解，没有人可以理解世界上的所有人事物，说理解只能表明个人的虚伪，只有去宽容，去将他视为和自身一样高贵的抑或低贱的生命，才是一种真正诚挚的感情。</p>
<p>告白——下村直树</p>
<p>下村直树，运动和学习都显得乏力的小男孩。上进心？从他积极的参加锻炼和补习班来判断，应该是有上进心的。只是，既然如此，又为什么放学后要去游戏厅打游戏呢？孩子总是贪玩的。或许只是一个很好的解释，可以说明直树的上进心是真实的。</p>
<p>不妨先来谈谈“玩”。什么才能叫做游戏呢？给人快乐的应该就是游戏吧？如果这样定义，杀人对于杀人狂而言是不是游戏呢？相信除了杀人狂自己及其同类人，一般人是绝不会认同杀人乃是一种游戏的。如此一来，游戏首先应该是一种社会性的东西了。不在这个问题上纠结，毕竟只是一个看事情的立场问题罢了。我们发现，随着年龄的增加，从统计的角度来说，玩游戏的时间是越来越少了，玩的游戏更加倾向于高级智力、体力或者是博弈类，而很少再去玩魂斗罗，拳皇之类的相对而言显得小白的游戏。虽然有成年人反过来去追捧这样的游戏，但那也只是对于童年的追忆而已，他们该是不可能从点点鼠标，看看动画之中来获得真正的满足吧？故而，我们是不是能够从游戏的角度去为一个人的心理年龄的断定提供信息呢？</p>
<p>另一方面，角色扮演类和竞技类游戏占有很大的份额。那么，沉迷于其中的人会不会乃是在现实生活中显得失意或者达不到他所期望的高度，转而在这样的游戏中获得胜利与成功的快感呢？再者，沉迷于游戏是不是对于现实的一种逃避呢？还有一个可能，就是游戏可以提供给我们在现实生活中所不能有的经历，譬如暴力和杀戮。人是有潜藏的杀戮欲望的，通过游戏便可以得到释放。正如通过A片和类似游戏，人的情欲可以得到释放。</p>
<p>下村直树在明知爱美没有死的情况下依旧将她抛进游泳池，导致溺毙。被修哉所看不起的直树这样做，便是为了证明自己的所谓价值。是的，作为主谋的修哉没有成功杀掉爱美，作为从谋的直树却是成功了，这对于修哉该是一种打击与讽刺，同时是对自己能力的证明。</p>
<p>仅仅因为悠子没有去警察局接他便令他有了这么大的恨意么？仅仅因为修哉是唯一一个请他看电影，拿他当朋友的人，就可以为之死心塌地么？仅仅因为修哉的虚情假意与冷然漠视，便让他有了行凶杀人的勇气么？</p>
<p>直树有一个过分宠溺他的母亲，这让他无法受气。而另一方面，天子骄子一样的家庭生活和他在学校不被重视的剧烈反差势必导致了他对于学校对于老师的愤恨。需要注意的是，直树的家庭只有他的母亲出现，父亲和姐姐从来不曾出现在镜头，或者可以怀疑一下直树缺乏父爱，那样的话他的心理不健全就更容易解释了。</p>
<p>有一个细节是需要注意的。直树喝下含有艾滋病人血液的牛奶后休学在家。凡是他用过的厕所等都势必要亲自仔细擦拭干净，但它本身却完全不洗澡洗漱，以至于身上肮脏黑暗。如果说自身不清洁是在自暴自弃，那么擦拭使用过的器具是一种防止家人染上艾滋的责任使然么？</p>
<p>此处，确有深究之必要。不妨结合后期用血涂染超市的食物，以及在家吞食带血食物来做个分析。</p>
<p>最后我们还是来看看直树妈妈的行为吧。写下遗书，拿上刀，要先杀死直树，再自杀，结束这段痛苦。这就很有意思了。直树是她的儿子，丈夫不是她的丈夫，女儿不是她的女儿？在得知爱美之死因时她也不曾给过一句抱歉，只是不断强调直树是个可怜的孩子。她的自私也是显而易见的了。对于直树妈妈，其实还是很有可谈之处的，只是本人愚鲁懒惰，就不展开了。</p>
<p>爱，有的时候真的很盲目。对于悠子而言如是，对于直树妈妈而言亦如是。孩子蒙蔽了他们的眼。</p>
<p>告白——樱宫正义、池田寺辉</p>
<p>樱宫正义，爱美的父亲，曾经少年浪荡，后来幡然醒悟，将自己的一生奉献在教育事业上。直到生命的最后一刻，他也要阻止悠子的报复，使得悠子没有成功将血放入修哉和直树的牛奶。其实，即使放入，感染的几率也近乎为零。</p>
<p>由此可见，杯弓蛇影实在是拥有强大的杀伤力。有这样一则案例。某女孩为了减肥，就每天都让自己有厌食症，觉得吃东西恶心，最后肉体换上厌食症，无法吃下任何东西而死亡。很可惜，心理因素可以让肉体真的崩溃，但是却不能让崩溃的肉体复原，至少厌食症是不可逆的。素还真吞吃沙人畏无毒的至毒之药而心中惴惴，天下第一智者尚且如此，常人又能如何呢？</p>
<p>自己的心有的时候反而会欺骗自己。在心理学上有一个实验。两组实验者，一组走过危险的独木桥，一组走过安稳的木板桥，走过独立桥的实验者过半数认为是自己喜欢桥对面的异性。有的时候，我们往往将自己的情感混成一团，一旦发生刺激性事件，便认为全部是由其导致。如果让剪不断，理还乱的情感和感觉能够纹路清晰，那么我们就可以减少意气用事，也不至于做出个别重要的错误决定了吧，尤其在男女感情上。只是这样一来，或许就少了一份意外或是一段好姻缘。</p>
<p>另一方面，不该人言亦言，尤其对待一些令自己感到恐惧的事情，往往第一印象就成为了真理。</p>
<p>如何让心如明镜台一般澄明，照出万物的本来面目呢？如果真的可以看清事物的本来面目，也未必是种智慧吧？</p>
<p>池田寺辉，激情负责的年轻男教师，尽力去当学生们的大哥哥，让同学们影印笔记，制作祝福卡，每周都去送给直树，希望他能重返课堂。作为樱宫正义的狂热崇拜者，不知内情的他受悠子利用，无形中促成了悠子的报复计划。</p>
<p>想到倒扶事件，小悦悦事件，好人免责立法被广泛提出。只是，这样的立法真的适应中国当前的社会状况么。好心并不一定能做成好事，甚至于因为一时好心的盲目救助，可能让一个伤者死亡。当下的中国，教育的普及程度，尤其是紧急救助知识的普及，怕是还没有达到可以让每个好心人都不犯错误的程度吧？同时，这实际上也给恶人创造了机会，不是么？好人免责立法的确立，是好是坏也绝不是简简单单说说就可以的。</p>
<p>看到池田寺辉最后雨中没落的背影，不禁痛心。我们所欲为的，却偏偏走向了相反的方向。人生中有太多的无奈，有太多的遗憾，我们倾尽心力，却只能最后感叹，红酥手，黄腾柳，满园春色宫墙柳。</p>
<p>告白——渡边修哉</p>
<p>天才小博士——渡边修哉。获得全国大奖，本该万众瞩目的他，却因为获奖之日的露娜希事件而被遮掩了光彩。作为物理学者的母亲，从他幼年开始就采用批评打骂的方式对他进行电机学教育，对他寄予了厚望。父母离异后，母亲重回研究所，再也不曾见过他一面。</p>
<p>或许，对于一个全身心迷恋其研究领域的学者而言，让他将心力分给其他方面，哪怕是他自己的孩子，可能也很难。在模糊的记忆中读过这样一篇文章，约翰逊博士回忆年轻时埋头于书本，而忽略了对于老父的关心与爱，言辞中尽显了他的懊悔与遗恨。在知识与感情之间，到底更加更加重要呢？其实，人可以对人有感情，也同样可以将感情投注于知识。我是一个极端主义者，爱一个人，便爱的彻底，同时彻底的忽略其他人，投入一件事，就全身心的投入，而不再关心其他。我不认为人有限的感情和时间可以分给许多事物。如果可以这样，那么又为何不可以三妻四妾，不可以同时爱着两个乃至于更多的女人或男人呢？</p>
<p>修哉虽然是在母亲的打骂声中度过他的童年，但是他内心深处却是爱着他的母亲。不，不应该这样说，应该说他内心深处渴望着母亲的爱。对于一个自小缺少母爱的孩子，当他在他的父亲和继母身边无法赶到关怀，自然会投向他那已经远去的母亲，从中寄予自己的一丝希望和坚持。只是，这样的希望，或者只是在自欺欺人而已。</p>
<p>借北原美月之口，说出了修哉具有恋母情结。佛洛依德所提出的俄狄浦斯情结，便说道每个人对于母亲都有一种依恋与性欲望。只是，对于修哉而言，当真如此么？个人认为，修哉的恋母情结只是假象，不论他做的多好，得到的只是母亲的批评，从他一出生，母亲就宣判了他这一生的失败，因此他千方百计要获得母亲的承认，以此走出心里阴影。</p>
<p>我们常说血浓于水，两个自出生就不曾相见的母子，在二十年后可以抱头痛哭，母子关系较之一般人更加深厚。只是，这样的母子亲情从何而来，只是因为体内的血液与DNA？哈，或者，仅仅因为我们从一开始就被灌输了母子亲情的观念，这样无根之观念早已深深扎根于潜意识之中了吧？原来，所谓感情，不过是如此荒谬的社会造物。又想到心理学上的单纯接触效应。一对男女，仅仅因为每日见面，就可以互生好感，也即是所谓日久深情。这个世界是如此让我恐惧，哪里有什么是非她不可的，哪里有什么是非我不可的，哪里有什么感情是货真价实命中注定的。只是一种机缘巧合，只是一种随机现象，相互串合而成了这个世界。或许在这一片地域之内，我是独一无二的，因为没有人可以取代我。难道是我自己陷入了自己制造的陷阱？</p>
<p>本意是在修哉和美月这里大书特书，只是写到现在，确实是不想再写下去，而所写的也实在不成其为影评了，不过是几日来的日志串烧而已。</p>
<p>告白——北原美月</p>
<p>这样的角色，一直是我所喜欢的。带有几分妖艳，却是不妨碍她的真实与纯美。</p>
<p>自称为是第二个露娜希，唯一一个能够理解修哉的人，被称之为美呆，很贴合的外号。“呆”，什么才叫做呆呢？呆有很多种。有的是不论善恶一味包容的呆，有的是傻头傻脑无忧无虑的呆，有的是胡思乱想迷惘困惑的呆，有的是情根深种不能自拔的呆，有的是自欺欺人求真向觉的呆……有的人呆的痛苦，有的人呆的快乐。其实，每个人都是呆子，浑噩噩的在这苦难世上挣扎。</p>
<p>告白——鬼如来</p>
<p>开头说，这部电影的部分成功是建立在他的最终失败之上的。之所以如此说，虽然他的剧情却是有些出人意料之处，但是它却彻底颠覆了一个老师形象，这样做实在是太过冒险。拿电锯惊魂比较之，电锯的成功之处在于它所预留的一线生机，以及警察、凶犯、有罪者的圈子控制了其不良影响，这样就使得其可以达到警醒之目的。而此片虽然新奇，所营造的阴影个人认为有些过了。</p>
<p>鬼如来之名，取自霹雳布袋戏。曾经仁慈悲悯，救众生苦难的帝如来，为了消灭魔族，残杀亲友，血染犀角，潜伏进入魔族内部。怎料得，灭魔之后反受正道追杀，不得承认，又知这一切都是为凶残的厉族布局利用，使得如来成鬼的大意念不过是危害天下。至于此，正邪皆不能容，只好孑然独行，于那混沌难明的善恶之中求得己心澄明。</p>
<p>我亦希望在这茫茫浊世之中，求得不二法门，得到大自在。只是，可以求得么？</p>
<p>这篇影评本是对人之承诺，却不能完成。又欠小丹两个小时，其他不知凡几，我所欠的太多，我所负的太多，或许沉默寡言才可以不愧于人，只是这样一来，难道不是不敢担当么？</p>
<p>《礼记·缁衣》中说道：君子道人以言而禁人以行，故言必虑其所终，而行必稽其所敝，则民谨于言而慎于行。</p>
<p>以往的我，太嚣狂妄语，当戒之！</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2019/02/28/pansee/film review/津渡几曾迷/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/02/28/pansee/film review/津渡几曾迷/" class="post-title-link" itemprop="url">津渡几曾迷</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-02-28 20:45:06" itemprop="dateCreated datePublished" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/film-review/" itemprop="url" rel="index"><span itemprop="name">film review</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>津渡几曾迷</p>
<p>——《12怒汉》观感</p>
<p>女法官与圣母像重合，黑暗中的道路亮起了明灯，于是乎：</p>
<p>愤怒的士兵倒下，巨大的车轮滚动，“自由，劳动，幸福，向着人民光明的未来前行”！</p>
<p>民主即专制</p>
<p>——不得不说，你的想法有点……</p>
<p>——犹太。</p>
<p>12怒汉，实则是12使徒，来自不同职业，不同阶层，不同民族与地区，有着不同的经历，不同的生活习惯，不同的文化与思维方式，他们以12——这个基督教的完满之数包容了一个社会，所有的人民。</p>
<p>当人类能够直立起来，人类共生关系的相互依存性也同时被决定。如果说过去的人类社会还可以存在不同民族间的相互杀戮，上层阶级和下层阶级民众之间还可以存在压迫与反抗，在今天这个生产力高度发展，全球化的经济依赖与文化交融的情况下，每个人，乃至于每个民族都无法脱离整个社会而单独生存。</p>
<p>显然，一个无序混乱的社会无法满足要求，少数人专制的社会也必然会被意识觉醒，要求平等的被统治的大多数所推翻，人类对生存的越来越高要求，一个有序民主的社会必将出现。在此民主的社会之中，“人人生而平等，每个人一出生便拥有上帝所赋予的权利，包括生命权，财产权，自由权等”。</p>
<p>你相信么，人类是为了国家而存在？或者国家是为了个人而存在。</p>
<p>每个人为了实现其有效的最大利益，不得不放弃部分利益互相集结成国家与社会。橱窗造出的社会不是“一个他人”，并不存在凌驾于个人利益之上的“集体利益”。民主的价值在于通过每个人都有被才觉得可能从而理性的保护每个人有效的最大利益。正是每个人被裁决的等可能性要求着人与人间的平等，无视财富，无视民族。“如果他没有杀人，他俄语讲得好与不好又有什么区别呢？”每个人都是一个独立的个体，有与众不同的个性，如果不能包容不同的文化习俗，不同的生活方式，矛盾不能得到调和，争端持续出现，平等只是空谈，民主不过幻梦。</p>
<p>但如果乌玛确实杀了他的养父呢？宽容与接受依旧存在限度，海只能纳百川，而不能容沙石。道德的拥有便是宽容与接受的标准，异己者需要消灭，民主不过是多数人更确切的说是强力者所遵循与认同的道德的专制。民主即专制的成立条件乃是上帝赋予了不道德者相同与道德者的权利，不然专制无从谈起。虽然说存在即有理由，但上帝说人要向善，有趣的是上帝惨遭厄运，一切化为乌有。成熟的社会在动荡中获取类人格，摆脱工具身份，一跃而上上帝的宝座，订立宣扬善恶之分界，专制即民主！</p>
<p>法律即罪行</p>
<p>法律的目的乃是为了维护正义从而实现社会的全道德，消灭不正义及至不道德的可能。而正义乃是过往人类的造物，现如今只隶属于道德社会。显然，符合社会道德的乃是正义，不道德的乃是罪恶，故而傻人有罪，当受刑罚。</p>
<p>刑法之合理性何在？基于同意主义，报应主义抑或威慑主义？然而人类怎么能自欺欺人的相信犯罪者在犯罪时保有绝对的理性？“放下屠刀，立地成佛”，如果正有这样的屠夫，等待他的也只能是死刑或终身监禁。报应主义是如此之可笑！杀人者应当受到报应，被杀者难道不是已经受到了报应？法律有是否可以充当上帝之手，做出完美的调节与冲抵呢？扼杀可能与威慑主义才是真正的直接目的。“法律已经死了，我可对他没有偏见，不接触法律的俄罗斯人是空壳，他不会偷也不会保护自己。”想想那个哈佛人在恐惧之下的决定，刑法的目的乃是排除威胁，排除实在之后不再实在的可能甚至于不曾实在过的可能，以此保障大多数人的安全有效最大利益。在侵害部分人的利益之时，又不曾让另一部分人的利益有所增广，刑法是如此低效，甚至于是自私的罪行！幸运的是人类的法律行将就木，道德社会的公证法律愈加强势，人数人的专制，强权者的意志，将无处容身，自私，利益，不道德的行为，消亡！</p>
<p>“你们有权决定，凶手犯下的罪行，危害社会的行为，是否可以受到最严厉的惩罚。”问题在于，既然已经确定人的社会性事实，是否杀人者该为起杀人行为完全埋单？是否处死杀人者即可消灭罪恶确切的说不道德的可能？如果承认人性之恶，专制之民主再一次得到证明，异己的不道德只能被灭亡，只有基于共同道德的不同者之间才能有平等可言。如果承认环境因素之影响，犯罪者之罪行又如何能够由其一身承受？更重要的，如果说人一旦脱离了他人与他物，就无法证得自身的存在，那么人的独立个性何在？或者说，个性并不能独立存在，乃是他人与他物所赋予。正在装修的法院，已不再是孩子的小学生的学校，多么巧妙的位置关系。人从一出生就开始受到框定，道德坚定稳固的植根于人心，从而社会其自身可以达到天下人皆知美之为美，善善恶恶的完美道德境界。“好人就该受到帮助，坏人就该……”于是乎，每个道德社会的个人，都以社会道德为立场生存甚至于思想，每个人的个性也将基于道德共性之上。因而学生们离开学校之时，不道德的可能即被消灭，法院在建成的同时成为历史的建筑，又随着历史消逝于无形。然而，法律难道没有扼杀不道德的个性，确乎不是罪行！</p>
<p>还有更怪异的事情发生。城市化的发展，新居住地的筑建，已然开始侵损大多数人的利益。但又是什么保证它的合法性，是之不必受到法律的制裁？是少数专制独裁，是强权者的意志，还是一个美好道德的未来？可惜，现在的人类只存在于现在，人类不仅仅被丧失了自然个性，还丧失了自然的现在存在！</p>
<p>可是，难道说永久不止歇的纷争与残杀为人类所期冀？“法律是强势而稳定的，但是当宽容比法律更有强制力时，我们能做什么呢？”以刑去刑，利益被消融，代之以美德和公义，被消亡了罪恶可能的每个人再次得到道德的净化。银戒的光芒，12次的闪动，戴在残破的右手上。人类虽然是自然的自由思想和现在存在者，但却被法律以杀戮的罪行扬弃了甚至于一出生就扬弃了不道德的个性与现在存在，凭借道德之光的指引走向永恒的未来。在全道德的社会中，罪行即法律！</p>
<p>真理即谬误</p>
<p>回忆一下那些特写镜头，轻柔的羽毛，残破的书本，死亡的尸身。人，不过是一根能思想的苇草。人是软弱而无力的，在宇宙与自然面前，在大风的席卷之下，除了像羽毛一样顺从及至被毁灭，还能做些什么呢？死亡是如此之临近，或许明天，或许今天，思想凝结的书本与客观真理，又如何能够通达真理与永恒？</p>
<p>“如果你想飞走就飞走吧。道路通畅。如果你想留下就留下吧。但是做个决定，完全自主决定，没有人能帮你做这个决定，你该骄傲。”鸟儿飞入了寒冬，等待他的将是死亡。一叶扁舟，漂流在苦海上，风浪起，唯有相互连结以抵御。“1,2,3,4,5……1,2,3,4,5……”机械，习惯，重复，钢铁的锁链以道德为基连环，木制小舟本身倒显得无力，与强势的稳定同时而来的是单一的方向，不可更逆。偏向者与弱势者都将被无情的粉碎，不是风浪，而是钢铁的伟力！乌玛和维达亚的死亡不过是前进途中的必然，不是为了私利，不是为了权益，乃是道德与正义的奠基。巨大的质量，巨大的惯性，巨大的前进！如果不相信逻辑的花言巧语，不拿现实与纯粹虚构的绝对的，永恒的世界相比较，不持续不断的通过数字来扭造这个世界，人便没法生存——拒绝错误的判断将意味着拒绝生活，否定人生。“寻求真理，不能在日常生活的细节之中，而应在生命本身。”可悲的是，正是对生命本身的谎言构筑了道德的真理与人生的意义。如果可以选择，是在监狱中长生还是在监狱外死亡？是在黑夜的海上独自摸索还是欢聚前行？出创造出的社会乃是当时人类的自主选择，一次摆脱迷惘与无所依从，克服生命不能承受之轻。恐怖的是，选择的道路产生自然的巨大引力，社会以谬误的真理创造出封锁其他道路选择的法律，当全道德实现，法律消融，大道坦途，谬误即真理！</p>
<p>难道有善，难道有非善？遁去的一，杳无踪迹，虚无之路，同样不可求取。九天明月不可揽觅，其实对镜流连看花影。于是乎：</p>
<p>津渡几曾迷？</p>
<p>2010年11月11日星期四</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2019/02/28/machine learning/NN/Neural Networks for Applied Sciences and Engineering--Chapter 8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/02/28/machine learning/NN/Neural Networks for Applied Sciences and Engineering--Chapter 8/" class="post-title-link" itemprop="url">Neural Networks for Applied Sciences and Engineering--Chapter 8</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-02-28 20:45:06" itemprop="dateCreated datePublished" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/NN/" itemprop="url" rel="index"><span itemprop="name">NN</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Chapter-8-Discovering-Unknown-Clusters-in-Data-with-Self-Organizing-Maps"><a href="#Chapter-8-Discovering-Unknown-Clusters-in-Data-with-Self-Organizing-Maps" class="headerlink" title="Chapter 8 Discovering Unknown Clusters in Data with Self-Organizing Maps"></a>Chapter 8 Discovering Unknown Clusters in Data with Self-Organizing Maps</h1><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<h2 id="8-1-Introduction-and-Overview"><a href="#8-1-Introduction-and-Overview" class="headerlink" title="8.1 Introduction and Overview"></a>8.1 Introduction and Overview</h2><h2 id="8-2-Structure-of-Unsupervised-Networks"><a href="#8-2-Structure-of-Unsupervised-Networks" class="headerlink" title="8.2 Structure of Unsupervised Networks"></a>8.2 Structure of Unsupervised Networks</h2><h2 id="8-3-Learning-in-Unsupervised-Networks"><a href="#8-3-Learning-in-Unsupervised-Networks" class="headerlink" title="8.3 Learning in Unsupervised Networks"></a>8.3 Learning in Unsupervised Networks</h2><blockquote>
<p>Rosenblatt proposed a model of competitive learning between neurons.In his model that<br> attempts to mimic this brain function,neurons inhibit each other<br> by sending their activation as inhibitory signals,the goal being to<br> win a competition for the maximum activation corresponding to an input pattern.<br> <strong>The neuron with the maximum activation then represents the input pattern</strong> that<br> led to its activation. This neuron alone becomes the winner and is allowed to<br> <strong>adjust its weight vector by moving it closer to that input vector</strong>;however,the neurons that<br> lose the competition by succumbing to the inhibition are not allowed to change their weights.</p>
</blockquote>
<h2 id="8-4-Implementation-of-Competitive-Learning"><a href="#8-4-Implementation-of-Competitive-Learning" class="headerlink" title="8.4 Implementation of Competitive Learning"></a>8.4 Implementation of Competitive Learning</h2><blockquote>
<p>In many cases,the number of data clusters is unknown.When there is uncertainty,<br> it is better to <strong>have a larger number</strong> of output neurons than the possible number of clusters<br> <strong>because redundant neurons can be eliminated</strong>.<br> After the number of input variables and output neurons has been set,the next step is to<br> <strong>initialize the weights</strong>.These may be set to <strong>small random values</strong>,as was done in the MLP networks.<br> Another possibility is to <strong>randomly choose some input vectors and use their values for the weights</strong>.<br> This has the potential to speed up learning.</p>
</blockquote>
<h3 id="8-4-1-Winner-Selection-Based-on-Neuron-activation"><a href="#8-4-1-Winner-Selection-Based-on-Neuron-activation" class="headerlink" title="8.4.1 Winner Selection Based on Neuron activation"></a>8.4.1 Winner Selection Based on Neuron activation</h3><blockquote>
<p>Once each output neuron has computed its activation,competition can begin.There are several ways<br> this can happen;a simple way is for each neuron to <strong>send its signal</strong> in an inhibitory manner,<br> with <strong>an opposite sign to other neurons</strong>.Once each neuron has received signals from the others,<br> each neuron can compute its <strong>net activation</strong> by simply summing the incoming inhibitory signals and<br> its own activation.If the activation drops below a threshold(or zero),that neuron drops out of the competition.<br> As long as more than one neuron remians,the cycle of inhibition continues until one winner emerges;<br> its output is set to one.This neuron is declared the winner because it has the highest activation<br> and it alone represents the input vector.</p>
</blockquote>
<p><span style="color:blue"><em>Is the opposite sign only the sign,not the opposite activation?</em></span></p>
<h3 id="8-4-2-Winner-Selection-Based-on-Distance-to-Input-vector"><a href="#8-4-2-Winner-Selection-Based-on-Distance-to-Input-vector" class="headerlink" title="8.4.2 Winner Selection Based on Distance to Input vector"></a>8.4.2 Winner Selection Based on Distance to Input vector</h3><blockquote>
<p>Once the distance between an input vector and all the weights has been found,<br> the neuron with <strong>the smallest distance</strong> to the input vector is chosen as the winner,<br> and its weights are updated so that it <strong>moves closer to the input vector</strong>,as<br> \(\Delta\omega_j = \beta(x - \omega_j) = \beta{}d_j\).<br><img src="http://omdhuynsr.bkt.clouddn.com/17-3-6/13980399-file_1488786922883_a440.png" alt title="weight update"></p>
</blockquote>
<h4 id="8-4-2-1-Other-Distance-Measures"><a href="#8-4-2-1-Other-Distance-Measures" class="headerlink" title="8.4.2.1 Other Distance Measures"></a>8.4.2.1 Other Distance Measures</h4><h3 id="8-4-3-Competitive-Learning-Example"><a href="#8-4-3-Competitive-Learning-Example" class="headerlink" title="8.4.3 Competitive Learning Example"></a>8.4.3 Competitive Learning Example</h3><h4 id="8-4-3-1-Recursive-Versus-Batch-Learning"><a href="#8-4-3-1-Recursive-Versus-Batch-Learning" class="headerlink" title="8.4.3.1 Recursive Versus Batch Learning"></a>8.4.3.1 Recursive Versus Batch Learning</h4><blockquote>
<p>In the batch learning,the weight update for each input vector is noted,<br> but the weights are not changed until all the input patterns have been presented.<br> Training terminates when the mean distance between the winning neurons and<br> the inputs they repersent is at a minimum across the entire set of clusters,<br> or when this distance stops changing.</p>
</blockquote>
<h4 id="8-4-3-2-Illustration-of-the-Calculations-Involved-in-Winner-Selection"><a href="#8-4-3-2-Illustration-of-the-Calculations-Involved-in-Winner-Selection" class="headerlink" title="8.4.3.2 Illustration of the Calculations Involved in Winner Selection"></a>8.4.3.2 Illustration of the Calculations Involved in Winner Selection</h4><blockquote>
<p>The training criterion is the mean distance(the sum of the squared distance)<br> between all the inputs and their respective winning neuron weights which<br> represent the cluster centers.<br> The objective of training is to <strong>minimize the mean distance</strong> over iterations.<br> The mean distance \(D\) can be expressed as<br> $$D = \sum_{i=0}^k \sum_{n\in C_i}(x^n - \omega_i)^2$$</p>
</blockquote>
<h4 id="8-4-3-3-Network-Training"><a href="#8-4-3-3-Network-Training" class="headerlink" title="8.4.3.3 Network Training"></a>8.4.3.3 Network Training</h4><h2 id="8-5-Self-Organizing-Feature-Maps"><a href="#8-5-Self-Organizing-Feature-Maps" class="headerlink" title="8.5 Self-Organizing Feature Maps"></a>8.5 Self-Organizing Feature Maps</h2><blockquote>
<p>In SOMs,not only the winner neuron but also neurons in <strong>the neighborhood</strong> of the winner<br> <strong>adjust</strong> their weights together so that a neighborhood of neurons becomes sensitive to a specific input.<br> This neighborhood feature helps to preserve <strong>topological characteristics of inputs</strong>.<br> Therefore,inputs that are spatially closer together must be represented in close proximity<br> in the output layer or map of a network.</p>
</blockquote>
<h3 id="8-5-1-Learning-in-Self-Organizing-Map-Networks"><a href="#8-5-1-Learning-in-Self-Organizing-Map-Networks" class="headerlink" title="8.5.1 Learning in Self-Organizing Map Networks"></a>8.5.1 Learning in Self-Organizing Map Networks</h3><h4 id="8-5-1-1-Selection-of-Neighborhood-Geometry"><a href="#8-5-1-1-Selection-of-Neighborhood-Geometry" class="headerlink" title="8.5.1.1 Selection of Neighborhood Geometry"></a>8.5.1.1 Selection of Neighborhood Geometry</h4><blockquote>
<p>There are several ways to define a neighborhood.<br> <img src="http://omdhuynsr.bkt.clouddn.com/17-3-6/15425123-file_1488797380352_ba79.png" alt><br> If only the most immediate neighbors of the winer are considered,the distance,<br> also called <strong>radius r</strong>,is 1.If two levels of adjacent neighbors are considered,then the radius is 2.</p>
</blockquote>
<h4 id="8-5-1-2-Training-of-Self-Organizing-Maps"><a href="#8-5-1-2-Training-of-Self-Organizing-Maps" class="headerlink" title="8.5.1.2 Training of Self-Organizing Maps"></a>8.5.1.2 Training of Self-Organizing Maps</h4><blockquote>
<p>$$\omega_j^{‘} = \omega_j + \beta NS<em>[x - \omega_j]$$<br> where \(NS\) is the <em>*neighbor strength</em></em> that varies with the distance to a neighbor neuron from the winner.<br> Neighbor strengh defines the strength of weight adjustment of the neighbors with respect to that of the winner.</p>
</blockquote>
<h4 id="8-5-1-3-Neighbor-Strength"><a href="#8-5-1-3-Neighbor-Strength" class="headerlink" title="8.5.1.3 Neighbor Strength"></a>8.5.1.3 Neighbor Strength</h4><blockquote>
<p>The winning neuron update is the most pronounced and the farther away a neighbor neuron is,<br> the less its weight update.The \(NS\) function determines how the weight adjustment<br> <strong>decays</strong> with distance from the winner.There are several possibilities for this function and<br> some commonly usedd functions are <strong>linear,Gaussian,and exponential</strong>.<br> The Gaussian form of the \(NS\) function makes the weight adjustments decay smoothly with distance,<br> and is given by \(NS = Exp[\frac{-d_{i,j}^2}{2\delta^2}]\)<br> The exponential decay \(NS\) function is given by \(NS = Exp[-kd_{i,j}]\)</p>
</blockquote>
<h4 id="8-5-1-4-Example-Training-Self-Organizing-Networks-with-a-Neighbor-Feature"><a href="#8-5-1-4-Example-Training-Self-Organizing-Networks-with-a-Neighbor-Feature" class="headerlink" title="8.5.1.4 Example:Training Self-Organizing Networks with a Neighbor Feature"></a>8.5.1.4 Example:Training Self-Organizing Networks with a Neighbor Feature</h4><h4 id="8-5-1-5-Neighbor-Matrix-and-Distance-to-Neighbors-from-the-Winner"><a href="#8-5-1-5-Neighbor-Matrix-and-Distance-to-Neighbors-from-the-Winner" class="headerlink" title="8.5.1.5 Neighbor Matrix and Distance to Neighbors from the Winner"></a>8.5.1.5 Neighbor Matrix and Distance to Neighbors from the Winner</h4><blockquote>
<p>When the map is large,an efficient method is required to determine the distance of a neighbor<br> from the winner to compute neighor strength.<br> Use a neighbor matrix(\(NM\),also called distance matrix) for a two-dimensional map as a example.<br> For a map of 12 neurons arranged in three rows and four columns,the neighbor matrix for a<br> rectangular neighborhood is<br> $$NM = \begin{bmatrix}<br> 3 &amp; 2 &amp; 2 &amp; 2 &amp; 2 &amp; 2 &amp; 3 \\\\<br> 3 &amp; 2 &amp; 1 &amp; 1 &amp; 1 &amp; 2 &amp; 3 \\\\<br> 3 &amp; 2 &amp; 1 &amp; 0 &amp; 1 &amp; 2 &amp; 3 \\\\<br> 3 &amp; 2 &amp; 1 &amp; 1 &amp; 1 &amp; 2 &amp; 3 \\\\<br> 3 &amp; 2 &amp; 2 &amp; 2 &amp; 2 &amp; 2 &amp; 3<br> \end{bmatrix}_{5\times7}$$<br> Suppose that the horizontal and vertical coordinates of the winner neuron on the two-dimensional map<br> are indicated by (\(i_{win},j_{win})\).Then the distance between the winner and<br> any neighbor neuron at position \((i,j)\) is<br> $$d = NM[\begin{bmatrix} c_1-i_{win}+i,c_2-j_{win}+j \end{bmatrix}]$$<br> where \({c_1,c_2}\) is the position of the winner in the neighbor matrix \(NM\).For this case,<br> \(c_1 = 3\) and \(c_2 = 4\)</p>
</blockquote>
<h4 id="8-5-1-6-Shrinking-Neighborhood-Size-with-iterations"><a href="#8-5-1-6-Shrinking-Neighborhood-Size-with-iterations" class="headerlink" title="8.5.1.6 Shrinking Neighborhood Size with iterations"></a>8.5.1.6 Shrinking Neighborhood Size with iterations</h4><blockquote>
<p><strong>A larger initial neighborhood is necessay because smaller initial neighborhoods can lead to<br> metastable states corresponding to local minima</strong>.However,subsequent <strong>shrinking</strong> of neighborhood<br> is required to further <strong>refine</strong> the representation of the input probability distribution by the map.<br> The equation below shows a linear function commnonly used for this purpose:$$\delta_t = \delta_0(1-t/T)$$<br> Exponential decay is another form used for adjusting neighborhood size with iterations,as given by<br> $$\delta_t = \delta_0Exp[-t/T]$$<br> And the decay in neighborhood size is integrated into the NS function as<br> $$NS(d,t) = Exp[-d_{i,j}^2/2\delta_t^2] = Exp[-d_{i,j}^2/2\\{\delta_0Exp(-t/T)\\}^2]$$<br> where \(T\) is a constant that allows the decay function to decay to zero with iterations.<br> A recommendation is  that the neighborhood size should initially cover almost all neurons in the network<br> when centered on a winning neuron and then shrink slowly with iterations.</p>
</blockquote>
<h4 id="8-5-1-7-Learning-Rate-Decay"><a href="#8-5-1-7-Learning-Rate-Decay" class="headerlink" title="8.5.1.7 Learning Rate Decay"></a>8.5.1.7 Learning Rate Decay</h4><blockquote>
<p>The step length,or the learning rate \(\beta\),is also reduced with iterations in<br> self-organizing learning and a common form of this function is the linear decay,given by<br> $$\beta_t = \beta_0(1-t/T)$$<br> Another form is the exponential decay of the learning rate given by<br> $$\beta_t = \beta_0Exp[-t/T]$$<br> where \(T\) is a time constant that brings the learning rate to a very small value with iterations.<br> A general guide is to start with a relatively high learning rate and let it decrease gradually but<br> remain above 0.01.</p>
</blockquote>
<h4 id="8-5-1-8-Weight-Update-Incorporating-Learning-Rate-and-Neighborhood-Decay"><a href="#8-5-1-8-Weight-Update-Incorporating-Learning-Rate-and-Neighborhood-Decay" class="headerlink" title="8.5.1.8 Weight Update Incorporating Learning Rate and Neighborhood Decay"></a>8.5.1.8 Weight Update Incorporating Learning Rate and Neighborhood Decay</h4><blockquote>
<p>Thus,the weight update after presenting an input vector \(\mathbf{x}\) to a SOM incorporating both<br> neighborhood size and learning rate that decrease with the number of iterations can be expressed as<br> $$\omega_j(t) = \omega_j(t-1) + \beta(t)NS(d,t)[\mathbf{x}(t)-\omega_j(t-1)]$$</p>
</blockquote>
<h4 id="8-5-1-9-Recursive-and-Batch-Training-and-Relation-to-K-Means-Clustering"><a href="#8-5-1-9-Recursive-and-Batch-Training-and-Relation-to-K-Means-Clustering" class="headerlink" title="8.5.1.9 Recursive and Batch Training and Relation to K-Means Clustering"></a>8.5.1.9 Recursive and Batch Training and Relation to K-Means Clustering</h4><blockquote>
<p>In batch mode,the unsupervised algorithm without neighbor feature becomes <strong>equivalent to</strong> K-means clustering.<br> When the neighbor feature is incorporated,it allows <strong>nonlinear projection</strong> of the data as well as<br> the very attractive feature of <strong>topology preservation</strong>,by which regions closer in input space are<br> represented by neurons that are closer in the map.<strong>For this reason it is called a feature map.</strong></p>
</blockquote>
<h4 id="8-5-1-10-Two-Phases-of-Self-Organizing-Map-Training"><a href="#8-5-1-10-Two-Phases-of-Self-Organizing-Map-Training" class="headerlink" title="8.5.1.10 Two Phases of Self-Organizing Map Training"></a>8.5.1.10 Two Phases of Self-Organizing Map Training</h4><blockquote>
<p>Training is usually performed in two phases:ordering and convergence.<br> In the ordering phase,learning rate and neighborhood size are reduces with iterations until<br> the winner or a few neighbors around the winner remain.<br> In the convergence phase,the feature map is fine tuned with the shrunk neighborhood so that<br> it produces an accurate representation of the input space.<br> In this phase,<strong>learning rate is maintained at a small value</strong>,on the order of 0.01,<br> to achieve convergence with good statistical accuracy.Haykin states that the learning rate<br> must not become zero because the network can get stuck in a metastable state that<br> corresponds to a feature map configuration with a topological defect.The \(NS\) function should<br> <strong>contain only the nearest neighbors</strong> of the winning neuron and may <strong>slowly reduce to one or zero neighbors</strong>(i.e.,only the winner remains).</p>
</blockquote>
<h4 id="8-5-1-11-Example-Illustrating-Self-Organizing-Map-Learning-with-a-Hand-Calculations"><a href="#8-5-1-11-Example-Illustrating-Self-Organizing-Map-Learning-with-a-Hand-Calculations" class="headerlink" title="8.5.1.11 Example:Illustrating Self-Organizing Map Learning with a Hand Calculations"></a>8.5.1.11 Example:Illustrating Self-Organizing Map Learning with a Hand Calculations</h4><h4 id="8-5-1-12-SOM-Case-Study-Determination-of-Mastitis-Health-Status-of-Dairy-Herd-from-Combined-Milk-Traits"><a href="#8-5-1-12-SOM-Case-Study-Determination-of-Mastitis-Health-Status-of-Dairy-Herd-from-Combined-Milk-Traits" class="headerlink" title="8.5.1.12 SOM Case Study:Determination of Mastitis Health Status of Dairy Herd from Combined Milk Traits"></a>8.5.1.12 SOM Case Study:Determination of Mastitis Health Status of Dairy Herd from Combined Milk Traits</h4><h3 id="8-5-2-Example-of-Two-Dimensional-Self-Organizing-Maps-Clustering-Canadian-and-Alaskan-Salmon-Based-on-the-Diameter-of-Growth-Rings-of-the-Scales"><a href="#8-5-2-Example-of-Two-Dimensional-Self-Organizing-Maps-Clustering-Canadian-and-Alaskan-Salmon-Based-on-the-Diameter-of-Growth-Rings-of-the-Scales" class="headerlink" title="8.5.2 Example of Two-Dimensional Self-Organizing Maps:Clustering Canadian and Alaskan Salmon Based on the Diameter of Growth Rings of the Scales"></a>8.5.2 Example of Two-Dimensional Self-Organizing Maps:Clustering Canadian and Alaskan Salmon Based on the Diameter of Growth Rings of the Scales</h3><h4 id="8-5-2-1-Map-Structure-and-Initialization"><a href="#8-5-2-1-Map-Structure-and-Initialization" class="headerlink" title="8.5.2.1 Map Structure and Initialization"></a>8.5.2.1 Map Structure and Initialization</h4><h4 id="8-5-2-2-Map-Training"><a href="#8-5-2-2-Map-Training" class="headerlink" title="8.5.2.2 Map Training"></a>8.5.2.2 Map Training</h4><blockquote>
<p>For example,the map was trained using a square neighborhood with learning rate \(\beta\) expressed as<br> $$\beta = \left\\{\begin{array}{ll}<br>    0.01&amp;{t &lt; 5} \\\\<br>    \frac{2}{3+t}&amp;{t &gt; 5}\end{array}\right.$$<br> Learning rate is a <strong>samll constant value</strong> in the first four iterations so that the codebook vectors<br> <strong>find a good orientation</strong>(this is not always done).<br> The neighbor strength function used was<br> $$NS = \left\\{\begin{array}{ll}<br>        Exp[-0.1d]&amp;if \quad t &lt; 5 \\\\<br>        Exp[-\frac{(t-4)}{10}d]&amp;otherwise\end{array}\right.$$<br> During the first four iterations,<strong>all neurons on the map are neighbors</strong> of a winning neuron and<br> <strong>all neighbors are <em>strongly</em> influenced</strong>.The stronger influence on the neighbors in the initial iterations<br> makes the network conform to a nice structure and avoids knots.<br> <strong><em>Ordering phase</em></strong>.The map was trained using <strong>recursive update</strong>.<br> <strong><em>Convergence phase</em></strong>.To finetune and make sure that the map has converged,the trained map was trained further in <strong>batch mode</strong>.<br> Because the network in this case appears to have approached <strong>convergence</strong>,the learning rate<br> has been <strong>set to 1.0</strong> because there will be only small or straightforward adjustments to<br> the position of the codebook vectors with further training.The neighbor strength is <strong>limited to<br> the winning neuron</strong>.If,however,the network has <strong>not approached convergence</strong> in the ordering phase,<br> further training with a smaller constant learning rate on <strong>the order of 0.01</strong> may be appropriate.<br> The neighbor strength then may be <strong>limited to a few neighbors</strong> and decrease to the winner<br> or the nearest neighbors towards the end of training.<br> The final map has reached more data in the outlying regions compared to the map formed<br> at the end of the ordering phase,is expressed as below figure:<br> <img src="http://omdhuynsr.bkt.clouddn.com/17-3-7/10981036-file_1488895472789_17ff2.png" alt><br> <strong>We can set larger number of codebook vectors than the number of classes.</strong><br> So,a cluster of codebook vectors,not a single vector,defines each class.<strong>This gives the map its<br> ability to form nonlinear cluster boundaries.</strong>This cluster structure can be used to<br> discover unknown clusters in data.The map can also be used for subsequent supervised classification.<br> For example,when class labels are known,the codebook vectors that represent corresponding<br> input vectors can <strong>be used as input</strong> to train a feedforward classification network to obtain<br> the calss to which a particular unknown input vector belongs.This is called <strong>learning vector quantization</strong>.<br> Each codebook vector represents the center of gravity of a cluster of inputs that it represents and<br> therefore approximates <strong>the average or point density</strong> of the original distribution in a small cluster region.<br> <strong>Therefore,the magnitude(length) of the codebook vectors should reflect this.</strong> A properly ordered map<br> should show evenly varying length of the codebook vectors on the map.</p>
</blockquote>
<p><span style="color:red">It is a good example of model designment and parameters adjustment.</span><br><span style="color:blue">why the magnitude could relect the point density of the original distribution?</span></p>
<h4 id="8-5-2-3-U-Matrix"><a href="#8-5-2-3-U-Matrix" class="headerlink" title="8.5.2.3 U-Matrix"></a>8.5.2.3 U-Matrix</h4><blockquote>
<p>The distance between the neighboring codebook vectors can highligh different cluster regions<br> in the map and can be a useful visualization tool.The average of the distance to the nearest neighbors<br> is called unified distance,and the matrix of these values for all neurons is called the U-matrix.<br> Thus the map has not only orientated itself in the principal directions of the data,but has also<br> learned to represent the density distribution of the input data,like the figure below:<br> <img src="http://omdhuynsr.bkt.clouddn.com/17-3-8/37014593-file_1488941605615_b781.png" alt></p>
</blockquote>
<p><span style="color:blue">I can understand the figure,but I can’t understand why the distance is average<br>and how to plot the figure.Does the average of the distance mean the average of all input in related cluster regions?</span></p>
<h3 id="8-5-3-Map-Initialization"><a href="#8-5-3-Map-Initialization" class="headerlink" title="8.5.3 Map Initialization"></a>8.5.3 Map Initialization</h3><blockquote>
<p><strong>Random initialization</strong> clusters the initial vectors near the center of gravity of inputs and assigns<br> random values in this cener region.<br> <strong>Deterministic initialization</strong> is another approach,where some input vectors from the dataset<br> are used as initial vectors.This can accelerate map training.<br> Yet another approach is to train a map with random initialization for a few iterations and<br> use the resulting vectors as initial vectors(<strong>random-derministic</strong>).<br> Another possible approach to initialization is to find the first two<br> <strong>principal directions</strong> of data using principal component analysis and<br> use these two directions for map directions.</p>
</blockquote>
<h3 id="8-5-4-Example-Training-Two-Demensional-Maps-on-Multidimensional-Data"><a href="#8-5-4-Example-Training-Two-Demensional-Maps-on-Multidimensional-Data" class="headerlink" title="8.5.4 Example:Training Two-Demensional Maps on Multidimensional Data"></a>8.5.4 Example:Training Two-Demensional Maps on Multidimensional Data</h3><blockquote>
<p>The SOMs can be used not only to cluster input data,but also to <strong>explore the relationship<br> between different attributes of input data.</strong></p>
</blockquote>
<h4 id="8-5-4-1-Data-Visualization"><a href="#8-5-4-1-Data-Visualization" class="headerlink" title="8.5.4.1 Data Visualization"></a>8.5.4.1 Data Visualization</h4><blockquote>
<p>It is a good example for EDA with iris datasets:<br> <img src="http://omdhuynsr.bkt.clouddn.com/17-3-8/91990443-file_1488956403117_5fd3.png" alt><br> Where use color distinct different clusters.</p>
</blockquote>
<p><span style="color:red">It is a good example for EDA.</span></p>
<h4 id="8-5-4-2-Map-Structure-and-Training"><a href="#8-5-4-2-Map-Structure-and-Training" class="headerlink" title="8.5.4.2 Map Structure and Training"></a>8.5.4.2 Map Structure and Training</h4><blockquote>
<p><img src="http://omdhuynsr.bkt.clouddn.com/17-3-8/3745649-file_1488961989125_16d9f.png" alt><br> <img src="http://omdhuynsr.bkt.clouddn.com/17-3-8/88751501-file_1488962053638_139d5.png" alt><br> <img src="http://omdhuynsr.bkt.clouddn.com/17-3-8/50439332-file_1488962099950_d395.png" alt></p>
</blockquote>
<h4 id="8-5-4-3-U-matrix"><a href="#8-5-4-3-U-matrix" class="headerlink" title="8.5.4.3 U-matrix"></a>8.5.4.3 U-matrix</h4><p><span style="color:blue">8.5.4 should be inspection again.It provides many tricks of EDA.</span></p>
<h4 id="8-5-4-4-Point-Estimates-of-Probability-Density-of-Inputs-Caotured-by-the-Map"><a href="#8-5-4-4-Point-Estimates-of-Probability-Density-of-Inputs-Caotured-by-the-Map" class="headerlink" title="8.5.4.4 Point Estimates of Probability Density of Inputs Caotured by the Map"></a>8.5.4.4 Point Estimates of Probability Density of Inputs Caotured by the Map</h4><blockquote>
<p>From the trained map,we can also determine the number of input vectors represented by each neuron.<br> Each neuron represents the local probability density of inputs.In the below figure,the lighter the color,<br> the larger the number of inputs falling onto thar neuron.<br> <img src="http://omdhuynsr.bkt.clouddn.com/17-3-8/29639643-file_1488963168903_8ffa.png" alt></p>
</blockquote>
<h4 id="8-5-4-5-Quantization-Error"><a href="#8-5-4-5-Quantization-Error" class="headerlink" title="8.5.4.5 Quantization Error"></a>8.5.4.5 Quantization Error</h4><blockquote>
<p>Quantization error is a measure of the distance between codebook vectors and inputs.<br> If for an input vector \(\mathbf{x}\),the winner’s weights vector is \(\pmb{\omega}_c\),<br> then the quantization error can be described as a distortion error,\(e\),expressed as<br> $$e = d(\mathbf{x},\pmb{\omega}_c)$$</p>
</blockquote>
<blockquote>
<p>which is the distance from the input to the closet codebook vector.<br> It may be more appropriate to define the distortion error in terms of neighborhood function<br> because the neighbor featuer is central to SOM.With the neighbor feature,<br> the distortion error of the map for an input vector \(\mathbf{x}\) becomes<br> $$e = \sum_{i}NS_{ci}d(\mathbf{x},\pmb{\omega_i})$$<br> where \(NS_{ci}\) is the neighbor strength,\(c\) is the index of the winning neuron closest to input vector \(\mathbf{x}\),<br> and \(i\) is any neuron in the neighborhood of the winner,including the winner.<br> Computing the distortion measure for all input vectors in the input space,the average distortion error \(E\)<br> for the map can be calculated from<br> $$E = \frac{1}{N}\sum_n\sum_iNS_{ci}d(\mathbf{x}^n,\pmb{\omega}_i)$$<br> When the neighbor feature is not used,equation above simplifies to<br> $$E = \frac{1}{N}\sum_nd(\mathbf{x}^n,\pmb{\omega}_i)$$<br> Thus the goal of SOM can alternatively be expressed as finding the set of codebook vectors \(\pmb{\omega}_i\)<br> that <strong>globally minimizes the average map distortion error \(E\)</strong>.<br> The information from figure below can be used to refine the map to obtain a more uniform distortion error measure<br> if a more faithful reproduction of the input distribution from the map is desired.<br> <img src="http://omdhuynsr.bkt.clouddn.com/17-3-8/32153453-file_1488971907071_18164.png" alt></p>
</blockquote>
<p><span style="color:blue">The format of Latex here has some problems,so I split the words to two parts.How could fix it?</span></p>
<h4 id="8-5-4-6-Accuracy-of-Retrieval-of-Input-Data-from-the-Map"><a href="#8-5-4-6-Accuracy-of-Retrieval-of-Input-Data-from-the-Map" class="headerlink" title="8.5.4.6 Accuracy of Retrieval of Input Data from the Map"></a>8.5.4.6 Accuracy of Retrieval of Input Data from the Map</h4><blockquote>
<p>If the dataset is sent through the map,it identifies the best matching codebook vector.The resulting codebook vector<br> can be thought of as the retrieved input because it is the closest to that input.<br> If a neighborhood of neurons is used in the retrieval,more than one codebook vector can be activated and<br> these codebook vectors can be interpolated to obtain a recalled match of the input to the map.Then the retrieved inputs<br> are not the codebook vectors,but <strong>fall between them due to <em>interpolation</em></strong>.<br> The retrieval error is the average distance between the actual data vectors and their corresponding interpolated<br> codebook vectors defining the best position for those input vectors in the trained map.Thus,a neighborhood provides<br> a <strong>better approximation</strong> to this input distribution than a single codebook vector.</p>
</blockquote>
<h3 id="8-5-5-Forming-Clusters-on-the-Map"><a href="#8-5-5-Forming-Clusters-on-the-Map" class="headerlink" title="8.5.5 Forming Clusters on the Map"></a>8.5.5 Forming Clusters on the Map</h3><h4 id="8-5-5-1-Approaches-to-Clustering"><a href="#8-5-5-1-Approaches-to-Clustering" class="headerlink" title="8.5.5.1 Approaches to Clustering"></a>8.5.5.1 Approaches to Clustering</h4><h4 id="8-5-5-2-Example-Illustrating-Clustering-on-a-Trained-Map"><a href="#8-5-5-2-Example-Illustrating-Clustering-on-a-Trained-Map" class="headerlink" title="8.5.5.2 Example Illustrating Clustering on a Trained Map"></a>8.5.5.2 Example Illustrating Clustering on a Trained Map</h4><h3 id="8-5-6-Validation-of-a-Trained-Map"><a href="#8-5-6-Validation-of-a-Trained-Map" class="headerlink" title="8.5.6 Validation of a Trained Map"></a>8.5.6 Validation of a Trained Map</h3><h4 id="8-5-6-1-n-Fold-Cross-Validation"><a href="#8-5-6-1-n-Fold-Cross-Validation" class="headerlink" title="8.5.6.1 n-Fold Cross Validation"></a>8.5.6.1 n-Fold Cross Validation</h4><h2 id="8-6-Evolving-Self-Organizing-Maps"><a href="#8-6-Evolving-Self-Organizing-Maps" class="headerlink" title="8.6 Evolving Self-Organizing Maps"></a>8.6 Evolving Self-Organizing Maps</h2>
          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2019/02/28/machine learning/NN/Time Series Prediction with LSTM Recurrent Neural Networks in Python with Keras/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/02/28/machine learning/NN/Time Series Prediction with LSTM Recurrent Neural Networks in Python with Keras/" class="post-title-link" itemprop="url">Time Series Prediction with LSTM Recurrent Neural Networks in Python with Keras</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-02-28 20:45:06" itemprop="dateCreated datePublished" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/NN/" itemprop="url" rel="index"><span itemprop="name">NN</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p>
<h1 id="来源"><a href="#来源" class="headerlink" title="来源"></a>来源</h1><p>此文为 <a href="http://machinelearningmastery.com/time-series-prediction-lstm-recurrent-neural-networks-python-keras/" target="_blank" rel="noopener">Time Series Prediction with LSTM Recurrent Neural Networks in Python with Keras</a> 的翻译。</p>
<h1 id="翻译"><a href="#翻译" class="headerlink" title="翻译"></a>翻译</h1><p>时序预测问题是预测问题中的一个困难类型。</p>
<p>不像回归预测模型，时序问题存在输入变量中序列依赖的复杂性。</p>
<p>一种强大的神经网络的类型被设计来处理序列依赖，它被称作RNN。LSTM是一种RNN的类型，由于它可以成功训练大型结构，因此被用于深度学习中。</p>
<p>在这篇post中，你将发现如何在python中使用keras深度学习库运用lstm网络去处理一个可示范性的时序预测问题。</p>
<p>在完成这篇教程之后你应该知道如何使用lstm处理自己的时序预测问题和其他更多的通用序列问题。你将知道：</p>
<ul>
<li><p>关于国际航线顾客量的时序问题</p>
</li>
<li><p>怎样使用lstm对时序问题进行回归，窗口和基于时序的框架。</p>
</li>
<li><p>怎样使用长序列下维持state的lstm进行预测</p>
</li>
</ul>
<p>在这个教程中，我们将要发展许多lstm类型进行一个标准时序的预测。</p>
<blockquote>
<p>问题及其选择的lstm配置仅仅只是为了示范目的而不是最优的。</p>
</blockquote>
<p>这些例子将会精确的向你展示你可以怎样发展不一样的lstm结构进行时序预测。</p>
<p>让我们开始吧！</p>
<ul>
<li><p>2016/10更新：每个样本rmse的计算有一个问题。原先的rmse是错误的。现在，rmse</p>
</li>
<li><p>2017/03更新：在keras 2.0.2 tensorflow 1.0.1 和theano 0.9.0 下更新例子</p>
</li>
</ul>
<h1 id="问题描述"><a href="#问题描述" class="headerlink" title="问题描述"></a>问题描述</h1><p>我们在本post中将要研究的问题是国际航班顾客量预测问题。</p>
<p>给定年月，任务是预测国际航班顾客量，以1000为单位。数据范围从1949年1月到1960年12月，12年，144个观察值。</p>
<p>数据集免费可用，来自于 <a href="https://datamarket.com/data/set/22u3/international-airline-passengers-monthly-totals-in-thousands-jan-49-dec-60#!ds=22u3&amp;display=line" target="_blank" rel="noopener">DataMarket webpage as a CSV download</a> ,文件名是<br>“international-airline-passengers.csv“。</p>
<p>下面是文件头几行的一个样本。</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="string">"Month"</span>,<span class="string">"International airline passengers: monthly totals in thousands. Jan 49 ? Dec 60"</span>  </span><br><span class="line"><span class="string">"1949-01"</span>,<span class="number">112</span>  </span><br><span class="line"><span class="string">"1949-02"</span>,<span class="number">118</span>  </span><br><span class="line"><span class="string">"1949-03"</span>,<span class="number">132</span>  </span><br><span class="line"><span class="string">"1949-04"</span>,<span class="number">129</span>  </span><br><span class="line"><span class="string">"1949-05"</span>,<span class="number">121</span></span><br></pre></td></tr></table></figure>
<p>我们可以轻易的使用pandas库导入这个数据集。在观察值是一个月等间隔分割的情况下，我们不需要关心具体date。因此，当我们导入数据的时候我们排除第一列。</p>
<p>下载的数据集也包含页脚信息。我们可以将pandas.read_csv的skipfooter参数设置为3排除最后三行页脚信息。一旦导入数据我们可以轻易的绘制整个数据集图形。导入并绘图的代码在下面给出。</p>
<figure class="highlight xl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">dataset = pandas.read_csv(<span class="string">'international-airline-passengers.csv'</span>, usecols=[<span class="number">1</span>], engine=<span class="string">'python'</span>, skipfooter=<span class="number">3</span>)</span><br><span class="line">plt.plot(dataset)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>你可以看到数据集随着时间的上升趋势。</p>
<p>你也可以看到一些数据集的周期性，可能和北半球的节假日周期有关。</p>
<p>我们将保持事情的简单性并处理数据集。</p>
<p>正常的，调查各种各样的数据预处理技术进行数据转换，并使之平稳是个好主意。</p>
<h1 id="长短期记忆网络（LSTM）"><a href="#长短期记忆网络（LSTM）" class="headerlink" title="长短期记忆网络（LSTM）"></a>长短期记忆网络（LSTM）</h1><p>lstm是一个循环神经网络，它使用后向传播进行训练并且克服了消失的梯度问题。</p>
<p>因此，他可以被用来创造大型循环网络，并反过来被用于处理机器学习中困难的序列问题并获得了目前最佳结果。</p>
<p>代替神经元，lstm网络使用通过层被连接的记忆块。</p>
<p>记忆块（memory block）有一些成分使得它比经典的神经元更加聪明。block包含了管理block状态和输出的门（gates）。block操作一个输入序列，其中的每一个门使用sigmoid激活函数控制其自身触发与否，进行状态改变和信息流的选择性增加。</p>
<p>一个单元中的门有三种类型：</p>
<ul>
<li>forget gate：条件性的选择什么信息从记忆块中剔除</li>
<li>input gate：条件性的选择输入中的什么值用于更新记忆状态</li>
<li>output gate：条件性的决定基于输入和记忆状态输出什么</li>
</ul>
<p>每个单元类似于一个小型状态机。单元中门的权重在训练中被学习。</p>
<p>你可以看到你怎样从一个lstm层中获得一个复杂的学习和记忆。想象基于多个层的高阶抽象也并不困难。</p>
<h1 id="LSTM-进行回归"><a href="#LSTM-进行回归" class="headerlink" title="LSTM 进行回归"></a>LSTM 进行回归</h1><p>我们可以将问题解析为一个回归问题。</p>
<p>那就是，给定这个月的顾客数量，下个月的顾客数量是多少？</p>
<p>我们可以写一个简单的函数把我们单列数据集转换成两列：第一列包含了这个月的顾客数，第二列包含了需要被预测的下个月的顾客数。</p>
<p>在我们开始之前，首先让我们导入所有需要使用的函数和类。这个假定已经安装了scipy和keras环境。</p>
<figure class="highlight elm"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">import</span> pandas</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="title">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="title">from</span> keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"><span class="title">from</span> keras.layers <span class="keyword">import</span> LSTM</span><br><span class="line"><span class="title">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"><span class="title">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br></pre></td></tr></table></figure>
<p>在我们做任何事之前，固定随机数种子确保我们的结果可以复现是个好主意。</p>
<figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"># <span class="built_in">fix</span> <span class="built_in">random</span> seed <span class="keyword">for</span> reproducibility</span><br><span class="line">numpy.<span class="built_in">random</span>.seed(<span class="number">7</span>)</span><br></pre></td></tr></table></figure>
<p>我们也可以使用先前的代码加载数据集形成一个dataframe。接着我们从dataframe中抽取numpy数组，并将整型转化为浮点型，这个类型更加适合神经网络模型。</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># load the dataset</span></span><br><span class="line"><span class="attr">dataframe</span> = pandas.read_csv(<span class="string">'international-airline-passengers.csv'</span>, usecols=[<span class="number">1</span>], engine=<span class="string">'python'</span>, skipfooter=<span class="number">3</span>)</span><br><span class="line"><span class="attr">dataset</span> = dataframe.values</span><br><span class="line"><span class="attr">dataset</span> = dataset.astype(<span class="string">'float32'</span>)</span><br></pre></td></tr></table></figure>
<p>lstm对于输入数据的尺度是敏感的，特别是当使用sigmoid或者tanh激活函数的时候。把数据缩放到0-1之间是一个好的方法，叫做规范化。我们可以使用sklearn的MinMaxScaler预处理类轻松的进行数据缩放。</p>
<figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># normalize the dataset</span></span><br><span class="line"><span class="attr">scaler</span> = MinMaxScaler(feature_range=(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line"><span class="attr">dataset</span> = scaler.fit_transform(dataset)</span><br></pre></td></tr></table></figure>
<p>在我们对数据建模并估计其在训练集上的表现后，我们需要估计模型在未知数据上的表现。对于一个正常的分类或者回归问题，我们通过交叉验证来处理。</p>
<p>时序数据中，值的序惯性很重要。我们可以使用的一个简单方法是将有序数据分裂为训练和测试集。下面的代码计算了分裂点的索引，将67%的数据分为了训练集用于建模，留下了33%的数据用于测试模型。</p>
<figure class="highlight vim"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"># <span class="keyword">split</span> into train <span class="built_in">and</span> test sets</span><br><span class="line">train_size = <span class="keyword">int</span>(<span class="built_in">len</span>(dataset) * <span class="number">0.67</span>)</span><br><span class="line">test_size = <span class="built_in">len</span>(dataset) - train_size</span><br><span class="line">train, test = dataset[<span class="number">0</span>:train_size,:], dataset[train_size:<span class="built_in">len</span>(dataset),:]</span><br><span class="line"><span class="keyword">print</span>(<span class="built_in">len</span>(train), <span class="built_in">len</span>(test))</span><br></pre></td></tr></table></figure>
<p>现在我们可以定义一个函数去创造一个新的数据集，如上面所述的那样。</p>
<p>函数有两个参数：<strong>数据集</strong>，它是一个numpy数组，我们想将它转换为一个数据集。 <strong>look_back</strong>,它是预测下一刻的输入变量的时间窗口，这个例子中默认为1。</p>
<p>它默认将创造一个数据集，X是给定时间的顾客数，Y是下一时刻的顾客数。</p>
<p>它可以被配置参数，我们将会在下一部份构造一个不一样的数据集。</p>
<figure class="highlight maxima"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># <span class="built_in">convert</span> an <span class="built_in">array</span> of <span class="built_in">values</span> into a dataset <span class="built_in">matrix</span></span><br><span class="line">def create_dataset(dataset, look_back=<span class="number">1</span>):</span><br><span class="line">	dataX, dataY = [], []</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> <span class="built_in">range</span>(len(dataset)-look_back-<span class="number">1</span>):</span><br><span class="line">		a = dataset[i:(i+look_back), <span class="number">0</span>]</span><br><span class="line">		dataX.<span class="built_in">append</span>(a)</span><br><span class="line">		dataY.<span class="built_in">append</span>(dataset[i + look_back, <span class="number">0</span>])</span><br><span class="line">	<span class="built_in">return</span> numpy.<span class="built_in">array</span>(dataX), numpy.<span class="built_in">array</span>(dataY)</span><br></pre></td></tr></table></figure>
<p>让我们通过数据集的前几行来看看这个函数的效果</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X		Y</span><br><span class="line"><span class="number">112</span>		<span class="number">118</span></span><br><span class="line"><span class="number">118</span>		<span class="number">132</span></span><br><span class="line"><span class="number">132</span>		<span class="number">129</span></span><br><span class="line"><span class="number">129</span>		<span class="number">121</span></span><br><span class="line"><span class="number">121</span>		<span class="number">135</span></span><br></pre></td></tr></table></figure>
<p>如果你可以就这前五行数据跟前一部分所列出的进行对比，你可以从中发现X=t 和T=t+1的模式。</p>
<p>让我们使用这个函数去准备模型的训练和测试数据集。</p>
<figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># reshape into X=t and Y=t+1</span></span><br><span class="line"><span class="attr">look_back</span> = <span class="number">1</span></span><br><span class="line">trainX, <span class="attr">trainY</span> = create_dataset(train, look_back)</span><br><span class="line">testX, <span class="attr">testY</span> = create_dataset(test, look_back)</span><br></pre></td></tr></table></figure>
<p>lstm需要输入数据通过一种特殊的数组结构被提供，这个形式是：[samples，time steps，features]</p>
<p>当前，我们的数据形式是：[samples，features]，我们需要为每个样本构造1 time steps。我们可以使用 numpy.reshape() 转换训练和测试集成为我们需要的结构。</p>
<figure class="highlight fortran"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"># <span class="built_in">reshape</span> input to be [samples, time steps, features]</span><br><span class="line">trainX = numpy.<span class="built_in">reshape</span>(trainX, (trainX.<span class="built_in">shape</span>[<span class="number">0</span>], <span class="number">1</span>, trainX.<span class="built_in">shape</span>[<span class="number">1</span>]))</span><br><span class="line">testX = numpy.<span class="built_in">reshape</span>(testX, (testX.<span class="built_in">shape</span>[<span class="number">0</span>], <span class="number">1</span>, testX.<span class="built_in">shape</span>[<span class="number">1</span>]))</span><br></pre></td></tr></table></figure>
<p>我们现在准备为这个问题设计并拟合一个lstm网络。</p>
<p>网络有一个1维输入的可见层，一个包含4个lstm block的隐藏层，一个单值输出层。默认的sigmoid激活函数被用于lstm block。网络按照100个epoch和1的batch size进行训练。</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># create and fit the LSTM network</span></span><br><span class="line">model = Sequential()</span><br><span class="line">model.<span class="builtin-name">add</span>(LSTM(4, input_shape=(1, look_back)))</span><br><span class="line">model.<span class="builtin-name">add</span>(Dense(1))</span><br><span class="line">model.compile(<span class="attribute">loss</span>=<span class="string">'mean_squared_error'</span>, <span class="attribute">optimizer</span>=<span class="string">'adam'</span>)</span><br><span class="line">model.fit(trainX, trainY, <span class="attribute">epochs</span>=100, <span class="attribute">batch_size</span>=1, <span class="attribute">verbose</span>=2)</span><br></pre></td></tr></table></figure>
<p>一旦模型拟合，我们可以在训练和测试集上估计模型的效果。这将给我们的新模型一个比较的基准。</p>
<p>注意到我们在计算误差得分前将预测值进行了转换，以确保模型效果是在和原数据单位一致的情况下进行评估的。</p>
<figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># make predictions</span></span><br><span class="line">trainPredict = model.predict(trainX)</span><br><span class="line">testPredict = model.predict(testX)</span><br><span class="line"><span class="comment"># invert predictions</span></span><br><span class="line">trainPredict = scaler.inverse_transform(trainPredict)</span><br><span class="line">trainY = scaler.inverse_transform([trainY])</span><br><span class="line">testPredict = scaler.inverse_transform(testPredict)</span><br><span class="line">testY = scaler.inverse_transform([testY])</span><br><span class="line"><span class="comment"># calculate root mean squared error</span></span><br><span class="line">trainScore = math.sqrt(mean_squared_error(trainY[0], trainPredict[:,0]))</span><br><span class="line">print('Train Score: %.2f RMSE' % (trainScore))</span><br><span class="line">testScore = math.sqrt(mean_squared_error(testY[0], testPredict[:,0]))</span><br><span class="line">print('Test Score: %.2f RMSE' % (testScore))</span><br></pre></td></tr></table></figure>
<p>最后，我们使用模型在训练和测试集上产生了预测值，得到了模型效果的可视化指标。</p>
<p>在以下的绘图中，蓝色是原始数据，绿色是训练集上的拟合数据，红色是在测试集上的拟合数据。</p>
<figure class="highlight makefile"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># shift train predictions for plotting</span></span><br><span class="line">trainPredictPlot = numpy.empty_like(dataset)</span><br><span class="line"><span class="section">trainPredictPlot[:, :] = numpy.nan</span></span><br><span class="line"><span class="section">trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict</span></span><br><span class="line"><span class="comment"># shift test predictions for plotting</span></span><br><span class="line">testPredictPlot = numpy.empty_like(dataset)</span><br><span class="line"><span class="section">testPredictPlot[:, :] = numpy.nan</span></span><br><span class="line"><span class="section">testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict</span></span><br><span class="line"><span class="comment"># plot baseline and predictions</span></span><br><span class="line">plt.plot(scaler.inverse_transform(dataset))</span><br><span class="line">plt.plot(trainPredictPlot)</span><br><span class="line">plt.plot(testPredictPlot)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>我们可以看到模型在训练和测试集上都有很好的效果。</p>
<p>为了完整性，下面是例子所用的全部代码。</p>
<figure class="highlight nix"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># LSTM for international airline passengers problem with regression framing</span></span><br><span class="line"><span class="built_in">import</span> numpy</span><br><span class="line"><span class="built_in">import</span> matplotlib.pyplot as plt</span><br><span class="line">from pandas <span class="built_in">import</span> read_csv</span><br><span class="line"><span class="built_in">import</span> math</span><br><span class="line">from keras.models <span class="built_in">import</span> Sequential</span><br><span class="line">from keras.layers <span class="built_in">import</span> Dense</span><br><span class="line">from keras.layers <span class="built_in">import</span> LSTM</span><br><span class="line">from sklearn.preprocessing <span class="built_in">import</span> MinMaxScaler</span><br><span class="line">from sklearn.metrics <span class="built_in">import</span> mean_squared_error</span><br><span class="line"><span class="comment"># convert an array of values into a dataset matrix</span></span><br><span class="line">def create_dataset(dataset, <span class="attr">look_back=1):</span></span><br><span class="line">	dataX, <span class="attr">dataY</span> = [], []</span><br><span class="line">	for i <span class="keyword">in</span> range(len(dataset)-look_back-<span class="number">1</span>):</span><br><span class="line">		<span class="attr">a</span> = dataset[i:(i+look_back), <span class="number">0</span>]</span><br><span class="line">		dataX.append(a)</span><br><span class="line">		dataY.append(dataset[i + look_back, <span class="number">0</span>])</span><br><span class="line">	return numpy.array(dataX), numpy.array(dataY)</span><br><span class="line"><span class="comment"># fix random seed for reproducibility</span></span><br><span class="line">numpy.random.seed(<span class="number">7</span>)</span><br><span class="line"><span class="comment"># load the dataset</span></span><br><span class="line"><span class="attr">dataframe</span> = read_csv('international-airline-passengers.csv', <span class="attr">usecols=[1],</span> <span class="attr">engine='python',</span> <span class="attr">skipfooter=3)</span></span><br><span class="line"><span class="attr">dataset</span> = dataframe.values</span><br><span class="line"><span class="attr">dataset</span> = dataset.astype('float32')</span><br><span class="line"><span class="comment"># normalize the dataset</span></span><br><span class="line"><span class="attr">scaler</span> = MinMaxScaler(<span class="attr">feature_range=(0,</span> <span class="number">1</span>))</span><br><span class="line"><span class="attr">dataset</span> = scaler.fit_transform(dataset)</span><br><span class="line"><span class="comment"># split into train and test sets</span></span><br><span class="line"><span class="attr">train_size</span> = int(len(dataset) * <span class="number">0.67</span>)</span><br><span class="line"><span class="attr">test_size</span> = len(dataset) - train_size</span><br><span class="line">train, <span class="attr">test</span> = dataset[<span class="number">0</span>:train_size,:], dataset[train_size:len(dataset),:]</span><br><span class="line"><span class="comment"># reshape into X=t and Y=t+1</span></span><br><span class="line"><span class="attr">look_back</span> = <span class="number">1</span></span><br><span class="line">trainX, <span class="attr">trainY</span> = create_dataset(train, look_back)</span><br><span class="line">testX, <span class="attr">testY</span> = create_dataset(test, look_back)</span><br><span class="line"><span class="comment"># reshape input to be [samples, time steps, features]</span></span><br><span class="line"><span class="attr">trainX</span> = numpy.reshape(trainX, (trainX.shape[<span class="number">0</span>], <span class="number">1</span>, trainX.shape[<span class="number">1</span>]))</span><br><span class="line"><span class="attr">testX</span> = numpy.reshape(testX, (testX.shape[<span class="number">0</span>], <span class="number">1</span>, testX.shape[<span class="number">1</span>]))</span><br><span class="line"><span class="comment"># create and fit the LSTM network</span></span><br><span class="line"><span class="attr">model</span> = Sequential()</span><br><span class="line">model.add(LSTM(<span class="number">4</span>, <span class="attr">input_shape=(1,</span> look_back)))</span><br><span class="line">model.add(Dense(<span class="number">1</span>))</span><br><span class="line">model.compile(<span class="attr">loss='mean_squared_error',</span> <span class="attr">optimizer='adam')</span></span><br><span class="line">model.fit(trainX, trainY, <span class="attr">epochs=100,</span> <span class="attr">batch_size=1,</span> <span class="attr">verbose=2)</span></span><br><span class="line"><span class="comment"># make predictions</span></span><br><span class="line"><span class="attr">trainPredict</span> = model.predict(trainX)</span><br><span class="line"><span class="attr">testPredict</span> = model.predict(testX)</span><br><span class="line"><span class="comment"># invert predictions</span></span><br><span class="line"><span class="attr">trainPredict</span> = scaler.inverse_transform(trainPredict)</span><br><span class="line"><span class="attr">trainY</span> = scaler.inverse_transform([trainY])</span><br><span class="line"><span class="attr">testPredict</span> = scaler.inverse_transform(testPredict)</span><br><span class="line"><span class="attr">testY</span> = scaler.inverse_transform([testY])</span><br><span class="line"><span class="comment"># calculate root mean squared error</span></span><br><span class="line"><span class="attr">trainScore</span> = math.sqrt(mean_squared_error(trainY[<span class="number">0</span>], trainPredict[:,<span class="number">0</span>]))</span><br><span class="line">print('Train Score: %.<span class="number">2</span>f RMSE' % (trainScore))</span><br><span class="line"><span class="attr">testScore</span> = math.sqrt(mean_squared_error(testY[<span class="number">0</span>], testPredict[:,<span class="number">0</span>]))</span><br><span class="line">print('Test Score: %.<span class="number">2</span>f RMSE' % (testScore))</span><br><span class="line"><span class="comment"># shift train predictions for plotting</span></span><br><span class="line"><span class="attr">trainPredictPlot</span> = numpy.empty_like(dataset)</span><br><span class="line">trainPredictPlot[:, :] = numpy.nan</span><br><span class="line">trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict</span><br><span class="line"><span class="comment"># shift test predictions for plotting</span></span><br><span class="line"><span class="attr">testPredictPlot</span> = numpy.empty_like(dataset)</span><br><span class="line">testPredictPlot[:, :] = numpy.nan</span><br><span class="line">testPredictPlot[len(trainPredict)+(look_back*<span class="number">2</span>)+<span class="number">1</span>:len(dataset)-<span class="number">1</span>, :] = testPredict</span><br><span class="line"><span class="comment"># plot baseline and predictions</span></span><br><span class="line">plt.plot(scaler.inverse_transform(dataset))</span><br><span class="line">plt.plot(trainPredictPlot)</span><br><span class="line">plt.plot(testPredictPlot)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p>运行例子代码会产生如下输出。</p>
<figure class="highlight subunit"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">...</span><br><span class="line">Epoch 95/100</span><br><span class="line">0s - loss: 0.0020</span><br><span class="line">Epoch 96/100</span><br><span class="line">0s - loss: 0.0020</span><br><span class="line">Epoch 97/100</span><br><span class="line">0s - loss: 0.0020</span><br><span class="line">Epoch 98/100</span><br><span class="line">0s - loss: 0.0020</span><br><span class="line">Epoch 99/100</span><br><span class="line">0s - loss: 0.0020</span><br><span class="line">Epoch 100/100</span><br><span class="line">0s - loss: 0.0020</span><br><span class="line">Train Score: 22.93 RMSE</span><br><span class="line"><span class="keyword">Test </span>Score: 47.53 RMSE</span><br></pre></td></tr></table></figure>
<p>我们可以看到模型在训练集上有23个顾客作用的平均误差，在测试集上有52个顾客左右的平均误差。并不算坏。</p>
<h1 id="使用窗口方法进行lstm回归"><a href="#使用窗口方法进行lstm回归" class="headerlink" title="使用窗口方法进行lstm回归"></a>使用窗口方法进行lstm回归</h1><p>我们也可以使用过往多个时刻的数据来预测下一时刻顾客量。</p>
<p>这个叫做窗口，窗口的尺寸是需要进行调整的参数。</p>
<p>例如，给定当前时刻，我们希望预测下一时刻数据，我们可以使用当前时刻，以及上一时刻，上上时刻作为输入变量。</p>
<p>作为一个回归问题，输入变量是t-2,t-1,t，输出变量是t+1。</p>
<p>我们前面写的 create_dataset() 函数允许我们可以构造出这样形式的数据集，只需要把 look_back 参数调整为3。</p>
<p>新构造的数据集前几行如下：</p>
<figure class="highlight lsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">X1	X2	X3	Y</span><br><span class="line"><span class="number">112</span>	<span class="number">118</span>	<span class="number">132</span>	<span class="number">129</span></span><br><span class="line"><span class="number">118</span>	<span class="number">132</span>	<span class="number">129</span>	<span class="number">121</span></span><br><span class="line"><span class="number">132</span>	<span class="number">129</span>	<span class="number">121</span>	<span class="number">135</span></span><br><span class="line"><span class="number">129</span>	<span class="number">121</span>	<span class="number">135</span>	<span class="number">148</span></span><br><span class="line"><span class="number">121</span>	<span class="number">135</span>	<span class="number">148</span>	<span class="number">148</span></span><br></pre></td></tr></table></figure>
<p>我们可以使用这个新的窗口数据重新预测。完整代码如下，这有窗口做了改变。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># LSTM for international airline passengers problem with window regression framing</span></span><br><span class="line"><span class="keyword">import</span> numpy</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> read_csv</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> keras.models <span class="keyword">import</span> Sequential</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> Dense</span><br><span class="line"><span class="keyword">from</span> keras.layers <span class="keyword">import</span> LSTM</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"><span class="comment"># convert an array of values into a dataset matrix</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">create_dataset</span><span class="params">(dataset, look_back=<span class="number">1</span>)</span>:</span></span><br><span class="line">	dataX, dataY = [], []</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> range(len(dataset)-look_back<span class="number">-1</span>):</span><br><span class="line">		a = dataset[i:(i+look_back), <span class="number">0</span>]</span><br><span class="line">		dataX.append(a)</span><br><span class="line">		dataY.append(dataset[i + look_back, <span class="number">0</span>])</span><br><span class="line">	<span class="keyword">return</span> numpy.array(dataX), numpy.array(dataY)</span><br><span class="line"><span class="comment"># fix random seed for reproducibility</span></span><br><span class="line">numpy.random.seed(<span class="number">7</span>)</span><br><span class="line"><span class="comment"># load the dataset</span></span><br><span class="line">dataframe = read_csv(<span class="string">'international-airline-passengers.csv'</span>, usecols=[<span class="number">1</span>], engine=<span class="string">'python'</span>, skipfooter=<span class="number">3</span>)</span><br><span class="line">dataset = dataframe.values</span><br><span class="line">dataset = dataset.astype(<span class="string">'float32'</span>)</span><br><span class="line"><span class="comment"># normalize the dataset</span></span><br><span class="line">scaler = MinMaxScaler(feature_range=(<span class="number">0</span>, <span class="number">1</span>))</span><br><span class="line">dataset = scaler.fit_transform(dataset)</span><br><span class="line"><span class="comment"># split into train and test sets</span></span><br><span class="line">train_size = int(len(dataset) * <span class="number">0.67</span>)</span><br><span class="line">test_size = len(dataset) - train_size</span><br><span class="line">train, test = dataset[<span class="number">0</span>:train_size,:], dataset[train_size:len(dataset),:]</span><br><span class="line"><span class="comment"># reshape into X=t and Y=t+1</span></span><br><span class="line">look_back = <span class="number">3</span></span><br><span class="line">trainX, trainY = create_dataset(train, look_back)</span><br><span class="line">testX, testY = create_dataset(test, look_back)</span><br><span class="line"><span class="comment"># reshape input to be [samples, time steps, features]</span></span><br><span class="line">trainX = numpy.reshape(trainX, (trainX.shape[<span class="number">0</span>], <span class="number">1</span>, trainX.shape[<span class="number">1</span>]))</span><br><span class="line">testX = numpy.reshape(testX, (testX.shape[<span class="number">0</span>], <span class="number">1</span>, testX.shape[<span class="number">1</span>]))</span><br><span class="line"><span class="comment"># create and fit the LSTM network</span></span><br><span class="line">model = Sequential()</span><br><span class="line">model.add(LSTM(<span class="number">4</span>, input_shape=(<span class="number">1</span>, look_back)))</span><br><span class="line">model.add(Dense(<span class="number">1</span>))</span><br><span class="line">model.compile(loss=<span class="string">'mean_squared_error'</span>, optimizer=<span class="string">'adam'</span>)</span><br><span class="line">model.fit(trainX, trainY, epochs=<span class="number">100</span>, batch_size=<span class="number">1</span>, verbose=<span class="number">2</span>)</span><br><span class="line"><span class="comment"># make predictions</span></span><br><span class="line">trainPredict = model.predict(trainX)</span><br><span class="line">testPredict = model.predict(testX)</span><br><span class="line"><span class="comment"># invert predictions</span></span><br><span class="line">trainPredict = scaler.inverse_transform(trainPredict)</span><br><span class="line">trainY = scaler.inverse_transform([trainY])</span><br><span class="line">testPredict = scaler.inverse_transform(testPredict)</span><br><span class="line">testY = scaler.inverse_transform([testY])</span><br><span class="line"><span class="comment"># calculate root mean squared error</span></span><br><span class="line">trainScore = math.sqrt(mean_squared_error(trainY[<span class="number">0</span>], trainPredict[:,<span class="number">0</span>]))</span><br><span class="line">print(<span class="string">'Train Score: %.2f RMSE'</span> % (trainScore))</span><br><span class="line">testScore = math.sqrt(mean_squared_error(testY[<span class="number">0</span>], testPredict[:,<span class="number">0</span>]))</span><br><span class="line">print(<span class="string">'Test Score: %.2f RMSE'</span> % (testScore))</span><br><span class="line"><span class="comment"># shift train predictions for plotting</span></span><br><span class="line">trainPredictPlot = numpy.empty_like(dataset)</span><br><span class="line">trainPredictPlot[:, :] = numpy.nan</span><br><span class="line">trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict</span><br><span class="line"><span class="comment"># shift test predictions for plotting</span></span><br><span class="line">testPredictPlot = numpy.empty_like(dataset)</span><br><span class="line">testPredictPlot[:, :] = numpy.nan</span><br><span class="line">testPredictPlot[len(trainPredict)+(look_back*<span class="number">2</span>)+<span class="number">1</span>:len(dataset)<span class="number">-1</span>, :] = testPredict</span><br><span class="line"><span class="comment"># plot baseline and predictions</span></span><br><span class="line">plt.plot(scaler.inverse_transform(dataset))</span><br><span class="line">plt.plot(trainPredictPlot)</span><br><span class="line">plt.plot(testPredictPlot)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<h1 id="使用time-steps进行lstm回归"><a href="#使用time-steps进行lstm回归" class="headerlink" title="使用time steps进行lstm回归"></a>使用time steps进行lstm回归</h1>
          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2019/02/28/pansee/book review/《娱乐至死》/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/02/28/pansee/book review/《娱乐至死》/" class="post-title-link" itemprop="url">《娱乐至死》</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-02-28 20:45:06" itemprop="dateCreated datePublished" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/book-review/" itemprop="url" rel="index"><span itemprop="name">book review</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="总评"><a href="#总评" class="headerlink" title="总评"></a>总评</h1><p>本书所论述的乃是电视时代对于印刷术时代的冲击。是支离破碎的话语结构对于严肃逻辑话语结构的冲击。作者认为支离破碎的话语结构将会使得生活娱乐化，思想娱乐化。更确切的说，只剩下娱乐，而无生活和思想。</p>
<p>然而，窃以为虽然该书很好的抓住了话语结构的转变，以及价值倾向上的转变。但是对其逻辑实难认可。</p>
<ol>
<li><p>作者在论述上列举了大量的例子来说明印刷术时代民众的严肃性以及与之形成强烈对比的电视时代的娱乐性。但是，首先举例子从来都没法证明逻辑或观点，至多不过是增强读者对其所欲证明观点的理解罢了。而该书缺乏足够的逻辑论证，甚为遗憾。其次，其所举例子的偏向性实在太过明显，不得不让人怀疑其客观性。</p>
</li>
<li><p>表面的因果并不一定是真实的因果。这个问题在统计学中有一个专有名词“伪回归”。也就是说，娱乐化表面上看起来是随着电视而发展的，但或许这只是表面而已。我个人更认为娱乐化不过是一个典型的“饱暖思淫欲”的过程。同时，难道电视时代之前娱乐就不像现下这样嚣张了么？只是缺少传播媒介，娱乐只是在小范围内形成。而现下借助于网络形成了一个足以引起大范围共鸣的娱乐形式，使得娱乐可以触及生活的边边角角并被推波助澜而已。</p>
</li>
<li><p>作者并没有掺杂过多的个人观点，而是希望安静的描述事实。这个动机至少是值得赞扬的，尽管无可避免的带来了一定事实上我认为是相当程度的误导性。很克制的没有在明面上作出自己的猜想和推断，让读者自己去感受“部分事实”。</p>
</li>
<li><p>印刷术时代真的有想像的那么美好么？这样的美好即使存在，也很难归因于书面话语结构。印刷初期的高昂成本以及他所面对的精英受众都使得娱乐很难出现。至于书本如果能够在底层传播，那也相对部分是由国家公权力或者教会这样的庞大势力所推动的。一旦大众有更多的自由选择权利，一旦印刷成本降低，读书认字不再成为精英特权，书本的庸俗化是可以想见的，并且已经被今天的社会所证明，一如大半个书架的言情，玄幻，以及各种以文字修辞和精美修饰包之以文艺外衣来进行推销的娱乐读物。部分所谓“文青”亦如是。但是不可否认的，相较于书面话语结构，电视这样的影像结构更加容易使人娱乐并诱人娱乐。</p>
</li>
<li><p>影像结构具有哪些好处呢？书本的表现力是很受限制的。直接的视觉冲击和听觉冲击至少在某些方面是要远远强于书本的。作者却对此几乎不提，实在偏颇。纵然“隐晦”的提了，比如在论述教育娱乐化时所举的鲸鱼习性录制的例子，竟然还要以成本高昂作为批评。虽然成本过高会带来商业化偏向，但至少影像结构本身的价值是显而易见的。另外，真理不等于逻辑。有很多美好的东西是不依赖于逻辑的。同样，作者对于印刷术时代的回忆更多的在于民众对于公共事务的热衷。很遗憾的，这并不是生活的全部。又将诗歌音乐等至于何地呢？作者更多的使用了线性结构一词来描述印刷术时代的话语结构特点。我以为这是不恰当的。至少我们可以看到《尤利西斯》等作品对于线性结构的突破。当然，或许作者看到的确乎只有线性结构，诗歌音乐等在他看来其实并不比电视好多少。不知是否确乎如此，似乎书中对此是暗含批评态度的。好像一个严肃的人的生活，就只能是关心身边的事，积极参与公共事务一样。</p>
</li>
<li><p>影像结构的高成本带来的盈利诉求以及其受众广泛的潜在可能性带来的资本涌入，导致了娱乐化。实际上，对于前者而言，现下独立电影的成本已经很低了，并不构成盈利诉求。资本涌入已经成了影像结构娱乐化最主要的根源。那么，到了这里我们是否可以说，真正导致娱乐化的不是电视，而是资本呢！只是电视的形式能够更好的实现商业盈利目的，从而成为资本重点关注的对象。</p>
</li>
<li><p>电视为何会构成一个支离破碎，缺少上下文的话语结构？作者对此的批评至少有以下两个方面：电视节目着力于场景的呈现与感受，而不是严密的逻辑语辞;为了吸引受众而降低门槛，节目片段之间也保持相对独立性，不再完整。对于前者，我觉得这恰恰是影像结构的优势。不是通过枯燥而又不一定站的住脚的逻辑，而是通过感同身受的生活场景的呈现来打动你，引起共鸣，这实在是没有什么非说不可的坏处吧？至于后者，难道不是古以有之么？要不然哪里来的“且听下文分解”？更何况，就完整的一个电视节目而言，纵然被切分成了很多小块，也不至于彻底的不需要依赖上下文吧？同样，一个可以得到赞同的电视节目，也不可能只是毫无逻辑和中心的梦人呓语吧？至于如果有人想要拿电视节目的深度说事，那么我只能说印刷物中的垃圾作品也实在不少。</p>
</li>
<li><p>电视时代真正的威胁应当是以下两点。一是信息过剩与信息不足之下的麻木，更确切的说，一方面将过剩信息当作思想而不再需要思考，一方面被过剩的信息淹没，进一步忽略了个人生活本身。同样也是因为信息源源不断的汹涌而来，才导致了话语结构的碎片化。因为信息太多，已经无法驻足停留了。另一方面，为了迎合大众而进行信息的扭曲和重塑，使得问题本身被掩盖，真实被表演所替代。人可以在自己虚幻的城堡里醉生梦死。只是，如果这个梦可以一直持续，又为什么需要醒来呢？更何况，一个可以一直持续的梦是梦还是真实？其实可以构建一个风险博弈模型来看看是否有醒来的必要，此处不做引申。</p>
</li>
<li><p>这里有一个重要的问题需要讨论。借用作者的话，“媒介构成了真理的一部分”。当真如此么？以电视为代表的技术，资本，价值三者的关系是怎样的呢？技术暂且放到一边吧，至少我不相信有什么技术一被发明出来就带上了恶的属性。那么，资本有没有可能凭空创造一种恶的价值呢？我是不大相信的，但我相信撒旦可以引导出人性恶的部分并加以放大直至完全侵蚀。请注意，我对资本不报恶意，这仅仅是一个不甚恰当的比方罢了！人是软弱的，不堪诱惑。然而，如果因此要归罪于技术或者资本就未免卑劣了。</p>
</li>
<li><p>最后，我不得不说，作者或许无形之中接近了《一九八四》的文化专制。漠视普通大众的诉求，对娱乐嗤之以鼻，高举精英主义文化的大旗。不难猜想，如果所谓正统学人窃居高位，自以为掌握了文化精髓，随着而来只能是文化专制。大众话语权的增长应该是件好事。当然我们更应该努力的乃是大众意识的觉醒。否则很容易沦为暴民政治，庸俗文化。不管是否精英主义者，不管是否相信民众可以教化，至少对于娱乐对于大众应当抱有更多的宽容。</p>
</li>
</ol>
<p>最后，不要消费苦难！能够躺着赚钱，为什么要去风吹日晒。对于娱乐，对于金钱嗤之以鼻，实在不能彰显思想者的格调！</p>
<h1 id="第一章-媒介即隐喻"><a href="#第一章-媒介即隐喻" class="headerlink" title="第一章 媒介即隐喻"></a>第一章 媒介即隐喻</h1><p>18世纪后期——波士顿——独立战争的序幕——民兵雕像</p>
<p>19世纪中叶——纽约——大熔炉式国家——自由女神像</p>
<p>20世纪早期——芝加哥——工业发展中心——屠夫雕像</p>
<p>20世纪后期——拉斯维加斯——娱乐之城——老虎机图片和歌舞女演员</p>
<blockquote>
<p>以总统竞选举例，智慧让位于化妆术。</p>
</blockquote>
<p>今日之境况同样如此。颜值已经堂而皇之的成为一种资产和凭证，遍布于职业选择，婚姻嫁娶，公众舆论方方面面，不论是否合宜或有必要。</p>
<blockquote>
<p>与其说经济学是一门学科，还不如说它是一种表演艺术  </p>
</blockquote>
<p>广告和外包装虽然可以部分解决信息不对称的问题，但现下确实更多的只是奇技淫巧，而没有花更多精力去提升产品质量本身。真是囚凸困境啊。大数据，共享单车的创投也是如此。</p>
<blockquote>
<p>宗教和学术也以取悦人为目的</p>
</blockquote>
<p>高校的娱乐化，阅读的低俗化，文青化何尝不是，部分所谓心理学之幼稚可笑空谈感受自不必谈。</p>
<blockquote>
<p>媒介即信息……某个文化中交流的媒介对于这个文化精神重心和物质重心的形成有着决定性的影响</p>
</blockquote>
<p>明确的指出了交流形式对于交流内容的反制。</p>
<blockquote>
<p>虽然文化是语言的产物，但是每一种媒介都会对它进行再创造</p>
</blockquote>
<p>文化真的只是语言的产物么？我承认语言或者其他交流媒介对于文化的形成有影响，但我很难接受文化内容本身完全受制于媒介。应当存在更为根本的决定性的东西才对。</p>
<h1 id="第二章-媒介即认识论"><a href="#第二章-媒介即认识论" class="headerlink" title="第二章 媒介即认识论"></a>第二章 媒介即认识论</h1><blockquote>
<p>我们衡量一种文化，是要看其中自认为重要的东西，而不是看那些毫无伪装的琐碎小事</p>
</blockquote>
<p>我们读书，也应当是汲取书中精华，而不是把时间浪费在对着书中的错误大加批判。当然，如果一本书，或者一个人他所带来的负面影响极为严重不可规避，那么也只好不要为了所谓精华而去接近了。进步的机会很多，而堕落只在于瞬间。</p>
<blockquote>
<p>任何一种媒介都有共鸣，因为共鸣就是扩大的隐喻。不管一种媒介原来的语境是怎样的，他都有能力越过这个语境并延伸到新的未知的语境中。</p>
</blockquote>
<blockquote>
<p>对于真理的认识是同表达方式密切相联的。真理不能、也从来没有，毫无修饰地存在。它必须穿着某种合适的外衣出现，否则就可能得不到承认，这也正说明了”真理“是一种文化偏见。</p>
</blockquote>
<p>认识论的发展：</p>
<p>神话宗教语言——科学语言</p>
<p>逻辑推理——回归事实</p>
<p>谚语俗语——书面语言</p>
<p>印刷术——电视</p>
<blockquote>
<p>任何认识论都是某个媒介发展阶段的认识论</p>
</blockquote>
<h1 id="第三章-印刷机统治下的美国"><a href="#第三章-印刷机统治下的美国" class="headerlink" title="第三章 印刷机统治下的美国"></a>第三章 印刷机统治下的美国</h1><blockquote>
<p>不可记录汝等之教义，更不可将其印刷成文，否则汝等将永远受其束缚</p>
</blockquote>
<blockquote>
<p>这种情形的造成只有一部分是受新教传统的影响……美国是一个由知识分子建立的国家，这在现代历史上是罕见的<br>新教传统的影响体现在哪里？真的只有一部分么？</p>
</blockquote>
<p>本章主要叙述了美国初期的人员构成，大部分都是知识分子或者具有阅读习惯的人。</p>
<h1 id="第四章-印刷机统治下的思想"><a href="#第四章-印刷机统治下的思想" class="headerlink" title="第四章 印刷机统治下的思想"></a>第四章 印刷机统治下的思想</h1><p>”阐释年代“——”娱乐业时代“</p>
<p>本章则是通过实例描绘了初期美国人民的真实生活境况，方便进一步论述其观点。</p>
<h1 id="第五章-躲躲猫的世界"><a href="#第五章-躲躲猫的世界" class="headerlink" title="第五章 躲躲猫的世界"></a>第五章 躲躲猫的世界</h1><p>或许可以从信息获取难易的角度去分析这一转变。人倾向于闲谈放松，只是过往难以获得谈资，只好严肃阅读？同样也因为精英阶层衰弱，平民庸俗文化兴起，使得娱乐大行其道。</p>
<p>美国初期的人口结构或许是知识分子占大多数，但随着后期移民，兼容土著，人口繁衍，这样的人口结构显然会发生变化。因此阐释年代的消亡或许是必然的。或者说阐释年代本来就只可以存在于小部分精英知识分子中间，对于占据社会大部分的工农阶层或许从来就没有过阐释年代。一如古中国一边是记录在案的谈笑有鸿儒，一边是隐匿消失却占据大部分的汗滴禾下土。</p>
<blockquote>
<p>有一些媒介，例如电影，从本质上就具有这样的潜能。</p>
</blockquote>
<p>窃以为作者对于摄影，电影等的认知存在偏差。“电影将人的生命延长了十倍”，我们有什么理由不通过他人的著述，经历去吸取经验教训，从而使得自己能够更高效的生活思考？当然，无可否认的，相较于少量的优秀电影作品，更多的是大量的娱乐电影而已。然而，这并不由电影这一媒介所决定。事实上，即使是印刷书籍不也明显的是如此的优劣比例分配么。至于过往时代，书籍大多优质，不过是因为书籍的写作权被掌控在精英阶层手上而已。拜偶像，著书立说真正带来的坏影响更本不是影响上帝教义，受印刷物之束缚。而是在这一过程之中，上帝的神圣话语权会被破坏！而现下，任何一个人都可以去写作，则是人本的另一极端。甚至于一个人竟然可以狂妄谈论涉及一个他完全缺乏知识的领域，也就是所谓民哲所为。这样的庸俗个人主义又将让社会走向何方呢？</p>
<p>我完全不能认同作者的根本前提与立意。我严重质疑作者犯了类似于幸存者偏差的错误！选择作为特例的美国来进行论述从一开始就是不合宜的！</p>
<p>即使回过头来看作者自己所举的初期美国民众热衷听演讲参与社会事务的例子，我也不经要怀疑那些民众当真不怀抱着一种娱乐的目的么？这叫好像学生热烈的考证难道真的是怀抱着一种学术的或者求知的目的么？</p>
<h1 id="第六章-娱乐业时代"><a href="#第六章-娱乐业时代" class="headerlink" title="第六章 娱乐业时代"></a>第六章 娱乐业时代</h1><blockquote>
<p>印刷术从来没有被专用于、或大量用于复制图像</p>
</blockquote>
<p>初期印刷术有用于复制图像的能力么？即使有能力，难道有复制图像的需求么？我完全接受印刷术是为了书面文字服务的，但应当是有需求推进技术革新，而不是相反！需求是本来就有的！</p>
<blockquote>
<p>我们的问题不在于电视为我们展示具有娱乐性的内容，而在于所有的内容都以娱乐的方式表现出来，这就完全是另一回事了。</p>
</blockquote>
<p>并不是所有的内容吧？难道初期就没有纪录片？没有好电影？深感作者看问题缺乏公正。</p>
<blockquote>
<p>播音员也会对观众说“明天用一时间再见”。为什么要再见？照理说，几分钟的屠杀和灾难应该会让我们整整一个月难以入眠，但现在我们却接受了播音员的邀请，因为我们知道“新闻”是不必当真的，是说着玩的。</p>
</blockquote>
<p>这倒是很有意思，值得细思。</p>
<blockquote>
<p>我们从来没有听说过，一种媒介的表现形式可以和这种媒介本身的倾向相对抗。</p>
</blockquote>
<p>我依旧无法理解，作者是如何论证出电视的娱乐倾向的。</p>
<h1 id="第七章-“好……现在“"><a href="#第七章-“好……现在“" class="headerlink" title="第七章 “好……现在“"></a>第七章 “好……现在“</h1><blockquote>
<p>它已经成为当今美国公众话语支离破碎的一种象征。</p>
</blockquote>
<p>支离破碎，很关键的一个词，对于此书的理解。</p>
<blockquote>
<p>难道电视为了迎合观众的喜好可以是非不分吗？</p>
</blockquote>
<p>当然不至于是非不分，但至少为了迎合观众换掉一个播演员真的没有其合理性么？可以展开思考。</p>
<blockquote>
<p>电视告诉杂志”新闻是一种娱乐“，杂志转而告诉电视”只有娱乐才是新闻“</p>
</blockquote>
<p>难道严肃印刷物的娱乐化也要怪罪与电视，或者怪罪于印刷这一媒介么？显然作者的观点站不住脚。</p>
<blockquote>
<p>广播本身的特点使它非常适合传播理性而复杂的语言</p>
</blockquote>
<p>没感觉，我觉得同时调动视觉和听觉从而可以看到肢体语言听到口头语言的电视更可以传播复杂语言。</p>
<blockquote>
<p>但历史从来没有证明过，一个自认为可以在22分钟内评价整个世界的文化还会有生存的能力。除非，新闻的价值取决于它能带来多少笑声。</p>
</blockquote>
<p>现在不就证明给你看了么？有为何要视而不见，过度修正？而不是直面问题？</p>
<h1 id="第八章-走向伯利恒"><a href="#第八章-走向伯利恒" class="headerlink" title="第八章 走向伯利恒"></a>第八章 走向伯利恒</h1><p>电视的魔力使信徒不堪诱惑，走向撒旦。然而并非只有电视是撒旦的代表而已。</p>
<h1 id="第九章-伸出你的手投上一票"><a href="#第九章-伸出你的手投上一票" class="headerlink" title="第九章 伸出你的手投上一票"></a>第九章 伸出你的手投上一票</h1><blockquote>
<p>如果政治真的像娱乐业，那么它的目的就不是追求一目了然、公正诚实和超越平凡，而是要做到看上去像这样。</p>
</blockquote>
<blockquote>
<p>电视广告把企业从哦你生产有价值的产品引向了设法使消费者感觉产品有价值，这意味着企业的业务已经成为一种伪疗法，消费者成了信赖心里表演疗法的病人。</p>
</blockquote>
<p>价值和使用价值的关系貌似并不是资本主义的问题。就使用价值而言，如果真的能够让消费者感到有价值，那么就足够了，并不能说就是伪疗法。那是更深一层的价值取向的问题。</p>
<blockquote>
<p>在这些广告里，他运用了类似麦当劳广告的视觉手段把自己表现成一个经验丰富、正直虔诚的人。</p>
</blockquote>
<blockquote>
<p>那些想当上帝的人把自己塑造成观众期望的形象。</p>
</blockquote>
<blockquote>
<p>在一个本身结构就是偏向图像和片段的媒介里，我们注定要丧失历史的视角。</p>
</blockquote>
<blockquote>
<p>公司国家通过电视控制了美国公众话语的流动。……我们要担心的是电视信息的过剩，而不是政府的限制……（公司国家：认为国家是一台巨大的机器，完全不受人的控制并置人的价值于不顾）</p>
</blockquote>
<blockquote>
<p>她想尽一切办法使信息变得没有内容、没有历史、没有语境，也就是说，信息被包装成为娱乐。</p>
</blockquote>
<h1 id="第十章-教学是一种娱乐活动"><a href="#第十章-教学是一种娱乐活动" class="headerlink" title="第十章 教学是一种娱乐活动"></a>第十章 教学是一种娱乐活动</h1><blockquote>
<p>电视对教育哲学的主要贡献是它提出了教学和娱乐不可分的理念。</p>
</blockquote>
<p>不是不可分，而是可以在娱乐中学。</p>
<blockquote>
<p>获得知识是一件困难的事情……教育的目的是让学生们摆脱现实的奴役，而现在的年轻人正竭力作着相反的努力——为了适应现实而改变自己</p>
</blockquote>
<p>算不上适应现实，人本性难道会自找苦吃么？电视教育提供了虚假的捷径？培训班也是相同？</p>
<p>将电视教育一棒子打死，论证有偏。</p>
<blockquote>
<p>在观看了两个30s长的商业电视节目和广告之后，只有3.5%的观众可以正确回答和节目相关的12个判断对错的问题。</p>
</blockquote>
<p>难道看书正确率就会提高？作者误导性论证太过！再者，总长60s，是如何设计出12个问题的？或者说这12个问题是不是过于细节刁难？<br>再者，印刷和电视作为两种不同媒介，怎么能够这样直接对比？何况还是拿广告和书籍对比？那么不妨换一种对比媒介，网络新闻或者其他类似印刷物。<br>不可否认，传统纸质书籍确实需要凝神细看，强调注意力，但是又有多少人看的下去？它的使用门槛带来的坏处就可以无视么？电视本来就不是作为印刷书的替代品而存在的！至于在电视出现之后，印刷书式微，难道不是因为它自己的缺陷原因？</p>
<blockquote>
<p>为什么“鲸鱼和它们的生活环境”会成为一个如此有趣的话题，值得人们花上一整年的时间来制作这个节目？……盲目而无形地花掉了365万美元……但这些他们也完全可以通过其他途径获得。最重要的的是，他们知道了学习是一种娱乐方式</p>
</blockquote>
<p>好吧，作者开始抨击动物世界了？请问有多少人看书也不过就是抱着这样的获取生活中用不到的知识的目的呢？为何对此你却不加批判？<br>何况有的情况下视频图像比印刷有好的多的教学效果，就比如对于鲸鱼的习性，海上航行。</p>
<p>电视的出现难道不是科技进步解决人类需求？就好像电话使得写信不再需要。于是乎有人站出来说丢失了写信的诗意。然而事实上呢？对于写信的漫长回复不加批评。对于写信的门槛漠视。更何况，又有几个人能写的出来明月几时有，把酒问青天？大部分还不是落于家常俗套？根源根本不再电视好么！</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2019/02/28/pansee/book review/《浪潮之巅》/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/02/28/pansee/book review/《浪潮之巅》/" class="post-title-link" itemprop="url">《浪潮之巅》</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-02-28 20:45:06" itemprop="dateCreated datePublished" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/book-review/" itemprop="url" rel="index"><span itemprop="name">book review</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>《浪潮之巅》——作者：吴军</p>
<h1 id="综述"><a href="#综述" class="headerlink" title="综述"></a>综述</h1><p>正如我第一次知道作为业界成功人士的李开复竟然是语音识别的顶级科学家一样，当我看完《浪潮之巅》，不得不惊叹一个计算机领域的工程师竟然对企业管理与运作有如此深刻的洞察。更何况，书中还展露出吴军博士对经济、金融的理解、对历史（对欧洲封建社会变革的论述）、人文（对海明威作品的引用）领域的熟悉。最为难能可贵的是，这些知识与思考是如此自然的流于笔端，无比的真实与贴近生活。而不是生硬机械的硬搬概念、强行关联，刻意堆砌。</p>
<p>读书何为？难道是为了知道一些高大上的概念然后到人前炫耀么？以此来展现自己的博学多才而收获仰慕之眼光？写作何为？难道是为了词藻华丽，为了用一堆似是而非的引用和例子，配合上貌似气势高昂的排比修辞来揭示空中浮萍般的可笑“真理”？交流何为？难道是为了用空洞而无根底的言辞，空想而无逻辑的语言来交流思想？</p>
<p>那么，读《浪潮之巅》又有什么意义？</p>
<p>事实上，对于类似形式的书目，即书写一个恢宏大世或者一个领域思想，每一个章节通过聊聊几十页就妄图说清楚其中一个思想流派这样的概述书我是相当抵制的。这就好像以亚当斯密为代表的古典经济学到了最后只剩下看不见的手五个字，看起来精简概括，实则丢掉了太多细节，甚至于引入了谬误。如此书目往往只能从中学到概念，而对其真正的思维过程一无所知。当然，借这样的书来先行建立一个整体框架和概念有利于后期深入研究倒是尚有可取之处，可惜大部分人的所谓“研究”也就止步于这样的概述了。</p>
<p>但《浪潮之巅》不同，或许书本身就并不是想要向读者说清每家科技公司的盛衰兴亡，而是用各家公司的发展与衰落来说明一场场浪潮的发展趋势，并从中抽离出共性，“世界上失败者的失败各有各的不同，而成功者的成功却具有惊人的相似之处”。其实，失败者的病痛也往往相似。</p>
<p>然而，我依旧不建议不在科技行业之内或者对计算机科技不敢兴趣的读者给予此书过高的阅读优先级。跨领域的阅读往往痛苦，而一本书如果不能感同身受或者怀有极大的兴趣，读完之后不过束之高阁，了不起作为茶余饭后无意义的谈资。至于所谓广度的延展与知识的积累，恕我偏狭之眼光，很多人收获的只有肤浅。广度当然很重要，可以开阔视野和思路，但其只应该作为一种补充。如果一个人连在一个特定领域的深度都没有，不过一事无成，又有什么资格谈思想之丰满。科幻小说本该带来对于宇宙的敬畏，有人反而从中读出了量子力学，岂不可笑。</p>
<p>回归本书，我并不做管理，很难在具体的企业运作细节上有太大的感触。但也简单总结下。</p>
<p>成功或者失败大体出于以下几点：</p>
<ul>
<li>是否顺应潮流，把握机遇</li>
<li>是否能够平衡长短期利益、公司股东利益。克制贪婪，甚至于短期让利</li>
<li>是否有优秀的管理层和领导者</li>
<li>是否具有技术优势，以及找到合适的商业模式</li>
</ul>
<p>最后，想要屹立不倒还需要做到：</p>
<ul>
<li>有坚实的”群众基础“，或者说营收不依赖于泡沫。</li>
<li>扬弃。一方面不断创新，不断寻找新的增长点;另一方面对于旧的不再盈利或者江河日下的部门需要果断处理。避免机构冗余和旧体制妨碍新部门的发展。</li>
</ul>
<p>对于不是创业者的我而言，以上诸点或许除了知道之外很难有进一步的作用，也很难真正知道其中痛点。</p>
<p>那么，本书对我更为真实的意义在哪里呢？</p>
<ol>
<li><p>让我对近百年来对科技发展（主要是计算机领域）有了一个相对清晰的认识，从最初的有线电话到PC到互联网再到云计算的脉络有了一定了解。</p>
</li>
<li><p>书中展示了一个个科技公司的发展兴衰。</p>
</li>
</ol>
<p>如此一来，至少在找工作时不至于被各种打着大数据、人工智能名号的创业公司忽悠，能够自身对于公司业务的靠谱性有一个评判。再者，可以一定程度上培养科技、商业、业务上的嗅觉，还可以和面试官扯淡，以此来加点分。</p>
<p>我无意于将贪婪、平衡、稳健这些公司或成或败的因素与个人生活相结合，以便于长篇大论，各种感触深刻。这些东西其实早已知道，然而翻来覆去也不曾有所大的改变。</p>
<p>正如尼采的强力意志或者海德格尔的存在主义，如果仅仅是归纳成那么两三句话，就会发现其实有太多的人早已有相似想法。可惜的是，尼采和海德格尔终究都只有一个。真正高贵而丰满的东西显然不会只是犀利深刻的那么几句话。回归细节和真实，才是真正的困难也是真正的珠宝。正如苹果的每一款产品都并非完全的原创，但它做到了极致，并从中产生了独属于它的亮点和创新。虽然我完全不看好苹果软硬件一体的封闭式发展。</p>
<p>本书中揭示的另一个现象也让我惊诧。复杂指令集打败了更先进的精简指令集，DOS通过持久战坚持到了windows的出现。这样两个以弱胜强的案例乍看起来无法理解，其实是充分利用用户依赖或者说”群众基础“所营造的的商业优势来打败技术优势。但更基本的前提应当是：”够用“。厂家是否应该花精力去制造更先进的产品，消费者是否应该去追求过剩的性能？当一个对于产品的公正统一而客观的标准丧失，市场又可以相对轻易的进入时，只好通过各种性能指标或其他来进行质量宣示，营造壁垒，很多时候门槛的存在不再是追求良币，而是为了排除劣币。这不仅体现在商品市场，还体现在人才市场的学历门槛以及其他。</p>
<p>思科支持雇员创业并回购的管理与创新模式实在精彩。华为曾经也尝试发动高管和干部自立门户，进行创新，然而最后却是勾心斗角，反目成仇。何以如此？</p>
<p>雅虎虽然没有什么技术上的革新与贡献，但是杨致远和菲洛所开创的互联网免费模式却足以让他们英明不朽。</p>
<p>斯坦福的校园文化亦让我心向往之。确切的说，我所向往的是洪堡式的研究型教育和纽曼式的大行之道（学生间的交流学习）的结合。然而，如何真正有益的交流而不沦为聚众扯淡实在很难，当然最容易的做就是找靠谱的人。</p>
<p>那么，这本书是否对得起我所花去的阅读时间呢？以及什么样的书才是值得读的好书呢？</p>
<p>其实，很多情况下，与其谈怎么利用时间，不如想办法不浪费时间更为实在。所以，与其谈什么书值得花时间去读，不如少读点毫无价值的书，少做些毫无价值的事。也就是争取机会成本最小化而已。</p>
<h1 id="第一章-帝国的余晖——AT-amp-T公司"><a href="#第一章-帝国的余晖——AT-amp-T公司" class="headerlink" title="第一章 帝国的余晖——AT&amp;T公司"></a>第一章 帝国的余晖——AT&amp;T公司</h1><p>由发明电话的贝尔所创立，传统电话通信行业的巨头。贝尔实验室的拥有者。</p>
<p>没有被美国反垄断法所打败，却死于贪婪。为了获得资本运作所产生的短期既得利益，多次进行拆分，导致拆分后的各个公司无法共享资金和技术优势，最终死亡。</p>
<p>反垄断法诉讼也导致了AT&amp;T的拆分，但是那更多的是修剪枝干，使得公司管理组织结构更合理而不臃肿。但是其自行拆分导致资金和技术的分离使其无法保有并发展其创新优势却是致命的。最终被互联网浪潮所吞没。</p>
<h1 id="第二章-蓝色巨人——IBM公司"><a href="#第二章-蓝色巨人——IBM公司" class="headerlink" title="第二章 蓝色巨人——IBM公司"></a>第二章 蓝色巨人——IBM公司</h1><p>紧紧抓住了大型政企客户。</p>
<p>小沃森抓住了时代的潮流，领导了电子技术革命的浪潮。将计算机从政府部门和军方推广到民间。将它的功能由科学计算变成商用。</p>
<p>没有能够重视PC市场。当微机性能不断提升开始危险大型计算机时，IBM在80年代末出现了第一次严重亏损，有史以来第一次开始大规模裁员。原因主要有三个：</p>
<ol>
<li>由于开始阶段微机利润远远无法和IBM传统的大型机市场相比（第一年pc营业额只占IBM总营业额1%）,因此没有上升到战略高度</li>
<li>反垄断的限制，使得其他小公司可以比较容易的入场竞争</li>
<li>盖茨的长远眼光。不管是以版权费的方式卖DOS（从每台PC中收取版权费而非一次性卖断）还是独立开发windows（而非和IBM合作研发）都是明智之举，尤其是前者</li>
</ol>
<p>危机时刻郭士纳的救场：</p>
<ol>
<li>裁掉冗余部门和毫无前途的项目以节流</li>
<li>变卖一些资产获取资金以开源</li>
<li>买回曾经分离出去的服务公司，将硬件制造，软件开发，服务合成一体</li>
<li>公司内部引入竞争机制，将员工退休金和整个公司（以前是和部门）效益挂钩，从而加强部门间合作</li>
<li>削减研发经费，砍掉一些偏理论而无效益的研究，将研究和开发结合起来。研究人员的部分工资直接和产品项目挂钩</li>
<li>为了弥补由此带来的长线研究和基础研究方面的损失，加强了和大学的合作</li>
</ol>
<p>IBM将用户群定位为企业级客户，最终放弃了终端消费市场。IBM实验室不断创新，但是在产品与市场上相当保守，创新更多的是一种自我保护而非开拓。</p>
<p>为什么IBM能够在2000年后的两次金融危机中屹立不倒？</p>
<ol>
<li>核心业务是IT服务业，和金融关系不大。即使其企业终止产品采购，对已有产品的服务需求依旧存在</li>
<li>由于其内部不断的优胜劣汰，不断淘汰低利润服务，使得其毛利润一直很高。虽然存在机构臃肿，但是危机时裁员并不会影响公司运转。更进一步的，工作岗位开始永久性向印度转移，降低人力成本</li>
<li>全球化背景下跨国公司抗打击能力提升</li>
</ol>
<p>保守和谨慎是其鲜明特质。</p>
<h1 id="第三章-“水果”公司的复兴——乔布斯和苹果公司"><a href="#第三章-“水果”公司的复兴——乔布斯和苹果公司" class="headerlink" title="第三章 “水果”公司的复兴——乔布斯和苹果公司"></a>第三章 “水果”公司的复兴——乔布斯和苹果公司</h1><blockquote>
<p>公平的讲，现在苹果的每一款产品都并非它的原创……但是，苹果把每一款产都做到了极致，这很大程度上是乔布斯达到了一个将技术和艺术结合的炉火纯青的境界……如果要问什么是创新，这就是创新！</p>
</blockquote>
<blockquote>
<p>乔布斯送给年轻人两句话：永远渴望、大智若愚（Stay Huntry，Stay Foolish）</p>
</blockquote>
<p>无中生有的创新太难，能把已有的东西做好就已经领先于人。</p>
<h1 id="第四章-计算机工业的生态链"><a href="#第四章-计算机工业的生态链" class="headerlink" title="第四章 计算机工业的生态链"></a>第四章 计算机工业的生态链</h1><ul>
<li><p>摩尔定律：每18个月硬件性能翻一番或者说旧产品价格降低一半，远超传统行业的更新速度</p>
</li>
<li><p>安迪-比尔定律：软件更新把硬件提升带来的好处几乎全部占光了</p>
</li>
<li><p>反摩尔定律：每18个月，同样产品营业额要降一半。因此逼着硬件设备公司必须赶上摩尔定律规定的的更新速度。由于过不了多少年，量变潜力就会被挖掘光，因此为了跟上摩尔定律的更新速度还需要质变。反摩尔定律使得新兴小公司有希望和大公司站在同一起跑线上。创新更容易被迅速接受，赢得市场。</p>
</li>
</ul>
<p>IT行业，犹如逆水行舟，不进则退。处于上游的是软件和IT服务业，下游才是硬件和半导体。而非相反！</p>
<h1 id="第五章-奔腾的芯——英特尔公司"><a href="#第五章-奔腾的芯——英特尔公司" class="headerlink" title="第五章 奔腾的芯——英特尔公司"></a>第五章 奔腾的芯——英特尔公司</h1><p>专心只作一件产品，这样的孤注一掷让它在微机处理器上不断进步</p>
<p>市场依赖和市场选择让intel没有被RISC的高性能所迷惑，也最终让处于性能劣势的复杂指令集打败了精简指令集</p>
<p>没有新的增长点将会让它在新的浪潮中老去</p>
<h1 id="第六章-IT领域的罗马帝国——微软公司"><a href="#第六章-IT领域的罗马帝国——微软公司" class="headerlink" title="第六章 IT领域的罗马帝国——微软公司"></a>第六章 IT领域的罗马帝国——微软公司</h1><p>DOS对苹果和IBM OS/2的胜利，为windows的诞生赢得了时间，又是利用市场依赖以弱胜强。单单技术的先进并不是决定性的。<strong>利用人民的力量和市场的依赖却导致先进的技术难以快速普及，这实在是一个有趣的现象，值得深思。</strong></p>
<p>苹果在兼容性上的失误，选择了封闭式发展，导致更新换代软硬件的成本巨大。</p>
<p>同时，苹果在一定程度上违反了信息领域的摩尔定律和安迪比尔定律。软硬件全部抓在手上，导致很难平衡硬件和软件的研发速度。选择性舍弃反而会铸就更大的成功。</p>
<p>微软的成功：</p>
<ol>
<li>保守和冒险的平衡;</li>
<li>心比天高却又脚踏实地。</li>
</ol>
<p>在互联网和浏览器窗口上的战略失误。</p>
<p>在家庭娱乐中心战场的失利。</p>
<h1 id="第七章-互联网的金门大桥——思科公司"><a href="#第七章-互联网的金门大桥——思科公司" class="headerlink" title="第七章 互联网的金门大桥——思科公司"></a>第七章 互联网的金门大桥——思科公司</h1><p>合适时间点出现的多协议路由器从一开始就获得了市场优势。</p>
<p>开明的内部管理模式：提供投资让员工创业在回购，垄断了技术市场。</p>
<p>诺威格定律：当一家公司的市占率超过50%，就不要指望在市场占有率上翻番了。寻找新的增长点——家庭娱乐。</p>
<h1 id="第八章-英明不朽——杨致远、菲洛和雅虎公司"><a href="#第八章-英明不朽——杨致远、菲洛和雅虎公司" class="headerlink" title="第八章 英明不朽——杨致远、菲洛和雅虎公司"></a>第八章 英明不朽——杨致远、菲洛和雅虎公司</h1><p>制定了互联网这个行业全世界至今遵守的游戏规则——开放、免费、盈利。使“免费的午餐”成为了可能。</p>
<p>选错了战略方向，在不擅长的搜索引擎上徒费心力。在优势的品牌传媒方向又止步不前。</p>
<p>没有强力的技术支撑，缺少技术基因，整体的工作环境和企业文化也不对技术人员胃口。</p>
<p>失败的高层管理，差劲的掌舵人。</p>
<h1 id="第九章-硅谷的见证人——惠普公司"><a href="#第九章-硅谷的见证人——惠普公司" class="headerlink" title="第九章 硅谷的见证人——惠普公司"></a>第九章 硅谷的见证人——惠普公司</h1><p>惠普衰落的原因：</p>
<ul>
<li>领导者的错误</li>
<li>“日本/中国制造“的冲击</li>
</ul>
<h1 id="第十章-没落的贵族——摩托罗拉公司"><a href="#第十章-没落的贵族——摩托罗拉公司" class="headerlink" title="第十章 没落的贵族——摩托罗拉公司"></a>第十章 没落的贵族——摩托罗拉公司</h1><p>衰落的原因：</p>
<ol>
<li>低估了摩尔定律的作用和产品更新的速度，对数字手机关注不足，依旧将主要精力放在落后的模拟手机上</li>
<li>领导层不能开拓，或许也无力守成</li>
</ol>
<p>铱星计划： 对市场的错误评估</p>
<blockquote>
<p>摩托罗拉至今都看不起三星和诺基亚不注重核心技术、只在外型和功能上搞花架子的做法。</p>
</blockquote>
<p>长于技术，弱于市场。不够重视市场和客户需求。</p>
<p>与AT&amp;T的短视和贪婪相反，摩托罗拉作为一个家族企业，不会出现把AT&amp;T拆了卖的败家行为，但家族第三代领导人也心有余而力不足。君子之泽，五世而折。</p>
<h1 id="第十一章-硅谷的另一面"><a href="#第十一章-硅谷的另一面" class="headerlink" title="第十一章 硅谷的另一面"></a>第十一章 硅谷的另一面</h1><p>幸存者偏差</p>
<p>勤奋与创新</p>
<h1 id="第十二章-短暂的春秋——与机会失之交臂的公司"><a href="#第十二章-短暂的春秋——与机会失之交臂的公司" class="headerlink" title="第十二章 短暂的春秋——与机会失之交臂的公司"></a>第十二章 短暂的春秋——与机会失之交臂的公司</h1><h1 id="第十三章-幕后的英雄——风险投资"><a href="#第十三章-幕后的英雄——风险投资" class="headerlink" title="第十三章 幕后的英雄——风险投资"></a>第十三章 幕后的英雄——风险投资</h1><h1 id="第十四章-信息产业的规律性"><a href="#第十四章-信息产业的规律性" class="headerlink" title="第十四章 信息产业的规律性"></a>第十四章 信息产业的规律性</h1><ul>
<li><p>70-20-10律：科技产品的指标往往是硬性的，没有办法制造差异度。以至于强者恒强。然而我不大认可这种说法。。更多的应该是先发至人和（或）配套软硬件带来的强大用户粘性导致的市场独占。</p>
</li>
<li><p>诺威格定律：一家公司市场占有率超过50%后，就无法再使市场占有率翻番了。而开拓新的财源有效的途径只有两条：扩展现有业务和转型。</p>
</li>
<li><p>基因决定定律：基因决定了公司的市场运作，发展模式等，并难以改变。我觉得基因有两种：一种是从创始之时开始确立的公司的发展方向和管理方式，市场运作方式等，还有一场则是公司成型之后各个部门之间的利益纠葛。</p>
</li>
</ul>
<h1 id="第十五章-硅谷的摇篮——斯坦福大学"><a href="#第十五章-硅谷的摇篮——斯坦福大学" class="headerlink" title="第十五章 硅谷的摇篮——斯坦福大学"></a>第十五章 硅谷的摇篮——斯坦福大学</h1><ul>
<li><p>纽曼教育</p>
</li>
<li><p>洪堡教育</p>
</li>
<li><p>斯坦福和工业界的紧密联系，创业成风的校园氛围，以及能够给学生和教授提供商业资源的能力。</p>
</li>
</ul>
<h1 id="第十六章-科技公司的吹鼓手——投资银行"><a href="#第十六章-科技公司的吹鼓手——投资银行" class="headerlink" title="第十六章 科技公司的吹鼓手——投资银行"></a>第十六章 科技公司的吹鼓手——投资银行</h1><p>华尔街对经济的影响确实可怖，但也确实有所益处。</p>
<h1 id="第十七章-挑战者——Google公司"><a href="#第十七章-挑战者——Google公司" class="headerlink" title="第十七章 挑战者——Google公司"></a>第十七章 挑战者——Google公司</h1><ul>
<li><p>个人英雄主义的文化，宁缺勿滥、一个顶十个的择人标准</p>
</li>
<li><p>公平、平等的领导、员工关系，开放、自由的工作环境</p>
</li>
<li><p>不作恶的企业原则，善待竞争者（不采用收购等合法而略显无耻的手段，在雅虎放弃低价购买google股票后悔不迭之后依旧以低价给了雅虎大量股票等），赢得了业界的尊重和支持，组建了强大的同盟军</p>
</li>
<li><p>“快速向前跑，不要看两边”的研发策略，以领先的技术击溃对手</p>
</li>
<li><p>商业运作、市场脉搏等方面的成功等</p>
</li>
</ul>
<h1 id="第十八章-成功的转基因——诺基亚、3M、GE公司"><a href="#第十八章-成功的转基因——诺基亚、3M、GE公司" class="headerlink" title="第十八章 成功的转基因——诺基亚、3M、GE公司"></a>第十八章 成功的转基因——诺基亚、3M、GE公司</h1><p>3M不断保持活力的原因：</p>
<ul>
<li><p>扬新。创新往往伴随着失败。只有能够容忍失败、容忍表面的不可能才能真正的实现创新。</p>
</li>
<li><p>弃旧。不仅弃掉已经旧的，还要弃掉虽然有盈利但是前景不佳还会阻碍创新的旧部门、旧势力</p>
</li>
<li><p>群众基础。日用品之类是最不容易受到冲击的。</p>
</li>
</ul>
<h1 id="第十九章-印钞机——最佳的商业模式"><a href="#第十九章-印钞机——最佳的商业模式" class="headerlink" title="第十九章 印钞机——最佳的商业模式"></a>第十九章 印钞机——最佳的商业模式</h1><p>下面三种商业模式都算不上好的：</p>
<ul>
<li><p>每增加一份营收就必须多雇佣一个人</p>
</li>
<li><p>无法横向拓展的业务，例如从一个地区扩展到另外一个地区需要做需要额外的工作</p>
</li>
<li><p>需要消耗过多的原料和成本</p>
</li>
</ul>
<h1 id="第二十章-互联网2-0"><a href="#第二十章-互联网2-0" class="headerlink" title="第二十章 互联网2.0"></a>第二十章 互联网2.0</h1><p>互联网2.0公司应该具备的特征（至少其中之一）：</p>
<ul>
<li><p>必须有一个平台，可以接受并且管理用户提交的内容，并且这些内容是服务主体</p>
</li>
<li><p>提供一个开放平台，让用户可以在平台上开发自己的应用程序，并且提供给其他用户使用</p>
</li>
<li><p>交互性</p>
</li>
<li><p>非竞争性和自足性</p>
</li>
</ul>
<h1 id="第二十一章-金融风暴的冲击"><a href="#第二十一章-金融风暴的冲击" class="headerlink" title="第二十一章 金融风暴的冲击"></a>第二十一章 金融风暴的冲击</h1><ul>
<li><p>生产和供给能力的大大提高，资本从实体经济向虚拟虚拟经济转移</p>
</li>
<li><p>超前消费的流行</p>
</li>
</ul>
<p>上述两种风气使社会变得浮躁和短视，很多良好的价值观被破坏。在经济危机之下，这些泡沫将会被打破，人们会捡起稳健、勤勉的传统。</p>
<p>对于欧洲封建社会变革的分析非常真实而有趣。商人用金钱作为借贷一步步蚕食领主的权力。</p>
<h1 id="第二十二章-云计算"><a href="#第二十二章-云计算" class="headerlink" title="第二十二章 云计算"></a>第二十二章 云计算</h1><h1 id="第二十三章-下一个Google"><a href="#第二十三章-下一个Google" class="headerlink" title="第二十三章 下一个Google"></a>第二十三章 下一个Google</h1><p>分析了几个新兴行业发展方向以及国内的潜力公司和发展状况。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2019/02/28/pansee/book review/宽容——房龙/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2019/02/28/pansee/book review/宽容——房龙/" class="post-title-link" itemprop="url">宽容——房龙</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2019-02-28 20:45:06" itemprop="dateCreated datePublished" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
            

            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/book-review/" itemprop="url" rel="index"><span itemprop="name">book review</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h1><p>本书的宽容主题主要围绕着的是宗教变革过程中种种的斗争、迫害、不宽容。并在最后指出，人类历经千年斗争终于在不同信仰不同教义方面取得了初步的宽容，但紧接着种族不宽容、社会不宽容以及许多不足挂齿的不宽容又开始正式登上舞台，带给人们更大的痛苦。</p>
<p>如果说宗教不宽容更多的是团体之间的对抗，个人精神的解放带来的可能是更加极端更加琐碎而难以想象的各种不宽容。团体之间的对抗可以更加容易的通过法令、通过政治去控制，个人的不宽容却更加难以防范与规范，更加危险而无处不在，更频繁而尖锐的让每个人处在痛苦之中。</p>
<p>我想，个人的不宽容应当是自古而今始终存在的，也是各类不宽容的根源所在。只是原先大众尚未开化，个体思想不显，容易被个别领袖所操纵，又受限于生产力产生的群体依赖关系，因而更显式的表现为团体对抗。</p>
<p>我并没有在此书中看到人应该如何宽容。仅仅是对自己的思想报以警惕，看到自身的局限性与相对性并不能指导人在遭遇矛盾时的做法。绝对容易衍生强权与独裁，相对容易导致软弱与犹豫。警惕与防范不宽容要么将宽容的敌人引进大门，要么陷入另一种不宽容。我看不到恰到好处的可能性。相对是没有地基的，要么被敌人毁灭，要么从自身内部腐烂瓦解。</p>
<p>大和谐的前提一定是人拥抱同一种基本原则。在这一种原则上没有任何的宽容可以放任。尤其追求虚无缥缈的宽容与自由，不如实实在在的讨论什么是必须认同的，以及尽量放宽这一认同原则的范围，减少不必要的束缚。并以此为指导消灭异端。很遗憾的，这一基本原则的找寻不可能是一蹴而就的，势必需要不停的迭代，也就在此过程中必不可少的会发生迫害与变革。在这个过程中我们能做的也就是不要去犯相同的错误，不要采用极端的手段。不要如加尔文一般在掌权之后将相同的迫害加之于后人。</p>
<p>至于说恐惧是不宽容的根源。或许在人类伊始是这样的，但到了今天或许不宽容对部分人而言成为了一种本能。就如电影《老无所依》的恶。</p>
<h1 id="序言"><a href="#序言" class="headerlink" title="序言"></a>序言</h1><h1 id="一、无知的暴虐"><a href="#一、无知的暴虐" class="headerlink" title="一、无知的暴虐"></a>一、无知的暴虐</h1><blockquote>
<p>该书第二十六卷一○五二页这样写道“宽容(来源于拉丁字 tolerare):容许别人有行 动和判断的自由，对不同于自己或传统观点的见解的耐心公正的容忍。” </p>
</blockquote>
<blockquote>
<p>原始社会非常复杂，原始语言的时态和变格比俄语和阿拉伯语还要多，原始人不仅是现实的奴隶，也是过去和未来的奴隶;一句话，他们是凄凉悲惨的生灵，在恐惧中求生，在战栗中死去。</p>
</blockquote>
<blockquote>
<p>他们根本不懂因果法则。……<br>因此，在一个社会中，如果一切事情都被认为是由看不见的生灵操纵的，那么社会要维持下去，就必须绝对服从能平息上帝怒火的律法。<br>……<br>而低级社会形态的特点是，人们认为现状已经完 美无暇了，没有理由再做什么改进，因为他们从未见过别的世界。<br>如果上面所说的是真的，那么怎样才能防止律法和已定的社会形式有所变更呢?<br>就是靠及时惩处拒不把公共条例看做是上天旨意具体体现的那些人，说得露骨一点，就是靠僵化的专横制度。</p>
</blockquote>
<p>因为无知，所以信奉初始阶段总结出来的错误律法和“忌讳”。</p>
<blockquote>
<p>我们有时看到的宽容，其实是由于无知导致的漠不关心。</p>
</blockquote>
<blockquote>
<p>为宽容的斗争直到个性发现以后才开始。 </p>
</blockquote>
<h1 id="二、希腊人"><a href="#二、希腊人" class="headerlink" title="二、希腊人"></a>二、希腊人</h1><blockquote>
<p>“只有所有种族、气候、经济和政治条件在不健全的世界中达到或接近一种理想比例时， 高级形式的文明才会突然地、貌似自动地脱颖而出。” </p>
</blockquote>
<blockquote>
<p>他们提出，苏格拉底只要摈弃辩论、争吵、说教这些可怕陋习，不再干涉别人所偏爱的东西，不再用永无止境的疑问去纠缠他们，就可以被赦免。</p>
</blockquote>
<p>苏格拉底这样的行为是宽容么？</p>
<blockquote>
<p>但柏拉图却是古代众多理论家中唯一的一个出于对完美精神世界的炽爱而鼓吹不宽客的人。</p>
<p>……</p>
<p>这个世界观转变的原因并不难寻找。苏格拉底扎根于民众之中，而柏拉图却惧怕生活。 他为了逃避丑陋的世界，躲到了自己臆想的王国中。 </p>
</blockquote>
<p>惧怕因而独裁。</p>
<blockquote>
<p>罗马人在许多事情上甚至比黄金时代的希腊人还要宽容。他们容许臣民自由思考，但是不允许人们对政治上的某些随机应变的原则提出质问，因为罗马政权之所以从史前时期就能保持繁荣安定，全部仰仗这些原则。</p>
</blockquote>
<blockquote>
<p>希腊思想体系的老一代领袖人物把其宽容精神基于某些明确的结论上，这些结论是他们经过数世纪认真实践和苦思冥想总结出来的。而罗马人却认为，他们用不着从事这方面的探讨。他们对理论问题漠不关心，还把这种态度引为自豪，他们对实用的东西感兴趣，注重行动，看不起高谈阔论。</p>
<p>……</p>
<p>人们时常争辩说，罗马人之所以能够摆出一副至高无上的宽容姿态，是因为他们对哥罗西人、卡帕迪西亚人以及其他所有野蛮部落的人都持有同等的轻蔑态度。这可能是正确的。</p>
</blockquote>
<p>宽容起于蔑视。</p>
<blockquote>
<p>结果，所有的美味佳肴都失去了味道，所有的图书都变得乏味，所有的女人都失去了魅力，甚至生存本身也成为一种负担，很多人宁可获取一个体面的机会使自己丧生。<br>剩下的只有一种安慰!对未知和无形世界的遐想。</p>
<p>……</p>
<p>不过从长远的观点看，这种纯理性的教义缺乏罗马人所需要的营养，他们开始追求一种 可以作为精神食粮的“情感”。 </p>
<p>由此说来，纯哲学色彩的“宗教”(如果我们把宗教思想和追求有益高尚生活的愿望联系 起来，这确是一种哲学色彩的宗教)只能取悦于一小部分人……</p>
</blockquote>
<p>自我无法支撑起自我的充盈，只好需求外物。物品也无法支撑的时候，就需要求诸“神圣”。</p>
<h1 id="三、桎梏的开始"><a href="#三、桎梏的开始" class="headerlink" title="三、桎梏的开始"></a>三、桎梏的开始</h1><blockquote>
<p>大多数罗马人水深火热的生活与最早期传教士的成功 有着很大关系，就象窘苦生活导致神学的成功一样。 </p>
</blockquote>
<p>贫穷也导致自我的消亡。</p>
<blockquote>
<p>没有文字规定，没有明确的条例规则，反而使信仰者可以自由地遵循耶稣的精神而不是<br>教规文字了。如果他们被一本书束缚了，势必会把全部精力用在理论讨论上，沉缅于对句号<br>冒号的迷人的研究中。</p>
</blockquote>
<h1 id="四、上帝的晨光"><a href="#四、上帝的晨光" class="headerlink" title="四、上帝的晨光"></a>四、上帝的晨光</h1><h1 id="五、囚禁"><a href="#五、囚禁" class="headerlink" title="五、囚禁"></a>五、囚禁</h1><h1 id="六、生活的纯洁"><a href="#六、生活的纯洁" class="headerlink" title="六、生活的纯洁"></a>六、生活的纯洁</h1><p>用圆圈做比喻，说明一旦社会失衡，过于偏向某些领域，会导致崩溃。</p>
<h1 id="七、宗教裁判所"><a href="#七、宗教裁判所" class="headerlink" title="七、宗教裁判所"></a>七、宗教裁判所</h1><p>为了巩固地位和权威而裁判异教徒</p>
<h1 id="八、求知的人"><a href="#八、求知的人" class="headerlink" title="八、求知的人"></a>八、求知的人</h1><blockquote>
<p>现代的不宽容就像高卢人一样，可以分为三种：处于懒惰的不宽容，处于无知的不宽容和出于自私自利的不宽容。</p>
</blockquote>
<blockquote>
<p>无知的人仅仅因为他对事物的一无所知 便可以成为极度危险的人物。但是，他如果还为自己的智力不足措辞辩解，那就更为可怕。</p>
</blockquote>
<p>超出时代的智者如何冲出愚蠢的思想的包围？宽容和坚持并不矛盾。但当人与人之间的差距过大的时候，就会难以交流，那个时候还可能有真的宽容和谦卑么？或许更多的是怜悯？</p>
<h1 id="九、向书开战"><a href="#九、向书开战" class="headerlink" title="九、向书开战"></a>九、向书开战</h1><blockquote>
<p>因此我主张，由他们去说去写吧。如果说的是至理名言，我们就应该知道，如不然，也会很快被忘记。</p>
</blockquote>
<p>真的是这样么</p>
<h1 id="十、关于一般历史书籍，尤其是这本书"><a href="#十、关于一般历史书籍，尤其是这本书" class="headerlink" title="十、关于一般历史书籍，尤其是这本书"></a>十、关于一般历史书籍，尤其是这本书</h1><blockquote>
<p>一句话，个人的不宽容只能以自由国家的大多数公民不介意为极限，不得超越。然而官方的不宽容却不然，它可以权力浩大。</p>
</blockquote>
<h1 id="十一、文艺复兴"><a href="#十一、文艺复兴" class="headerlink" title="十一、文艺复兴"></a>十一、文艺复兴</h1><blockquote>
<p>在我看来，作家在许多地方与攻城炮兵有相同之处。他们也在操纵一门重型火炮，他们的文学炮弹也许会在最不可能的地方引起革命或动乱。</p>
</blockquote>
<blockquote>
<p>不过严格他讲，文艺复兴起先并不是“向前看”的运动。它鄙视刚刚消失的过去，称上 一代人的著作为“野蛮”之作(或“哥特式的野蛮”之作，因为哥特人曾一度和匈奴人一样 名声狼藉)。文艺复兴的主要志趣在艺术品上，因为艺术品里蕴藏着一种叫“古典精神”的物  。 </p>
<p>文艺复兴的确大大振兴了良知的自由、宽容和更为美好的世界，不过运动的领袖们并没想这样做。</p>
</blockquote>
<blockquote>
<p>马可·波罗从生到死当然一直是教会的虔诚弟子，谁要是把他比做几乎是同时代的著名的罗吉尔·培根，他还会怒不可遏。……</p>
<p>不过这两个人中还是波罗更为危险。</p>
<p>十万人中最多只有一个人会跟随培根追逐天上的虹，琢磨娓娓动 的进化理论以颠扑当 时的神圣观点，而只学过 ABC 的平民百姓却可以从马可·波罗那儿得知世界上还存在着《旧 约》作者从 想到过的东西。 </p>
<p>我并不是说在世界尚 获得一丝一毫的自由之前，仅靠出版一 书就能引起对《圣经》权威性的反叛。普遍的启蒙开化是数世纪艰苦准备的结果。不过，探险家、航海家和旅行家的朴实宣言却得到了大家的理解，这对怀疑论精神的兴起起了重大作用。怀疑论是文艺复兴后期的特点，它允许人们去说去写那些仅在几年前还会使人落入宗教法庭的魔爪的言论。</p>
</blockquote>
<blockquote>
<p>文艺复兴不是自觉钻研科学的时代，在精神领域中也很遗憾缺乏真正的志趣。这三百年里在一切事物中作主导的是美和享乐。教皇虽然暴跳如雷反对一些臣民的异端教旨，可是只要这些反叛者健谈、懂一点印刷和建筑学，他倒也十分乐于邀请他们共进晚餐。……</p>
<p>人们表露的是对生活的新的向往，但是里面却无疑蕴藏着一种潜在的不满，反对现存的社会和拥有无上权力的教会对人类理解发展的束缚。</p>
</blockquote>
<h1 id="十二、基督教改革运动"><a href="#十二、基督教改革运动" class="headerlink" title="十二、基督教改革运动"></a>十二、基督教改革运动</h1><blockquote>
<p>宗教改革是形形色色的人出于形形色色的动机造成的。直到最近我们才开始明白，宗教上的不满只是这场大动乱的次要原因，实际上它是一场不可避免的社会和经济革命，神学的背景微乎其微。</p>
</blockquote>
<blockquote>
<p>可是新教徒没有受过长达数世纪的如何进行迫害和镇压的训练，他们想建立一个没有反对者的禁地，却失败了。……</p>
<p>这就是新教为宽容事业带来的帮助。  </p>
<p>它重建了人的尊严。</p>
</blockquote>
<h1 id="十三、伊拉斯谟"><a href="#十三、伊拉斯谟" class="headerlink" title="十三、伊拉斯谟"></a>十三、伊拉斯谟</h1><blockquote>
<p>也许乔纳森·斯威夫特(按我的记忆)接近了这个问题，他说，大多数人都有足 够的宗教信仰做依据憎恨旁人，却不能爱别人。遗憾的是，这条真知灼见还不能完全解决我 们目前的困难。有些人对宗教的熟悉不逊于任何人，也最从心底里仇恨别人。有些人全无信 仰宗教的天性，却对野猫、野狗和基督世界的人类倾注了真挚感情。 </p>
</blockquote>
<blockquote>
<p>大凡为宽容而战的人，不论彼此有什么不同，都有一点是一致的，他们的信仰总是伴随着怀疑;他们可以诚实地相信自己正确，却又从不能使自己的怀疑转化为坚固绝对的信念。</p>
</blockquote>
<p>如果没有绝对的确信又如何行事？抱持一种尝试的态度当然可以，但前提在于它是一种对未来不可把控而进行的无奈尝试。而对于既定共识的违反并由此带来伤害的行为，难道还要怀疑么？譬如杀人，譬如欺诈，行此等事之人或许有其可怜之处，迫于无奈，冲动，或者是愚蠢，然而这种恶难道不应该是被确信的么？如果不能确信，岂不是彻底的陷入混乱。然而一旦被确信，便容易陷入独裁，很难界定什么是需要被确信的，什么是值得存有怀疑的。</p>
<p>更进一步的，所谓的基本原则与共识，有是如何出现的呢？不过是绝大多数人都畏惧的。譬如说都不希望被杀，被偷盗。换言之，一些行为很难被人所防范，很容易危险到绝大多数人的利益，因此行为了集体的共识。但是，如果出现一个超人，譬如死侍这样不死的存在，共识、道德对他还有用处么？绝大多数人又有什么资格来要求超人从他们的弱者角度进行思考和行动？</p>
<p>宽容之所以存在，不过是因为人能看到自己的不足，看到不宽容的事也可能发生在自己身上而已？</p>
<h1 id="十四、-拉伯雷"><a href="#十四、-拉伯雷" class="headerlink" title="十四、 拉伯雷"></a>十四、 拉伯雷</h1><blockquote>
<p>人们说:“一个大组织只要有一个人说了算，而其他所有人都跪下喊阿门，服从他，那么 管理起来还不是易若反掌。” </p>
<p>在新教徒国家长大的人要对这个错误复杂的问题有一个正确全面的了解，那真是难上加 难。不过，如果我没有搞错，教皇“一 正确”的言论就象美国的宪法修定案一样历历可数。 </p>
<p>况且，重要决策总要经过充分讨论，而最后做出决定之前的争论常常会动摇教会的稳定。 这样产生的宣言是“一 正确”的，正如同我们的宪法修定案也一 正确一样，因为它们是 “最后”的，一经明确地并入最高法律，任何争持都到此结束。 </p>
<p>谁要是说管理美国很容易，因为人们在紧急时刻都会站在宪法的一边，那就大错特错了， 就象是说天主教徒既然在重大的信仰问题上承认教皇的绝对权威，那么，他们一定是一群驯 良的羔羊，把拥有自己独特想法的权力都放弃了。 </p>
<p>假如真是这样，那么住在拉特兰和梵蒂冈宫殿里的人倒是有好日子过了。但是，只要肤浅地研究一下一千五百年来的历史，就会发现事情恰恰相反。那些主张信仰改革的人在著书立说时，似乎以为罗马当权者全然不知道路德、加尔文和茨温利满怀仇恨谴 的那些罪恶，其实他们才是真正不知事情的真相，或者说不能处埋好他们对美好事业的热情。  </p>
<p>象艾德里安六世和克莱芒七世这样的人完全了解教会有重大弊病。不过，指出丹麦王国里有些腐 现象是一回事，而改正弊病则是另一回事，就连可怜的哈姆雷特最后也不得不承认这一点。</p>
</blockquote>
<p>统治阶层并不是真的傻到不知道正在发生的事，往往他们还是最先知道的。</p>
<h1 id="十五、旧时代的新招牌"><a href="#十五、旧时代的新招牌" class="headerlink" title="十五、旧时代的新招牌"></a>十五、旧时代的新招牌</h1><blockquote>
<p>宽容就如同自由。  </p>
<p>只是乞求是得不到的。只有永远保持警惕才能保住它。</p>
</blockquote>
<p>加尔文和路德的光辉形象全部崩塌了。。</p>
<h1 id="十六、-再洗礼教徒"><a href="#十六、-再洗礼教徒" class="headerlink" title="十六、 再洗礼教徒"></a>十六、 再洗礼教徒</h1><p>是再洗礼孕育了约翰还是倒霉的被约翰坑了？</p>
<h1 id="十七、索兹尼一家"><a href="#十七、索兹尼一家" class="headerlink" title="十七、索兹尼一家"></a>十七、索兹尼一家</h1><blockquote>
<p>我们为什么不记住，我们唯一的主是耶稣基督，大家都是兄弟，有谁被赋予了压服别人的力量呢？可能其中一个兄弟比别人博学一点，但是在自由和基督的关系上，我们是平等的。</p>
</blockquote>
<h1 id="十八、蒙田"><a href="#十八、蒙田" class="headerlink" title="十八、蒙田"></a>十八、蒙田</h1><blockquote>
<p>不久之后，欧洲大陆的条件大为好转，国际商业又成为可能，于是产生了另一种历史现象。</p>
<p>以三个双字词组表示便是：生意益于宽容。</p>
</blockquote>
<p> 是否可以说物质利益高于信仰坚持呢？</p>
<h1 id="十九、阿米尼斯"><a href="#十九、阿米尼斯" class="headerlink" title="十九、阿米尼斯"></a>十九、阿米尼斯</h1><h1 id="二十、布鲁诺"><a href="#二十、布鲁诺" class="headerlink" title="二十、布鲁诺"></a>二十、布鲁诺</h1><h1 id="二十一、斯宾诺沙"><a href="#二十一、斯宾诺沙" class="headerlink" title="二十一、斯宾诺沙"></a>二十一、斯宾诺沙</h1><h1 id="二十二、新的天国"><a href="#二十二、新的天国" class="headerlink" title="二十二、新的天国"></a>二十二、新的天国</h1><h1 id="二十三、太阳国王"><a href="#二十三、太阳国王" class="headerlink" title="二十三、太阳国王"></a>二十三、太阳国王</h1><h1 id="二十四、弗雷迪里克大帝"><a href="#二十四、弗雷迪里克大帝" class="headerlink" title="二十四、弗雷迪里克大帝"></a>二十四、弗雷迪里克大帝</h1><h1 id="二十五、伏尔泰"><a href="#二十五、伏尔泰" class="headerlink" title="二十五、伏尔泰"></a>二十五、伏尔泰</h1><h1 id="二十六、百科全书"><a href="#二十六、百科全书" class="headerlink" title="二十六、百科全书"></a>二十六、百科全书</h1><blockquote>
<p>他们常常后悔没有同时代的大部分人对各种事物的敬畏感，认为这不过是过去遗留下来的、虽然没什么害处却很幼稚的东西。</p>
<p>他们很少注意古代民族的历史，西方的人们出于某些好奇的原因，从巴比伦亚人、埃及人、赫梯人和迦勒底人的历史中挑出一些记载，作为道德和习俗的行动指南。但是大师苏格拉底的真正信徒们只倾 自己良心的呼唤，根 不管后果，他们无所畏惧地生活在早已变得屈服温顺的世界。</p>
</blockquote>
<h1 id="二十七、革命的不宽容"><a href="#二十七、革命的不宽容" class="headerlink" title="二十七、革命的不宽容"></a>二十七、革命的不宽容</h1><h1 id="二十八、莱辛"><a href="#二十八、莱辛" class="headerlink" title="二十八、莱辛"></a>二十八、莱辛</h1><blockquote>
<p>莱辛用这个古老的民间故事来证明他的信念:没有一种宗教可以垄断真理。人的内心世 界比他表面上遵奉某种规定的仪式和教条更有价值，因此人们的任务就是友好地相处，任何 人也无权把自己视为完美无缺的偶像让别人崇拜，无权宣布“我比其他任何人都好，因为只 有我掌握真理。” </p>
</blockquote>
<blockquote>
<p>至少可以说，抗泰姆的推理方法是有独创性的。 </p>
<p>他说:“上帝是万能的，他可以制定出对所有人民在任何时间任何情况下都适用的科学定 律。所以，只要他想做，就可以很容易地引导人们的思想，使人们在宗教问题上持相同的观 点。我们知道上帝并没有这么干。因此，如果我们用武力迫使别人相信自己是正确的，我们 就违背了上帝的明确旨意。” </p>
</blockquote>
<h1 id="二十九、汤姆·佩恩"><a href="#二十九、汤姆·佩恩" class="headerlink" title="二十九、汤姆·佩恩"></a>二十九、汤姆·佩恩</h1><blockquote>
<p>佩恩认为，真正的宗教，他称之为“人性的宗教”，有两个敌人，一个是无神论，另一个 是盲信主义。 </p>
</blockquote>
<blockquote>
<p>公众的不宽容刚一发泄完自己的愤怒，个人的不宽容又开始了。</p>
<p>官方死刑已告终止，而私刑处死又问世了。</p>
</blockquote>
<h1 id="三十、最后一百年"><a href="#三十、最后一百年" class="headerlink" title="三十、最后一百年"></a>三十、最后一百年</h1><blockquote>
<p>二十年前写这 书一定很容易。那时在大多数人的头脑中，“不宽容”这个词几乎完全和 “宗教不宽容”的意思一样 ……</p>
<p>社会刚开始摆脱宗教偏执的恐怖，又得忍受更为痛苦的种族不宽容、社会不宽容以及许多不足挂齿的不宽容，对于它们的存在，十年前的人们连想都没想过。</p>
</blockquote>
<p>宗教偏执的摆脱让其他的不宽容进入学者、媒体、社会活动家的视野而已。或许对普通人来说，这样的不宽容无时无刻不再发生，从来不曾间断？</p>
<blockquote>
<p> 我这里说的“教育”不是指纯粹的事实积累，这被看作是现代孩子们的必需有的精神库存。我想说的是，对现时的真正理解孕育于对过去的善意大度的了解之中。</p>
<p>在这 书中我已经力图证明，不宽容不过是老百姓自卫 能的一种表现。</p>
<p>……</p>
<p>我重复一遍，恐怖是所有不宽容的起因。 </p>
<p>……</p>
<p>只要不宽容是我们的自我保护法则中必不可少的一部分，要求宽容简直是犯罪。</p>
</blockquote>
<h1 id="后记-但是这个世界并不幸福"><a href="#后记-但是这个世界并不幸福" class="headerlink" title="后记 但是这个世界并不幸福"></a>后记 但是这个世界并不幸福</h1>
          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/17/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/17/">17</a><span class="page-number current">18</span>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">muzhen</p>
              <div class="site-description motion-element" itemprop="description"></div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">358</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">38</span>
                    <span class="site-state-item-name">categories</span>
                  
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">113</span>
                    <span class="site-state-item-name">tags</span>
                  
                </div>
              
            </nav>
          

          

          

          

          
          

          
            
          
          

        </div>
      </div>

      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">muzhen</span>

  

  
</div>


  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.0.1</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=7.0.1"></script>

  <script src="/js/src/motion.js?v=7.0.1"></script>



  
  


  <script src="/js/src/schemes/muse.js?v=7.0.1"></script>



  

  


  <script src="/js/src/next-boot.js?v=7.0.1"></script>


  
  



  




  

  

  
  

  
  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  


  

  

  

  

  

  

  

  

  

  

</body>
</html>
