<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang="en">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">


























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=7.0.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.0.1">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.0.1">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.0.1">


  <link rel="mask-icon" href="/images/logo.svg?v=7.0.1" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '7.0.1',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false,"dimmer":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta property="og:type" content="website">
<meta property="og:title" content="the Home of MuZhen">
<meta property="og:url" content="http://www.muzhen.tk/page/14/index.html">
<meta property="og:site_name" content="the Home of MuZhen">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="the Home of MuZhen">






  <link rel="canonical" href="http://www.muzhen.tk/page/14/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>the Home of MuZhen</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">the Home of MuZhen</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2017/05/05/machine learning/machine learning/《ESL第3章 回归的线性方法》/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/05/05/machine learning/machine learning/《ESL第3章 回归的线性方法》/" class="post-title-link" itemprop="url">《ESL第3章 回归的线性方法》</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-05-05 09:56:25" itemprop="dateCreated datePublished" datetime="2017-05-05T09:56:25+08:00">2017-05-05</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p>
<h1 id="第3章-回归的线性方法"><a href="#第3章-回归的线性方法" class="headerlink" title="第3章 回归的线性方法"></a>第3章 回归的线性方法</h1><h2 id="3-2-线性回归模型和最小二乘方"><a href="#3-2-线性回归模型和最小二乘方" class="headerlink" title="3.2 线性回归模型和最小二乘方"></a>3.2 线性回归模型和最小二乘方</h2><p>线性回归模型是指<strong>参数线性</strong>。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2017/05/04/machine learning/machine learning/《ESL第2章 有指导学习概述》/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/05/04/machine learning/machine learning/《ESL第2章 有指导学习概述》/" class="post-title-link" itemprop="url">《ESL第2章 有指导学习概述》</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-05-04 18:22:42" itemprop="dateCreated datePublished" datetime="2017-05-04T18:22:42+08:00">2017-05-04</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p>
<h1 id="第2章-有指导学习概述"><a href="#第2章-有指导学习概述" class="headerlink" title="第2章 有指导学习概述"></a>第2章 有指导学习概述</h1><p>线性判别边界：在不同类别服从区分明显的不同分布时合适<br>非线性判别边界：在不同类别所服从分布相对紧密时适用</p>
<p>最小二乘方：假定可以用一个全局线性函数很好地近似<br>k-最近邻：假定可以用一个局部常量函数很好地近似</p>
<p>knn有效参数个数：表面上是1个，即邻居个数k，其实是N/k个（其中N为样本量，k为邻居个数，如果邻域不重叠，需要为每个邻域配一个参数（均值））。  </p>
<p>真实数据往往是由类似高斯混合分布这样的形式所产生。</p>
<p>回归函数就是求给定x下的条件期望。（个人以为推广到分类问题中也没有问题。。）<br>\begin{split} f(x) = E(Y|X = x)\end{split}</p>
<p>最近邻则是<br>\begin{split} \hat{f(x)} = Ave(y_i|x_i \in N_k(x))\end{split}<br>发生了两次近似：</p>
<ol>
<li>用均值来近似期望</li>
<li>在点上取条件放宽为在“靠近”目标点的某区域上取条件</li>
</ol>
<p>可以证明：<br>\begin{split} \hat{f(x)} \rightarrow E(Y|X = x) \text{ when } N,k \rightarrow \infty,N/k \rightarrow 0,\end{split}</p>
<p>那么最近邻的缺点在哪里呢？</p>
<ol>
<li>在样本容量不够大的情况下，收敛效果并不会很好</li>
<li>随着样本维度提高，邻域度量规模也增大。虽然收敛性依然成立，但是收敛速度会随着维数增加而降低。</li>
</ol>
<p>因此我们可以考虑某种更结构化的模型，通常可以得到更稳定的估值。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2017/05/04/machine learning/dimensionality reduction/reduction/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/05/04/machine learning/dimensionality reduction/reduction/" class="post-title-link" itemprop="url">reduction</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-05-04 17:57:11" itemprop="dateCreated datePublished" datetime="2017-05-04T17:57:11+08:00">2017-05-04</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/dimensionality-reduction/" itemprop="url" rel="index"><span itemprop="name">dimensionality reduction</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p>
<h1 id="一些进行降维的方法"><a href="#一些进行降维的方法" class="headerlink" title="一些进行降维的方法"></a>一些进行降维的方法</h1><ul>
<li>use clustering to extract feature</li>
<li>t-sne</li>
<li>pca</li>
<li>autoencoder</li>
<li>any others to reduction?</li>
</ul>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2017/05/04/machine learning/ensemble/gcforest/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/05/04/machine learning/ensemble/gcforest/" class="post-title-link" itemprop="url">gcforest</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-05-04 17:14:25" itemprop="dateCreated datePublished" datetime="2017-05-04T17:14:25+08:00">2017-05-04</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/ensemble/" itemprop="url" rel="index"><span itemprop="name">ensemble</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type"text javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p>
<h1 id="gcForest"><a href="#gcForest" class="headerlink" title="gcForest"></a>gcForest</h1><p>gcForest包含两大模块：级联森林和多粒度扫描。<br><img src="http://i4.buimg.com/567571/19c73bb7503caa72.png" alt></p>
<h1 id="多粒度扫描（Multi-Grained-Scanning）"><a href="#多粒度扫描（Multi-Grained-Scanning）" class="headerlink" title="多粒度扫描（Multi-Grained Scanning）"></a>多粒度扫描（Multi-Grained Scanning）</h1><p><img src="http://omdhuynsr.bkt.clouddn.com/17-5-4/47305608-file_1493888234733_e64c.png" alt><br>对特征进行有规则滑动采样，拿采样后数据分别建立标准随机森林和完全随机森林，将输出概率进行拼接构成新的特征向量。</p>
<h1 id="级联森林（Cascade-Forest）"><a href="#级联森林（Cascade-Forest）" class="headerlink" title="级联森林（Cascade Forest）"></a>级联森林（Cascade Forest）</h1><p><img src="http://omdhuynsr.bkt.clouddn.com/17-5-4/88211704-file_1493888546752_1059d.png" alt><br>利用多个随机森林训练的输出作为新特征和原特征合并，多次级联操作，最终多个森林输出结果取均值，得到最后的类别概率分布。<br>每次级联后验证输出效果是否有提升，没有提升则终止级联，输出结果。</p>
<h1 id="references"><a href="#references" class="headerlink" title="references"></a>references</h1><p><a href="https://www.qcloud.com/community/article/536731001491381531" target="_blank" rel="noopener">小菜鸟对周志华大神gcForest的理解</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2017/05/03/machine learning/statistics/方差、偏差、噪声/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/05/03/machine learning/statistics/方差、偏差、噪声/" class="post-title-link" itemprop="url">方差、偏差、噪声</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-05-03 17:39:42" itemprop="dateCreated datePublished" datetime="2017-05-03T17:39:42+08:00">2017-05-03</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/statistics/" itemprop="url" rel="index"><span itemprop="name">statistics</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p>
<h1 id="误差-随机误差和系统误差之间的关系"><a href="#误差-随机误差和系统误差之间的关系" class="headerlink" title="误差,随机误差和系统误差之间的关系"></a><a href="http://www.gfjl.org/thread-175264-1-1.html" target="_blank" rel="noopener">误差,随机误差和系统误差之间的关系</a></h1><p>误差的来源可以分为系统误差（又称可定误差）、随机误差（又称未定误差）和毛误差（又称过失误差）</p>
<p>系统误差(System error)分为固定误差与比例误差，原因可能有仪器本身误差(instrumental errors)、采用方法的误差(method errors)、个人误差(personal errors)、环境误差（Environmental error）。理论上系统误差可以通过一定的手段(如:校正)来消除。举例而言，天平的两臂应是等长的，可实际上是不可能完全相等的；天平配置的相同质量的砝码应是一样的，可实际上它们不可能达到一样。</p>
<p>随机误差(Random error)，无法控制的变因，会使得测量值产生随机分布的误差。它服从统计学上所谓的“正态分布”或称“高斯分布”，它是不可消除的，在这个意义上，测量对象的真值是永远不可知的，只能通过多次测量获得的均值尽量逼近。系统误差以相同的方式影响所有测量值，将它们推向同一个方向；随机误差，则随着不同次的测量而变化，有时候向上或向下。</p>
<p>毛误差(Gross error)，毛误差主要是由于测量者的疏忽犯下不应有的错误造成的。例如读数错误、记录错误、测量时发生未察觉的异常情况等等，这种误差是可以避免的(如:舍弃有关数据重新测量)。</p>
<ul>
<li>系统误差中的个人误差(personal errors)与毛误差(Gross error)的差别<br>个人误差又称人员误差，是由于测定人员的分辨力、反应速度的差异和固有习惯引起的误差。这类误差往往因人而异，因而可以采取让不同人员进行分析，以平均值报告分析结果的方法予以限制。<br>毛误差主要是由于测量者的疏忽所造成的。</li>
</ul>
<p>残差是原值与拟合值的差――与预测有关，残差大小可以衡量预测的准确性。残差越大表示预测越不准确。残差与数据本身的分布特性，回归方程的选择有关。</p>
<h1 id="总平方和（SST）、回归平方和（SSR）与残差平方和（SSE）"><a href="#总平方和（SST）、回归平方和（SSR）与残差平方和（SSE）" class="headerlink" title="总平方和（SST）、回归平方和（SSR）与残差平方和（SSE）"></a>总平方和（SST）、回归平方和（SSR）与残差平方和（SSE）</h1><p>\begin{split} &amp;&amp;SST = \sum_{i=1}^n(y_i-\overline{y})^2 \newline<br>&amp;&amp;SSE = \sum_{i=1}^n(y_i - \hat{y_i})^2 \newline<br>&amp;&amp;SSR = \sum_{i=1}^n(\hat{y_i}-\overline{y})^2 \end{split}</p>
<p>SST是真值与均值的差平方和：衡量的是被解释变量(Y)波动的程度或不确定性的程度<br>SSR是估值与均值的差平方和：衡量的是被解释变量(Y)不确定性程度中能被解释变量(X)解释的部分<br>SSE是真值与估值的差平方和：衡量的是被解释变量(Y)不确定性程度中不能被解释变量(X)解释的部分<br>SSE很容易理解，SSR可以理解为SST-SSE的余项。</p>
<h1 id="偏差和方差"><a href="#偏差和方差" class="headerlink" title="偏差和方差"></a>偏差和方差</h1><p>偏差刻画的是真值与预测值之间的偏离程度。  </p>
<p>方差刻画的是模型在不同的样本集上performance的波动情况。</p>
<p>详参周志华《机器学习》(P45):</p>
<p>\begin{split} E_D[(f(x;D)-y_D)^2] &amp;&amp;= E_D[(f(x;D) - \overline{f}(x))^2] + (\overline{f}(x) - y)^2 +E_D[(y_D - y)^2] \newline<br>&amp;&amp;= var(x) + bias^2(x) + \epsilon^2 \newline<br>&amp;&amp; \overline{f}(x) = E_D[(f(x;D))] \newline<br>&amp;&amp; y_D \text{为x在数据集D中的标记; } y \text{为x真实标记; } f(x;D) \text{为算法在D上预测标记 }\end{split}</p>
<p>范化误差可以分解为偏差，方差与噪声之和。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2017/04/19/machine learning/optimization/拉格朗日乘子法和KKT条件/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/04/19/machine learning/optimization/拉格朗日乘子法和KKT条件/" class="post-title-link" itemprop="url">拉格朗日乘子法和KKT条件</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-04-19 18:51:01" itemprop="dateCreated datePublished" datetime="2017-04-19T18:51:01+08:00">2017-04-19</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/optimization/" itemprop="url" rel="index"><span itemprop="name">optimization</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p>
<h1 id="最优化问题"><a href="#最优化问题" class="headerlink" title="最优化问题"></a>最优化问题</h1><ol>
<li><p>无约束优化问题<br>可以通过Fermat定理，即令导数为0求得</p>
</li>
<li><p>等式约束优化问题<br>利用拉格朗日乘子法求得</p>
</li>
<li><p>不等式约束优化问题<br>一般是转化为小于等于的约束形式，然后利用KKT条件求得</p>
</li>
</ol>
<h1 id="拉格朗日乘子法"><a href="#拉格朗日乘子法" class="headerlink" title="拉格朗日乘子法"></a>拉格朗日乘子法</h1><p>必须明确的是，通过拉格朗日乘子法所求出的点不一定是极值点！在拉格朗日乘子法可用情况下，它是必要不充分条件。</p>
<p>可以通过图形来进一步理解拉格朗日乘子法。如下图，在$g(x,y)=c$约束下，$f(x,y)$极值在切点处取得。其实，换成不等式约束同样可以相似理解。只需把下图中约束条件绘制成绿线及以上部分。此处不严谨，不需深究。<br><img src="http://p1.bpimg.com/567571/1b0bfa20bbb4f7b0.png" alt></p>
<h1 id="KKT条件"><a href="#KKT条件" class="headerlink" title="KKT条件"></a>KKT条件</h1><ol>
<li><p>$g_i(x^<em>) \leq 0;h_j(x^</em>) = 0$</p>
</li>
<li><p>$\nabla f(x^<em>) + \sum\limits_{i=1}^p \mu_i^</em> \nabla g_i(x^<em>) + \sum\limits_{j=1}^q \lambda_j^</em> \nabla h_j(x^*) = 0$</p>
</li>
<li><p>$\mu_i^<em> \geq 0;\mu_i g(x^</em>) = 0$</p>
</li>
</ol>
<p>事实上，就是指需要满足约束条件，以及梯度消失（为0）。</p>
<h1 id="对偶"><a href="#对偶" class="headerlink" title="对偶"></a>对偶</h1><p>我目前发现最清晰的讲解<a href="http://blog.pluskid.org/?p=702" target="_blank" rel="noopener">支持向量机：Duality</a>*</p>
<p>设：<br>$$L(x,\lambda,\mu) = f(x) + \sum_{i=1}^p \mu_i g_i(x) + \sum_{j=1}^q \lambda_j h_j(x)$$</p>
<p>令：<br>$$z(x) = \max_{\mu \geq 0,\lambda} L(x,\lambda,\mu)$$<br>因为限制 $\mu \geq 0$,所以 $\mu_i g_i(x) \leq 0$,故而最大值 $z(x) = f(x)$ 。  </p>
<p>这样一来，原问题转化为 $\min\limits_{x} z(x)$ ，即是primal problem：<br>$$\min_{x} \max_{\mu \geq 0,\lambda} L(x,\lambda,\mu)$$<br>记其解为$p^*$</p>
<p>另一方面，令：<br>$$g(\lambda,\mu) = \min_x L(x,\lambda,\mu)$$</p>
<p>则有：<br>$$g(\lambda,\mu) = \min_x L(x,\lambda,\mu) \leq L(x^<em>,\lambda,\mu) \leq f(x^</em>) = p^*$$</p>
<p>因此，$g$为下界，则最大下界为：<br>$$\max_{\mu \geq 0,\lambda} g(\lambda,\mu)$$<br>这就是dual problem，记其最优值为$d^<em>$,则有：<br>$$d^</em> \leq p^*$$<br>这个性质叫做weak duality。  </p>
<p>如果满足：<br>$$d^<em> = p^</em>$$<br>这个性质叫做strong duality。</p>
<p>任何满足strong duality的问题都满足KKT条件，换句话说，KKT仅仅是必要条件。但当原问题是凸优化问题时，KKT可以升级为充要条件。</p>
<h1 id="references"><a href="#references" class="headerlink" title="references"></a>references</h1><p><a href="http://jacoxu.com/%E6%9C%80%E4%BC%98%E5%8C%96%E7%90%86%E8%AE%BA%E4%B8%8Ekkt%E6%9D%A1%E4%BB%B6/" target="_blank" rel="noopener">最优化理论与KKT条件</a>  </p>
<p><a href="http://blog.csdn.net/johnnyconstantine/article/details/46335763" target="_blank" rel="noopener">KKT条件介绍</a></p>
<p><strong><em><a href="http://blog.pluskid.org/?p=702" target="_blank" rel="noopener">支持向量机：Duality</a></em></strong></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2017/04/16/machine learning/imbalance data/learning from imbalance data/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/04/16/machine learning/imbalance data/learning from imbalance data/" class="post-title-link" itemprop="url">learning from imbalance data</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-04-16 10:41:51" itemprop="dateCreated datePublished" datetime="2017-04-16T10:41:51+08:00">2017-04-16</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/imbalance-data/" itemprop="url" rel="index"><span itemprop="name">imbalance data</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p>
<h1 id="references"><a href="#references" class="headerlink" title="references"></a>references</h1><p><a href="https://zhpmatrix.github.io/2017/02/20/learning-from-imbalanced-data/" target="_blank" rel="noopener">Learning from Imbalanced Data</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2017/04/13/machine learning/dimensionality reduction/PCA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/04/13/machine learning/dimensionality reduction/PCA/" class="post-title-link" itemprop="url">PCA</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-04-13 12:34:18" itemprop="dateCreated datePublished" datetime="2017-04-13T12:34:18+08:00">2017-04-13</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/dimensionality-reduction/" itemprop="url" rel="index"><span itemprop="name">dimensionality reduction</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p>
<h1 id="PCA原理"><a href="#PCA原理" class="headerlink" title="PCA原理"></a>PCA原理</h1><p><a href="http://blog.codinglabs.org/articles/pca-tutorial.html" target="_blank" rel="noopener">PCA的数学原理</a> 这篇blog已经做非常好的说明。我在这里简单归纳下。</p>
<p>PCA是一种降维技术。它希望降维后样本的新生成的特征方差尽可能大，同时特征之间的相关性尽可能小，以此来尽可能的保留全部信息信息。</p>
<p>方差尽可能大的直观理解是数据越分散，差异性越大，体现的信息越多。</p>
<p>特征间相关性尽可能小的直观理解是避免相同信息在不同特征中重复出现。</p>
<h1 id="PCA进一步讨论"><a href="#PCA进一步讨论" class="headerlink" title="PCA进一步讨论"></a>PCA进一步讨论</h1><p>传统PCA是使用协方差度量相关性。因此所度量的也只是线性相关性。对于非线性相关，可以考虑使用KernelPCA，通过Kernel函数将非线性相关转为线性相关。</p>
<p>PCA假设数据各主特征是分布在正交方向上，如果在非正交方向上存在几个方差较大的方向，PCA的效果就大打折扣了。</p>
<h1 id="Q-amp-A"><a href="#Q-amp-A" class="headerlink" title="Q&amp;A"></a>Q&amp;A</h1><ol>
<li><p>PCA在使用过程中必须标准化么？为什么？<br>PCA追求特征之间的协方差为0.而计算协方差需要对特征去中心化。<br>[TBC]不去中心化会有什么坏处？是否需要归一化消除量纲？</p>
</li>
<li><p>为什么协方差可以度量相关性？并且是线性相关性？<br>根本上是使用内积进行相似性度量，消除量纲后就是单位向量间的夹角余弦值。<br>从线性拟合和最小二乘的角度来看，拟合效果越好，相关系数绝对值越接近于1。从这个角度可以说相关系数刻画的是线性相关关系。（《概率论与数理统计》陈希孺 中科大出版社 第三章3.3节）</p>
</li>
<li><p>相关系数和修正余弦相似度区别？<br>根本上都是使用内积进行相似性度量，消除量纲后就是单位向量间的夹角余弦值。<br>区别在于去中心化上。<br>相关系数考虑的是消除不同样本的基数差异。比如A可能对所有电影的打分都虚高，而B都会压低，相关系数可以消除这样的基数。<br>修正余弦相似度考虑的是消除不同特征之间的量纲差异。比如电影a的评分水平高于电影b，消除之后就不会令电影a对于相似性度量带来更大的影响。保持不同电影权重相同。<br>数据稀疏时选择修正余弦相似度更好。</p>
</li>
</ol>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2017/04/12/python/discuss/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/04/12/python/discuss/" class="post-title-link" itemprop="url">discuss</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-04-12 16:30:57" itemprop="dateCreated datePublished" datetime="2017-04-12T16:30:57+08:00">2017-04-12</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/python/" itemprop="url" rel="index"><span itemprop="name">python</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="the-differences-between-package-and-module"><a href="#the-differences-between-package-and-module" class="headerlink" title="the differences between package and module"></a>the differences between package and module</h1><ul>
<li>the <strong>name</strong> of module from where?</li>
</ul>
<h1 id="the-effects-of-init-py"><a href="#the-effects-of-init-py" class="headerlink" title="the effects of init.py"></a>the effects of <strong>init</strong>.py</h1><h1 id="the-process-of-import"><a href="#the-process-of-import" class="headerlink" title="the process of import"></a>the process of import</h1><h1 id="the-meanings-of-and"><a href="#the-meanings-of-and" class="headerlink" title="the meanings of _ and __"></a>the meanings of _ and __</h1><h1 id="if-name-‘main‘"><a href="#if-name-‘main‘" class="headerlink" title="if name == ‘main‘"></a>if <strong>name</strong> == ‘<strong>main</strong>‘</h1><h1 id="class中def如果是做有关self修改，可以不需要return。而self自己就会变化么？"><a href="#class中def如果是做有关self修改，可以不需要return。而self自己就会变化么？" class="headerlink" title="class中def如果是做有关self修改，可以不需要return。而self自己就会变化么？"></a>class中def如果是做有关self修改，可以不需要return。而self自己就会变化么？</h1>
          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2017/04/11/machine learning/NN/rnn/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/04/11/machine learning/NN/rnn/" class="post-title-link" itemprop="url">rnn</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-04-11 20:09:22" itemprop="dateCreated datePublished" datetime="2017-04-11T20:09:22+08:00">2017-04-11</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/NN/" itemprop="url" rel="index"><span itemprop="name">NN</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<h1 id="lstm"><a href="#lstm" class="headerlink" title="lstm"></a>lstm</h1><p>此处先说明tensorflow中以下代码的输出<br><figure class="highlight stylus"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">outputs,final_state = tf<span class="selector-class">.nn</span><span class="selector-class">.dynamic_rnn</span>(lstm_cell,x_in,initial_state=init_state,time_major=False)</span><br></pre></td></tr></table></figure></p>
<p>outputs储存每个时刻的输出，也就是下面的分线输出（它也同时被复制为当时时刻的外部输出）,它的形状是[batch_size,n_steps,n_hidden_units]（如果一开始的输入形式为[n_steps,batch_size,n_inputs]，则time_major=True，output形状为[n_steps,batch_size,n_hidden_units]）</p>
<p>final_state仅仅只记录最后时刻的主线状态和分线状态，默认以元组形式保存，主线状态在前。</p>
<p>因此，分线state和outputs最后一个时刻的记录是相同的，至少在基础lstm上。</p>
<p>lstm一开始需要定义的初始状态应该就是指一开始的主线和分线state。</p>
<p><img src="http://i2.muimg.com/567571/e28747d6a50b8fad.png" alt></p>
<h1 id="references"><a href="#references" class="headerlink" title="references"></a>references</h1><p><a href="http://blog.csdn.net/mydear_11000/article/details/52414342" target="_blank" rel="noopener">解读tensorflow之rnn</a><br><a href="http://www.jianshu.com/p/9dc9f41f0b29" target="_blank" rel="noopener">[译] 理解 LSTM 网络</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2017/04/02/machine learning/NLP/WMD/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/04/02/machine learning/NLP/WMD/" class="post-title-link" itemprop="url">WMD</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-04-02 22:17:37" itemprop="dateCreated datePublished" datetime="2017-04-02T22:17:37+08:00">2017-04-02</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type"text javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p>
<h1 id="文章出处"><a href="#文章出处" class="headerlink" title="文章出处"></a>文章出处</h1><p>本文为<a href="https://github.com/RaRe-Technologies/gensim/blob/develop/docs/notebooks/WMD_tutorial.ipynb" target="_blank" rel="noopener">WMD_tutorial</a>的翻译。</p>
<h1 id="使用W2V和WMD发现文档的相似性"><a href="#使用W2V和WMD发现文档的相似性" class="headerlink" title="使用W2V和WMD发现文档的相似性"></a>使用W2V和WMD发现文档的相似性</h1><p>WMD（Word Mover’s Distance）是机器学习中一个有前途的新工具，它允许我们提交查询并返回最相似的文档。例如，在博客<a href="http://tech.opentable.com/2015/08/11/navigating-themes-in-restaurant-reviews-with-word-movers-distance/" target="_blank" rel="noopener">OpenTable</a>中，使用了WMD分析了餐厅评论。通过使用这种方法，他们能够从评论中挖掘出不同的方面。这本教程的第二部分，我们展示了如何使用gensim的WmdSimilarity去做类似与opentable所做的事情。在第一部分，我们说明了如何使用wmdistance去计算两个文档的WMD距离。如果你的目的是想使用WmdSimilarity，第一部分可以选读，但它是有意义的。</p>
<p>不管怎样，首先，我们浏览下关于wmd是什么的基础知识。</p>
<h1 id="WMD基础"><a href="#WMD基础" class="headerlink" title="WMD基础"></a>WMD基础</h1><p>wmd允许我们用一种有意义的方式去评估两个文档之间的距离，即使他们之间没有共同词汇。他使用w2v进行词向量嵌入。它的表现优于很多最先进的k近邻分类方法。</p>
<p>wmd被下面两个非常相似的句子所阐明（图例来源于<a href="http://vene.ro/blog/word-movers-distance-in-python.html" target="_blank" rel="noopener">Vlad Niculae’s blog</a>）。两个句子之间没有共同词汇，但是通过匹配相关词，wmd能够精确度量两个句子之间的相似性。这个方法也使用了文档的词袋表征（简单的说，文档的词频），在下面的图中被标记为d。该方法的直观理解是我们发现文档之间最小的”traveling distance”，换句话说将文档1的分布移向文档2的最有效方式。</p>
<p><img src="http://i1.piimg.com/567571/651dd1b6e0d46b4b.png" alt></p>
<p>这个方法已经在文章<a href="http://jmlr.org/proceedings/papers/v37/kusnerb15.pdf" target="_blank" rel="noopener">“From Word Embeddings To Document Distances” by Matt Kusner et al.</a>中被介绍。它被”Earth Mover’s Distance”激发灵感，并且利用了 “transportation problem”的一个解法。</p>
<p>在本教程中，我们将学习如何使用gensim的wmd函数。它由进行距离计算的wmdistance方法和基于相似查询的corpus计算的WmdSimilarity类构成。</p>
<blockquote>
<p>注意：<br>  如果你使用这个软件，请留意引用[1]，[2]和[3]。</p>
</blockquote>
<h1 id="运行notebook"><a href="#运行notebook" class="headerlink" title="运行notebook"></a>运行notebook</h1><p>你可以下载这个<a href="http://ipython.org/notebook.html" target="_blank" rel="noopener">iPython Notebook</a>，并在你自己电脑上运行它，如果你已经安装了gensim，pyemd，nltk，并下载了必要的数据。</p>
<p>这个notebook被运行在i7-4770cpu 3.40GHz (8 cores) 和 32 GB memory的ubuntu机器上。在这台机器上运行整个notebook需要话费3分钟。</p>
<h1 id="第一部分：计算词移距离（Word-Mover’s-Distance）"><a href="#第一部分：计算词移距离（Word-Mover’s-Distance）" class="headerlink" title="第一部分：计算词移距离（Word Mover’s Distance）"></a>第一部分：计算词移距离（Word Mover’s Distance）</h1><p>为了使用wmd，我们首先需要一些词嵌入。你需要在一些corpus上训练一个w2v模型（<a href="https://rare-technologies.com/word2vec-tutorial/" target="_blank" rel="noopener">教程</a>），但是我们将通过下载一些预训练的w2v嵌入模型来开始。下载 <a href="https://code.google.com/archive/p/word2vec/" target="_blank" rel="noopener">GoogleNews-vectors-negative300.bin.gz</a>（警告：1.5GB，第二部分并不需要这些文件）。训练你自己的嵌入模型是有益处的，但为了简化本教程，我们将首先使用预训练的嵌入。</p>
<p>让我们拿一些句子来计算它们之间的相似度。</p>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">from</span> <span class="built_in">time</span> import <span class="built_in">time</span></span><br><span class="line">start_nb = <span class="built_in">time</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize logging.</span></span><br><span class="line">import logging</span><br><span class="line">logging.basicConfig(<span class="built_in">format</span>=<span class="string">'%(asctime)s : %(levelname)s : %(message)s'</span>)</span><br><span class="line"></span><br><span class="line">sentence_obama = <span class="string">'Obama speaks to the media in Illinois'</span></span><br><span class="line">sentence_president = <span class="string">'The president greets the press in Chicago'</span></span><br><span class="line">sentence_obama = sentence_obama.<span class="built_in">lower</span>().<span class="built_in">split</span>()</span><br><span class="line">sentence_president = sentence_president.<span class="built_in">lower</span>().<span class="built_in">split</span>()</span><br></pre></td></tr></table></figure>
<p>这些句子有很相似的内容，因此wmd应该低。在我们计算wmd之前，我们想要移除停用词（”the”, “to”, etc.），因为它们对句子信息并没有很多贡献。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Import and download stopwords from NLTK.</span></span><br><span class="line"><span class="keyword">from</span> nltk.corpus <span class="keyword">import</span> stopwords</span><br><span class="line"><span class="keyword">from</span> nltk <span class="keyword">import</span> download</span><br><span class="line">download(<span class="string">'stopwords'</span>)  <span class="comment"># Download stopwords list.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Remove stopwords.</span></span><br><span class="line">stop_words = stopwords.words(<span class="string">'english'</span>)</span><br><span class="line">sentence_obama = [w <span class="keyword">for</span> w <span class="keyword">in</span> sentence_obama <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> stop_words]</span><br><span class="line">sentence_president = [w <span class="keyword">for</span> w <span class="keyword">in</span> sentence_president <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> stop_words]</span><br></pre></td></tr></table></figure>
<p>现在，正如先前提到的，我们将使用一些下载的预训练嵌入。我们加载这些进入gensim的w2v模型类中。注意我们这里选择的嵌入需要很多内存。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">start</span> = <span class="built_in">time</span>()</span><br><span class="line"><span class="keyword">import</span> os</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> gensim.models <span class="keyword">import</span> KeyedVectors</span><br><span class="line"><span class="keyword">if</span> <span class="keyword">not</span> os.path.exists(<span class="string">'/data/w2v_googlenews/GoogleNews-vectors-negative300.bin.gz'</span>):</span><br><span class="line">    <span class="keyword">raise</span> ValueError(<span class="string">"SKIP: You need to download the google news model"</span>)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">model</span> = KeyedVectors.load_word2vec_format(<span class="string">'/data/w2v_googlenews/GoogleNews-vectors-negative300.bin.gz'</span>, <span class="built_in">binary</span>=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'Cell took %.2f seconds to run.'</span> % (<span class="built_in">time</span>() - <span class="keyword">start</span>))</span><br></pre></td></tr></table></figure>
<p>因此让我们使用wmdistance方法计算wmd。</p>
<figure class="highlight swift"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">distance</span> = model.wmdistance(sentence_obama, sentence_president)</span><br><span class="line"><span class="built_in">print</span> '<span class="built_in">distance</span> = %.4f' % <span class="built_in">distance</span></span><br></pre></td></tr></table></figure>
<p>让我们对完全不相关的句子做相同的操作。注意距离变大了。</p>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">sentence_orange = <span class="string">'Oranges are my favorite fruit'</span></span><br><span class="line">sentence_orange = sentence_orange.<span class="built_in">lower</span>().<span class="built_in">split</span>()</span><br><span class="line">sentence_orange = [w <span class="keyword">for</span> w <span class="keyword">in</span> sentence_orange <span class="keyword">if</span> w <span class="keyword">not</span> <span class="keyword">in</span> stop_words]</span><br><span class="line"></span><br><span class="line">distance = model.wmdistance(sentence_obama, sentence_orange)</span><br><span class="line">print <span class="string">'distance = %.4f'</span> % distance</span><br></pre></td></tr></table></figure>
<h1 id="正则化w2v向量"><a href="#正则化w2v向量" class="headerlink" title="正则化w2v向量"></a>正则化w2v向量</h1><p>当使用wmdistance方法时，首先正则化w2v向量是有益的，因此它们都有相同的长度。为了实现正则化，简单的调用model.init_sims(replace=True)，gensim会为你实现它。</p>
<p>通常，使用<a href="https://en.wikipedia.org/wiki/Cosine_similarity" target="_blank" rel="noopener">余弦距离</a>度量两个w2v向量之间的距离。余弦距离度量的是两个向量之间的夹角。另一方面，wmd使用欧式距离。两个向量之间的欧式距离可能会因为她们之间的长度区别而变得很大，但是因为它们之间的夹角很小因此余弦距离很小。我们可以通过正则化向量减轻这个问题。</p>
<p>注意正则化向量会花费一些时间，特别是你有一个大的词汇表 和/或 大的向量集。</p>
<p>用法在下面的例子中被阐明。碰巧我们下载的向量已经被正则化了。因此，在这个例子中我们不会作出什么差别。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Normalizing word2vec vectors.</span></span><br><span class="line"><span class="keyword">start</span> = <span class="built_in">time</span>()</span><br><span class="line"></span><br><span class="line">model.init_sims(<span class="keyword">replace</span>=<span class="literal">True</span>)  <span class="comment"># Normalizes the vectors in the word2vec class.</span></span><br><span class="line"></span><br><span class="line">distance = model.wmdistance(sentence_obama, sentence_president)  <span class="comment"># Compute WMD as normal.</span></span><br><span class="line"></span><br><span class="line">print <span class="string">'Cell took %.2f seconds to run.'</span> %(<span class="built_in">time</span>() - <span class="keyword">start</span>)</span><br></pre></td></tr></table></figure>
<h1 id="第二部分：使用WmdSimilarity进行相似度查询"><a href="#第二部分：使用WmdSimilarity进行相似度查询" class="headerlink" title="第二部分：使用WmdSimilarity进行相似度查询"></a>第二部分：使用WmdSimilarity进行相似度查询</h1><p>你可以通过WmdSimilarity类，使用wmd得到一个查询对应的最相似的文档。它的交互过程相似于在gensim教程<a href="https://radimrehurek.com/gensim/tut3.html" target="_blank" rel="noopener">Similarity Queries</a>中所描绘的。</p>
<blockquote>
<p>重要注意<br> wmd是一种距离度量。WmdSimilarity中的相似度是简单的负距离。注意不要混淆距离和相似度。两个相似文档将有一个高相似得分和一个小距离;两个差异文档将有低的相似得分和一个大距离。</p>
</blockquote>
<h2 id="Yelp-data"><a href="#Yelp-data" class="headerlink" title="Yelp data"></a>Yelp data</h2><p>让我们使用一些真实世界数据来尝试下相似度查询。为此我们使用<a href="https://www.yelp.com/dataset_challenge" target="_blank" rel="noopener">yelp评论</a>。特别的，我们将使用单一餐馆也就是<a href="https://en.yelp.be/biz/mon-ami-gabi-las-vegas-2" target="_blank" rel="noopener">Mon Ami Gabi</a>的评论。</p>
<p>为了得到yelp数据，你需要使用名字和邮箱进行注册。数据是775MB。</p>
<p>这一次，我们将用我们自己的数据去训练W2V嵌入。一个餐馆的数据并不足以合适的训练出w2v，因此我们使用了6个餐馆的数据进行训练。但是仅仅在他们中的一个进行相似度查询试验。除了上面提到的Mon Ami Gabi，我们还将使用：</p>
<ul>
<li>Earl of Sandwich.</li>
<li>Wicked Spoon.</li>
<li>Serendipity 3.</li>
<li>Bacchanal Buffet.</li>
<li>The Buffet.</li>
</ul>
<p>我们选择的餐馆是yelp数据集中那些具有最高数量评论的餐馆。顺带一提，它们都在Las Vegas Boulevard中。我们用于训练w2v的corpus具有18957条文档（评论），而用于WmdSimilarity的corpus具有4137条文档。</p>
<p>下面代码是一个yelp评论的json文件被按行读取，文本被抽取，分词，停用词和标点符号被移除。</p>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Pre-processing a document.</span></span><br><span class="line"></span><br><span class="line"><span class="built_in">from</span> nltk import word_tokenize</span><br><span class="line">download(<span class="string">'punkt'</span>)  <span class="comment"># Download data for tokenizer.</span></span><br><span class="line"></span><br><span class="line">def preprocess(doc):</span><br><span class="line">    doc = doc.<span class="built_in">lower</span>()  <span class="comment"># Lower the text.</span></span><br><span class="line">    doc = word_tokenize(doc)  <span class="comment"># Split into words.</span></span><br><span class="line">    doc = [w <span class="keyword">for</span> w <span class="keyword">in</span> doc <span class="keyword">if</span> <span class="keyword">not</span> w <span class="keyword">in</span> stop_words]  <span class="comment"># Remove stopwords.</span></span><br><span class="line">    doc = [w <span class="keyword">for</span> w <span class="keyword">in</span> doc <span class="keyword">if</span> w.isalpha()]  <span class="comment"># Remove numbers and punctuation.</span></span><br><span class="line">    <span class="literal">return</span> doc</span><br><span class="line"></span><br><span class="line"><span class="built_in">start</span> = <span class="built_in">time</span>()</span><br><span class="line"></span><br><span class="line">import json</span><br><span class="line"></span><br><span class="line"><span class="comment"># Business IDs of the restaurants.</span></span><br><span class="line">ids = [<span class="string">'4bEjOyTaDG24SY5TxsaUNQ'</span>, <span class="string">'2e2e7WgqU1BnpxmQL5jbfw'</span>, <span class="string">'zt1TpTuJ6y9n551sw9TaEg'</span>,</span><br><span class="line">      <span class="string">'Xhg93cMdemu5pAMkDoEdtQ'</span>, <span class="string">'sIyHTizqAiGu12XMLX3N3g'</span>, <span class="string">'YNQgak-ZLtYJQxlDwN-qIg'</span>]</span><br><span class="line"></span><br><span class="line">w2v_corpus = []  <span class="comment"># Documents to train word2vec on (all 6 restaurants).</span></span><br><span class="line">wmd_corpus = []  <span class="comment"># Documents to run queries against (only one restaurant).</span></span><br><span class="line">documents = []  <span class="comment"># wmd_corpus, with no pre-processing (so we can see the original documents).</span></span><br><span class="line"><span class="keyword">with</span> <span class="built_in">open</span>(<span class="string">'/data/yelp_academic_dataset_review.json'</span>) <span class="keyword">as</span> data_file:</span><br><span class="line">    <span class="keyword">for</span> <span class="built_in">line</span> <span class="keyword">in</span> data_file:</span><br><span class="line">        json_line = json.loads(<span class="built_in">line</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> json_line[<span class="string">'business_id'</span>] <span class="keyword">not</span> <span class="keyword">in</span> ids:</span><br><span class="line">            <span class="comment"># Not one of the 6 restaurants.</span></span><br><span class="line">            continue</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Pre-process document.</span></span><br><span class="line">        <span class="keyword">text</span> = json_line[<span class="string">'text'</span>]  <span class="comment"># Extract text from JSON object.</span></span><br><span class="line">        <span class="keyword">text</span> = preprocess(<span class="keyword">text</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="comment"># Add to corpus for training Word2Vec.</span></span><br><span class="line">        w2v_corpus.append(<span class="keyword">text</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> json_line[<span class="string">'business_id'</span>] == ids[<span class="number">0</span>]:</span><br><span class="line">            <span class="comment"># Add to corpus for similarity queries.</span></span><br><span class="line">            wmd_corpus.append(<span class="keyword">text</span>)</span><br><span class="line">            documents.append(json_line[<span class="string">'text'</span>])</span><br><span class="line"></span><br><span class="line">print <span class="string">'Cell took %.2f seconds to run.'</span> %(<span class="built_in">time</span>() - <span class="built_in">start</span>)</span><br></pre></td></tr></table></figure>
<p>下面是一个文档长度的直方图，该图中也包含了平均文档长度。注意这些是经过预处理的文档，也就是说停用词已经被移除，标点符号已经被移除，等等。文档长度对wmd的运行时间有很大的影响，因此当和本次实验比较运行时间时，查询corpus的文档数量（大约4000）和文档长度（大约平均62个词）应该被考虑在内。</p>
<figure class="highlight livecodeserver"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="built_in">from</span> matplotlib import pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># Document lengths.</span></span><br><span class="line">lens = [<span class="built_in">len</span>(doc) <span class="keyword">for</span> doc <span class="keyword">in</span> wmd_corpus]</span><br><span class="line"></span><br><span class="line"><span class="comment"># Plot.</span></span><br><span class="line">plt.rc(<span class="string">'figure'</span>, figsize=(<span class="number">8</span>,<span class="number">6</span>))</span><br><span class="line">plt.rc(<span class="string">'font'</span>, size=<span class="number">14</span>)</span><br><span class="line">plt.rc(<span class="string">'lines'</span>, linewidth=<span class="number">2</span>)</span><br><span class="line">plt.rc(<span class="string">'axes'</span>, color_cycle=(<span class="string">'#377eb8'</span>,<span class="string">'#e41a1c'</span>,<span class="string">'#4daf4a'</span>,</span><br><span class="line">                            <span class="string">'#984ea3'</span>,<span class="string">'#ff7f00'</span>,<span class="string">'#ffff33'</span>))</span><br><span class="line"><span class="comment"># Histogram.</span></span><br><span class="line">plt.hist(lens, bins=<span class="number">20</span>)</span><br><span class="line">plt.hold(True)</span><br><span class="line"><span class="comment"># Average length.</span></span><br><span class="line">avg_len = <span class="built_in">sum</span>(lens) / float(<span class="built_in">len</span>(lens))</span><br><span class="line">plt.axvline(avg_len, color=<span class="string">'#e41a1c'</span>)</span><br><span class="line">plt.hold(False)</span><br><span class="line">plt.title(<span class="string">'Histogram of document lengths.'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'Length'</span>)</span><br><span class="line">plt.<span class="keyword">text</span>(<span class="number">100</span>, <span class="number">800</span>, <span class="string">'mean = %.2f'</span> % avg_len)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>
<p><img src="http://i2.muimg.com/567571/479272b7085337df.png" alt></p>
<p>现在，我们想要用corpus和w2v初始化相似类（提供了嵌入和wmdistance方法）。</p>
<figure class="highlight routeros"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Train Word2Vec on all the restaurants.</span></span><br><span class="line">model = Word2Vec(w2v_corpus, <span class="attribute">workers</span>=3, <span class="attribute">size</span>=100)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Initialize WmdSimilarity.</span></span><br><span class="line"><span class="keyword">from</span> gensim.similarities import WmdSimilarity</span><br><span class="line">num_best = 10</span><br><span class="line">instance = WmdSimilarity(wmd_corpus, model, <span class="attribute">num_best</span>=10)</span><br></pre></td></tr></table></figure>
<p>num_best参数决定了查询返回的结果数量。现在让我们来做个查询。输出是corpus中文档相似度和索引的一个列表，按照相似度排序。</p>
<p>注意当num_best为None（也就是没有指定参数）时输出形式有些不同。在这种情况下，你得到一个涵盖corpus中每个文档的相似度数组。</p>
<p>下面的查询直接取自corpus中的一条评论。让我们看看是否有其他的评论相似于这一条。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">start</span> = <span class="built_in">time</span>()</span><br><span class="line"></span><br><span class="line">sent = <span class="string">'Very good, you should seat outdoor.'</span></span><br><span class="line"><span class="keyword">query</span> = preprocess(sent)</span><br><span class="line"></span><br><span class="line">sims = <span class="keyword">instance</span>[<span class="keyword">query</span>]  <span class="comment"># A query is simply a "look-up" in the similarity class.</span></span><br><span class="line"></span><br><span class="line">print <span class="string">'Cell took %.2f seconds to run.'</span> %(<span class="built_in">time</span>() - <span class="keyword">start</span>)</span><br></pre></td></tr></table></figure>
<p>查询和最相似的文档，以及它们的相似度，在下面被打印出来。我们看到被检索到的文档讨论着和查询一样的事情，尽管使用了不同的单词。查询谈论的是得到一个户外的座位，而结果谈论的是坐在外面，结果中的一个说的是餐馆有一个好的景观。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="comment"># Print the query and the retrieved documents, together with their similarities.</span></span><br><span class="line"><span class="keyword">print</span> <span class="string">'Query:'</span></span><br><span class="line"><span class="keyword">print</span> sent</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(num_best):</span><br><span class="line">    <span class="keyword">print</span></span><br><span class="line">    <span class="keyword">print</span> <span class="string">'sim = %.4f'</span> % sims[i][<span class="number">1</span>]</span><br><span class="line">    <span class="keyword">print</span> documents[sims[i][<span class="number">0</span>]]</span><br></pre></td></tr></table></figure>
<p>让我们尝试一个不同的查询，同样直接取自corpus评论中的一个。</p>
<figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">start</span> = <span class="built_in">time</span>()</span><br><span class="line"></span><br><span class="line">sent = <span class="string">'I felt that the prices were extremely reasonable for the Strip'</span></span><br><span class="line"><span class="keyword">query</span> = preprocess(sent)</span><br><span class="line"></span><br><span class="line">sims = <span class="keyword">instance</span>[<span class="keyword">query</span>]  <span class="comment"># A query is simply a "look-up" in the similarity class.</span></span><br><span class="line"></span><br><span class="line">print <span class="string">'Query:'</span></span><br><span class="line">print sent</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> <span class="keyword">range</span>(num_best):</span><br><span class="line">    print</span><br><span class="line">    print <span class="string">'sim = %.4f'</span> % sims[i][<span class="number">1</span>]</span><br><span class="line">    print documents[sims[i][<span class="number">0</span>]]</span><br><span class="line"></span><br><span class="line">print <span class="string">'\nCell took %.2f seconds to run.'</span> %(<span class="built_in">time</span>() - <span class="keyword">start</span>)</span><br></pre></td></tr></table></figure>
<p>这次，结果更加直接。检索到的文档基本包含了与查询相同的词。</p>
<p>WmdSimilarity默认正则化了词嵌入（使用init_sims()，正如前面解释的），但你可以改变这个行为通过调用WmdSimilarity,并设置normalize_w2v_and_replace=False。</p>
<figure class="highlight applescript"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print 'Notebook took %<span class="number">.2</span>f seconds <span class="keyword">to</span> <span class="built_in">run</span>.' %(<span class="built_in">time</span>() - start_nb)</span><br></pre></td></tr></table></figure>
<h1 id="References"><a href="#References" class="headerlink" title="References"></a>References</h1><ol>
<li>Ofir Pele and Michael Werman, A linear time histogram metric for improved SIFT matching, 2008.</li>
<li>Ofir Pele and Michael Werman, Fast and robust earth mover’s distances, 2009.</li>
<li>Matt Kusner et al. From Embeddings To Document Distances, 2015.</li>
<li>Thomas Mikolov et al. Efficient Estimation of Word Representations in Vector Space, 2013.</li>
</ol>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2017/03/30/machine learning/NLP/idf/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/03/30/machine learning/NLP/idf/" class="post-title-link" itemprop="url">idf</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-03-30 14:59:25" itemprop="dateCreated datePublished" datetime="2017-03-30T14:59:25+08:00">2017-03-30</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/NLP/" itemprop="url" rel="index"><span itemprop="name">NLP</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type"text javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p>
<h1 id="idf-计算技巧"><a href="#idf-计算技巧" class="headerlink" title="idf 计算技巧"></a>idf 计算技巧</h1><p>假设不同词共有N个，文档共有M个，单文档最大长度是L。</p>
<p>我们看如下两种计算方式。</p>
<p>第一种是先作出所有词的词典，然后对每个词进行遍历，看它在多少个文档中出现，复杂度是N×M： </p>
<figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">from collections import Counter</span><br><span class="line"><span class="comment">#word frequency by words</span></span><br><span class="line">w_freq = Counter(<span class="string">' '</span>.join(train_que).<span class="keyword">split</span>())</span><br><span class="line"><span class="comment">#word frequency by docs</span></span><br><span class="line">d_freq = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> i in w_freq.keys():</span><br><span class="line">    <span class="keyword">for</span> j in train_que:</span><br><span class="line">        <span class="keyword">if</span> i in j:</span><br><span class="line">            <span class="keyword">if</span> i <span class="keyword">not</span> in d_freq:</span><br><span class="line">                d_fre<span class="string">q[i]</span> = <span class="number">0</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                d_fre<span class="string">q[i]</span> += <span class="number">1</span></span><br></pre></td></tr></table></figure>
<p>第二种是遍历所有文档，记录每个文档中出现的词，复杂度是M×L：<br><figure class="highlight perl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">from collections import Counter</span><br><span class="line"><span class="comment">#word frequency by words</span></span><br><span class="line">w_freq = Counter(<span class="string">' '</span>.join(train_que).<span class="keyword">split</span>())</span><br><span class="line"><span class="comment">#word frequency by docs</span></span><br><span class="line">d_freq = &#123;&#125;</span><br><span class="line"><span class="keyword">for</span> text in train_que:</span><br><span class="line">    <span class="keyword">for</span> i in set(text.<span class="keyword">split</span>()):</span><br><span class="line">        try:</span><br><span class="line">            d_fre<span class="string">q[i]</span> += <span class="number">1</span></span><br><span class="line">        except:</span><br><span class="line">            d_fre<span class="string">q[i]</span> = <span class="number">0</span></span><br></pre></td></tr></table></figure></p>
<p>显然，两者效率相差极大。一般L不过在10左右，而N却可以高达上万。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2017/03/27/machine learning/competition/trick/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/03/27/machine learning/competition/trick/" class="post-title-link" itemprop="url">trick</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-03-27 21:02:22" itemprop="dateCreated datePublished" datetime="2017-03-27T21:02:22+08:00">2017-03-27</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/competition/" itemprop="url" rel="index"><span itemprop="name">competition</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p>
<h1 id="benchmark"><a href="#benchmark" class="headerlink" title="benchmark"></a>benchmark</h1><p>就0-1分类问题而言，可以将训练集target整体均值作为测试集每个样本的预测概率。从而找到最基本的benchmark。</p>
<p><a href="https://www.kaggle.com/davidthaler/quora-question-pairs/how-many-1-s-are-in-the-public-lb" target="_blank" rel="noopener">而且，还可以根据提交得分反推出1类占比。</a></p>
<p>\begin{split} logloss &amp;&amp;= - \frac{1}{n} \sum y_i \log p + (1 - y_i) \log (1-p) \newline<br>&amp;&amp;= - (r \log p + (1 - r) \log (1 - p)) \newline<br>&amp;&amp;= r \log \frac{1-p}{p} - \log (1 - p)\end{split}<br>由此推得1类占比：<br>\begin{split} r = (logloss + \log (1-p))/ \log \frac{1-p}{p}\end{split}</p>
<h1 id="score"><a href="#score" class="headerlink" title="score"></a>score</h1><p>以logloss得分为例。</p>
<p>假设原训练集不均衡。在建模时通过采样使得训练集均衡，然后对测试集进行预测。通过设定threshold，将预测概率0-1化，计算准确率是没有问题的。</p>
<p>但是，由于采用logloss评分，实际上预测概率会偏大！因为真实的正负样本比例小于0.5！最简单的方法就是将预测结果除以采样后正负样本比例再乘以真实比例！</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2017/03/27/machine learning/optimization/非参方法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/03/27/machine learning/optimization/非参方法/" class="post-title-link" itemprop="url">非参方法</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-03-27 10:32:01" itemprop="dateCreated datePublished" datetime="2017-03-27T10:32:01+08:00">2017-03-27</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/optimization/" itemprop="url" rel="index"><span itemprop="name">optimization</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p>
<h1 id="参数学习方法："><a href="#参数学习方法：" class="headerlink" title="参数学习方法："></a>参数学习方法：</h1><p>假设了一个在整个输入空间上有效的模型，将问题归结为在样本上估计少量参数，(如:线性模型估计w，高斯分布估计mu和sigma).参数学习方法假定了一个模型，当模型假定不成立，或者样本不是一个分组，可能导致很大的误差。(如:语音识别，由于不同口音、性别、年龄、发音等，没有单个同样的模型).</p>
<h1 id="半参数方法："><a href="#半参数方法：" class="headerlink" title="半参数方法："></a>半参数方法：</h1><p>为样本每个分组假定一个参数模型，(如:使用混合分布估计输入样本).</p>
<h1 id="非参数方法："><a href="#非参数方法：" class="headerlink" title="非参数方法："></a>非参数方法：</h1><p>只假定相似输入具有相似输出(如:k近邻)，非参数方法使用合适的聚类度量相似性，对于输入样本，从训练集中找出它们的相似示例(输入样本的邻域)，并由相似的实例插值得到正确的输入。参数模型定义了一个全局模型，所以训练样本都影响最终估计，而非参数方法不存在全局模型，需要时估计局部模型(如:局部加权线性回归),它们只受邻近训练样本影响,是局部响应.因此非参数模型不是固定的，复杂性依赖训练集大小，非参数学习方法又称基于实例或基于记忆的方法，输入样本搜索训练集中相似样本，并基于相似子集插值。</p>
<h1 id="references"><a href="#references" class="headerlink" title="references"></a>references</h1><p><a href="http://blog.csdn.net/u013395544/article/details/53170207" target="_blank" rel="noopener">非参数方法、参数方法与半参数方法</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2017/03/26/machine learning/ensemble/GBM and Xgboost/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/03/26/machine learning/ensemble/GBM and Xgboost/" class="post-title-link" itemprop="url">GBM and Xgboost</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-03-26 22:41:49" itemprop="dateCreated datePublished" datetime="2017-03-26T22:41:49+08:00">2017-03-26</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/ensemble/" itemprop="url" rel="index"><span itemprop="name">ensemble</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type"text javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p>
<h1 id="GBDT概述"><a href="#GBDT概述" class="headerlink" title="GBDT概述"></a>GBDT概述</h1><p>与Adaboost不同，GBM虽然也是加性模型，但它却是通过不断拟合残差来逐步提高模型效果。  </p>
<p><strong><em>需要强调的是，融合模型融合的是函数空间，而不是参数空间！！！*</em></strong></p>
<h1 id="梯度提升"><a href="#梯度提升" class="headerlink" title="梯度提升"></a>梯度提升</h1><p>t轮迭代的损失函数为：<br>\begin{split} J_t = L(y,F_t(x)) = L(y,\sum_{t=0}^t h_t(x))\end{split}</p>
<p>通过梯度下降法优化损失函数：<br>\begin{split} F_{t+1}(x) := F_t(x) - \rho_t \frac{\partial J_t}{\partial F_t(x)} \end{split}<br><strong><em>注意，这里直接优化的是分类器！$\rho$是加法模型的系数（类似于adaboost的系数）,利用牛顿法最小化$J_{t+1}$求得。</em></strong>[TBC]学习率真的是这样??</p>
<p>注意shrinkage与学习率是两个不同概念,它是在得到新拟合出的模型之后通过乘以收缩率去控制过拟合，类似于动量。也就是说，最终应该有：<br>\begin{split} F_{t+1}(x) = (1 - \text{shrinkage})F_t(x) - \text{shrinkage } \rho_t \frac{\partial J_t}{\partial F_t(x)} \end{split}<br>[TBC]？？是每次迭代之后都直接使用shinkage，还是建好模型以后统一使用shrinkage？</p>
<p><strong><em>GBM用的都是回归树！</em></strong> 应该是为了计算方便和更加精确。如果是分类问题，最后再对结果进行转换。</p>
<h1 id="Xgboost"><a href="#Xgboost" class="headerlink" title="Xgboost"></a>Xgboost</h1><p>相较于GBM，引入了L2正则项，将叶节点数和叶节点得分作为罚项显化，并且利用泰勒展开进行优化求解。</p>
<p>利用得分增益进行树的分裂。</p>
<h1 id="references"><a href="#references" class="headerlink" title="references"></a>references</h1><p><a href="http://zkread.com/article/1012129.html" target="_blank" rel="noopener">GBDT（梯度提升决策树）</a></p>
<p><a href="http://www-stat.stanford.edu/~jhf/ftp/trebst.pdf" target="_blank" rel="noopener">Greedy function approximation: a gradient boosting machine</a></p>
<p><a href="http://www.chengli.io/tutorials/gradient_boosting.pdf" target="_blank" rel="noopener">A Gentle Introduction to Gradient Boosting</a></p>
<p><a href="http://xgboost.readthedocs.io/en/latest/model.html" target="_blank" rel="noopener">Xgboost</a></p>
<p><a href="http://blog.csdn.net/shenxiaoming77/article/details/51542982" target="_blank" rel="noopener">一步一步理解GB、GBDT、xgboost</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2017/03/26/machine learning/machine learning/高维稀疏特征建模/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/03/26/machine learning/machine learning/高维稀疏特征建模/" class="post-title-link" itemprop="url">高维稀疏特征建模</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-03-26 16:22:40" itemprop="dateCreated datePublished" datetime="2017-03-26T16:22:40+08:00">2017-03-26</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type"text javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><br>[TBC]为什么对于高维稀疏特征，很多算法无效。比如gbm？</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2017/03/25/machine learning/ensemble/adaboost/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/03/25/machine learning/ensemble/adaboost/" class="post-title-link" itemprop="url">adaboost</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-03-25 20:13:44" itemprop="dateCreated datePublished" datetime="2017-03-25T20:13:44+08:00">2017-03-25</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/ensemble/" itemprop="url" rel="index"><span itemprop="name">ensemble</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type"text javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p>
<h1 id="adaboost原理"><a href="#adaboost原理" class="headerlink" title="adaboost原理"></a>adaboost原理</h1><p>adaboost通过集成弱分类器来最终实现一个强分类器。</p>
<p>以下的讨论局限于二分类问题。并且注意正类标记为1,负类标记为-1。</p>
<p>弱分类器：优于随机水平。事实上，这很容易达到，因为对于低于随机水平的，只需要反转预测的label就可以高于随机水平了。</p>
<p>adaboost是一个加法模型，最终模型形如：<br>$$F(x) = \sum_{t=1}^T \alpha_t h_t(x)$$<br>$$H(x) = sign[F(x)]$$<br>其中，T是总迭代次数。$\alpha$是模型的权重，也就是需要求解的模型参数之一。另一部分参数就是各个基分类器。</p>
<p>我们的优化目标自然是最小化误差：<br>$$\arg \min Err(H;D) = \arg \min \frac{1}{m} \sum_{i=1}^m I(h(x_i) \neq y_i)$$<br>其中，m是样本数。</p>
<h1 id="权重调整"><a href="#权重调整" class="headerlink" title="权重调整"></a>权重调整</h1><p>adaboost通过调整误分类样本权重重新建模，来改进模型效果。<br>初始权重为$\frac{1}{m}$。</p>
<p>通过如下公式进行权重调整：<br>$$D_{t+1}(i) = \frac{1}{Z_t} D_t(i) \exp[-\alpha_t y_i h_t(x_i)]$$<br>其中，$Z_t$是规范化因子，保证权重大于等于0,和为1。$D_t(i)$是t轮迭代的样本权重。显然，当预测正确时，$y_i h_t(x_i) = 1$，新权重会减小;预测错误时，$y_i h_t(x_i) = -1$，新权重会增大。</p>
<p>因此，可以得到递推公式：<br>\begin{split} D_{t+1}(i) &amp;&amp;= \frac{1}{Z_t} D_t(i) \exp[-\alpha_t y_i h_t(x_i)] \newline<br>&amp;&amp;= \frac{1}{Z_t Z_{t-1}} D_{t-1}(i) \exp[-y_i(\alpha_t h_t(x_i) + \alpha_{t-1} h_{t-1}(x_i))] \newline<br>&amp;&amp;= \frac{1}{Z_t … Z_{1}} D_{1}(i) \exp[-y_i(\alpha_t h_t(x_i) + …+\alpha_{1} h_{1}(x_i))] \newline<br>&amp;&amp;= \frac{1}{Z_t … Z_{1}} D_{1}(i) \exp[-y_i F(x_i)] \end{split}</p>
<p>左右两边同时求和有：<br>\begin{split} 1 = \sum_{i=1}^m D_{t+1}(i) = \frac{1}{Z_t … Z_{1}} \frac{1}{m} \sum_{i=1}^m \exp[-y_i F(x_i)] \end{split}<br>\begin{split} Z = Z_t … Z_{1} = \frac{1}{m} \sum_{i=1}^m \exp[-y_i F(x_i)] \end{split}</p>
<h1 id="adaboost误差上界"><a href="#adaboost误差上界" class="headerlink" title="adaboost误差上界"></a>adaboost误差上界</h1><p>adaboost通过优化误差上界来优化模型效果。</p>
<p>下面证明：<br>$$ Err(H) = \frac{1}{m} \sum_{i=1}^m I(h(x_i) \neq y_i) \leq Z = \frac{1}{m} \sum_{i=1}^m \exp[-y_i F(x_i)]$$</p>
<p>证明如下：</p>
<p>对于求和的每一项有：<br>\begin{split} \text{If } H(x_i) \neq y_i \text{ then the LHS } = 1 \leq \text{RHS } = e^{+|F(x_i)|} \newline<br>\text{If } H(x_i) = y_i \text{ then the LHS } = 0 \leq \text{RHS } = e^{-|F(x_i)|} \end{split}<br>显然求和后不等式依旧成立。故而得证。</p>
<p>因此，我们的问题转化为优化Z。同时，我们采用step-wise的方法去进行优化，也就是逐步优化$Z_1,Z_2,Z_3,…,Z_t$。</p>
<h1 id="权重系数求解"><a href="#权重系数求解" class="headerlink" title="权重系数求解"></a>权重系数求解</h1><p>对$Z_t$最小化，令导数为0。</p>
<p>\begin{split} Z_t(\alpha_t,h_t) &amp;&amp;= \sum_{x_i \in A} D_t(x_i) exp[-\alpha_t] + \sum_{x_i \in \bar{A}} D_t(x_i) exp[-\alpha_t] \newline<br>\frac{d Z_t(\alpha_t,h_t)}{d \alpha_t} &amp;&amp;= \sum_{x_i \in A} -D_t(x_i) exp[-\alpha_t] + \sum_{x_i \in \bar{A}} D_t(x_i) exp[-\alpha_t] = 0 \newline<br>\sum_{x_i \in A} D_t(x_i) &amp;&amp;= \sum_{x_i \in \bar{A}} D_t(x_i) exp[2 \alpha_t] \end{split}</p>
<p>我们定义加权误差：<br>\begin{split} \epsilon_t(h) = \sum_{i=1}^m D_t(x_i)I(h(x_i) \neq y_i) = \sum_{x_i \in \bar{A}} D_t(x_i) \end{split}</p>
<p>因此有：<br>\begin{split} \alpha_t = \frac{1}{2} \ln \frac{1-\epsilon_t(h_t)}{\epsilon_t(h_t)} \end{split}</p>
<p><strong>由于每个分类器都要求是弱分类器。也就是要求每个基分类器的 <em>加权误差率</em> 小于0.5。</strong><br>因此，$\alpha_t$显然都大于0。</p>
<h1 id="误差分析"><a href="#误差分析" class="headerlink" title="误差分析"></a>误差分析</h1><p>将解出的权重系数带回：</p>
<p>\begin{split} Z_t(\alpha_t,h_t) &amp;&amp;= \sum_{x_i \in A} D_t(x_i) exp[-\alpha_t] + \sum_{x_i \in \bar{A}} D_t(x_i) exp[-\alpha_t] \newline<br>&amp;&amp;= (1-\epsilon_t(h_t))\sqrt{\frac{\epsilon_t(h_t)}{1-\epsilon_t(h_t)}} + \epsilon_t(h_t)\sqrt{\frac{1-\epsilon_t(h_t)}{\epsilon_t(h_t)}} \newline<br>&amp;&amp;= 2\sqrt{\epsilon_t(h_t) (1-\epsilon_t(h_t))}\end{split}</p>
<p>令:<br>\begin{split} \gamma_t = \frac{1}{2} - \epsilon_t(h_t) , \gamma_t \in (0,\frac{1}{2}]\end{split}</p>
<p>则有：</p>
<p>\begin{split} Z_t(\alpha_t,h_t) &amp;&amp;= 2\sqrt{\epsilon_t(h_t) (1-\epsilon_t(h_t))} \newline<br>&amp;&amp;= \sqrt{1-4\gamma_t^2} \newline<br>&amp;&amp; \leq \exp[-2 \gamma_t^2]\end{split}</p>
<p>因此：</p>
<p>\begin{split} Err(H) \leq Z \leq \exp[-2\sum_{t=1}^T \gamma_t^2] \end{split}</p>
<p>随着迭代次数增加，误差上界以指数级减小。</p>
<h1 id="过拟合问题"><a href="#过拟合问题" class="headerlink" title="过拟合问题"></a>过拟合问题</h1><p>adaboost在使用中常常不容易过拟合。也就是说验证集可以随着训练集精度提升而提升，并不存在一个下降的拐点。</p>
<p>一些相关的理论解释：</p>
<blockquote>
<p>作者：知乎用户<br>链接：<a href="https://www.zhihu.com/question/41047671/answer/127832345" target="_blank" rel="noopener">https://www.zhihu.com/question/41047671/answer/127832345</a><br>来源：知乎<br>著作权归作者所有。商业转载请联系作者获得授权，非商业转载请注明出处。<br>AdaBoost提出的论文对AdaBoost的泛化界进行了分析，使用了通常的学习器泛化界：<br>泛化错误（泛化错误可理解为测试错误） &lt; 训练错误 + 学习算法容量相关项     (1)<br>由(1)式可见，当训练错误不变时，应当选择简单的学习模型，从而减少学习算法容量。然而在实验中已经观察到，在一些情况，训练错误已经是0了，继续训练还能进一步减小泛化错误，这里的继续训练意味着增加学习算法容量，理应导致泛化错误上升才对。因此(1)直接套上AdaBoost是解释不通的。更有JMLR 2008的论文发现，更多的实验观察与Boosting的理论及其统计解释都不符合，例如使用多层的决策树竟然比使用一层的简单决策树更好。理论不能解释实验肯定是理论的问题，于是人们觉得，(1)式不够好，可能把更加细微的因素忽略了。<br>于是AdaBoost的作者针对这一情况，提出了新的学习器泛化界：<br>泛化错误 &lt; 训练Margin项 + 学习算法容量相关项     (2)<br>(1)中的训练错误是指一个数据样本分类对了就是0，错了就是1，而(2)里的“训练Margin”不仅看分类对错，还要看对的信心有多少，例如对于一个正类+1，分类器A输出+0.1，B输出+2，虽然都分类正确了，但B的信心更多。这样(2)就比(1)更加细致的刻画了学习器的表现。实验一看，果然AdaBoost在训练错误为0后，继续训练不能再减少训练错误了，确能够进一步减少训练Margin，也就是信心更足了。<br>看似解决了AdaBoost不容易过拟合的问题，然而好景不长，统计大牛Leo Breiman（bagging，random forest出自他手）来了个比 (2) 更紧的界（更紧就是更接近真实）：<br>泛化错误 &lt; 训练Margin的最小值 + 学习算法容量相关项     (3)<br>（3）的容量相关项目更小，但这不是关键，关键是根据这一理论，就应该去看训练Margin的最小值，也就是分类器在所有样本上信心最不足的那个。然而根据这一理论，Breiman设计了另一种Boosting算法arc-gv，最小训练Margin更小了，但实验效果比AdaBoost差了很多。于是乎Breiman的结论是，这个用训练Margin来刻画泛化错误整个就是不对的。大家都傻眼了，AdaBoost不容易过拟合的问题无解了。<br>7年之后。。。AdaBoost作者之一的工作发现，Breiman的实验竟然有问题：没有很好的控制决策树的复杂性，也就是说，AdaBoost和arc-gv关于“学习算法容量相关项”的值并不一样，虽然arc-gv的最小训练Margin更小，但后面一项更大啊，因此泛化错误就更大了。于是重做实验，都用一层决策树，这样后面一项都一样了，一看AdaBoost更好了，也就是说原来的Margin理论并没有错误，松了口气。该论文获ICML’06最佳论文奖。<br>但是这篇最佳论文奖并没有终结问题，当都用一层决策树时，AdaBoost的最小训练Margin比arc-gv还要小，也就是说，并没有否定(3)式。然而在实验中，最小化这个最小Margin的效果并不好。这篇论文也指出，可能要看Margin的分布，也就是算法在所有样本上的信心，而不是最差的那个样本上的信心。但这只是“可能”，理论研究者最求的是更紧的界。接下来都是我国研究者的贡献了，北京大学王立威等人的到了比(3)更紧的界，其中“训练Margin的最小值” 被替代为 “训练Margin的某一个值”，这某一个要解一个均衡式，但不管怎么说，最小Margin被替代掉了。南京大学的高尉和周志华教授推导出了“第k Margin界”，（3）和 “训练Margin的某一个值” 都是其k赋特定值的特例，并由此得到了基于“算法在所有样本上的信心”的界，比（3）式更好。<br>以上内容可见：<a href="https://wenku.baidu.com/view/8efc9b880975f46527d3e1cb.html" target="_blank" rel="noopener">CCL2014_keynote-周志华</a><br>Margin理论讨论的主要是学习算法在训练样本上的信心，学习算法的容量是不是随着训练轮数的增加而增加呢，其实并不一定，近来有工作表明，有差异的学习器的组合，能够起到正则化的作用，也就是减少学习算法容量（<a href="http://lamda.nju.edu.cn/yuy/GetFile.aspx?File=papers/ecml12-divprune.pdf" target="_blank" rel="noopener">Diversity regularized ensemble pruning. ECML’12</a>; <a href="https://arxiv.org/abs/1511.07110" target="_blank" rel="noopener">On the Generalization Error Bounds of Neural Networks under Diversity-Inducing Mutual Angular Regularization</a>）。在许多variance-bias 分解实验中也观察到，AdaBoost不仅是减少了bias，同时也减少了variance，variance的减少往往与算法容量减少有关。<br>总之这一方面还值得进一步探索，新原理的发型，有可能导致新型高效学习算法的发明，意义重大。</p>
</blockquote>
<h1 id="references"><a href="#references" class="headerlink" title="references"></a>references</h1><p><a href="https://www.cse.buffalo.edu/~jcorso/t/CSE555/files/lecture_boosting.pdf" target="_blank" rel="noopener">boosting and adaboost</a></p>
<p><a href="https://www.zhihu.com/question/41047671" target="_blank" rel="noopener">adaboost为什么不容易过拟合呢？</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2017/03/24/machine learning/machine learning/线性判别分析/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/03/24/machine learning/machine learning/线性判别分析/" class="post-title-link" itemprop="url">线性判别分析</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-03-24 15:32:44" itemprop="dateCreated datePublished" datetime="2017-03-24T15:32:44+08:00">2017-03-24</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p>
<h1 id="LDA思想"><a href="#LDA思想" class="headerlink" title="LDA思想"></a>LDA思想</h1><p>将训练样例投影到一条直线上，使得同类样例尽可能近，异类样例尽可能远。在对新样例进行分类时，将其投影到同样的这条直线上，在根据投影点位置来确定样例类别。</p>
<p><img src="http://i1.piimg.com/588926/e3666f601ae4b470.png" alt><br>其中，$\omega$是直线方向向量，x是样例向量，两者内积就是样例在直线上投影。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2017/03/24/machine learning/NN/《神经网络和深度学习》第五章——深度神经网络为何很难训练/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/03/24/machine learning/NN/《神经网络和深度学习》第五章——深度神经网络为何很难训练/" class="post-title-link" itemprop="url">《神经网络和深度学习》第五章——深度神经网络为何很难训练</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-03-24 09:56:27" itemprop="dateCreated datePublished" datetime="2017-03-24T09:56:27+08:00">2017-03-24</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/NN/" itemprop="url" rel="index"><span itemprop="name">NN</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p>
<h1 id="不稳定的梯度问题"><a href="#不稳定的梯度问题" class="headerlink" title="不稳定的梯度问题"></a>不稳定的梯度问题</h1><p><img src="http://p1.bpimg.com/567571/c337283e1a426f2f.png" alt>  </p>
<p>以上图为例，会出现：</p>
<ol>
<li><p><strong>消失的梯度问题</strong>：<br>当$\omega_i\sigma_i^{‘}$小于1时。由于一般初始权重在0附近，且使用sigmoid神经元，所以消失的梯度问题更容易出现。<br>其实，即使权重很大，对于sigmoid函数而言，越大的权重会导致对应的sigmoid函数倒数越接近0,因此更容易梯度消失。</p>
</li>
<li><p><strong>激增的梯度问题</strong>：<br>当$\omega_i\sigma_i^{‘}$大于1时。当初始权重很大时出现。</p>
</li>
</ol>
<p>根本问题是因为前面层上的梯度是来自后面层上项的乘积。当存在过多的层次时，就出现了内在本质上的不稳定场景。<br>所以，如果我们使用标准的基于梯度的学习算法，在网络中的不同层就会出现按照不同学习速度学习的情况。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2017/03/24/machine learning/NN/《神经网络和深度学习》第六章——深度学习/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/03/24/machine learning/NN/《神经网络和深度学习》第六章——深度学习/" class="post-title-link" itemprop="url">《神经网络和深度学习》第六章——深度学习</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-03-24 09:56:23" itemprop="dateCreated datePublished" datetime="2017-03-24T09:56:23+08:00">2017-03-24</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/NN/" itemprop="url" rel="index"><span itemprop="name">NN</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p>
<h1 id="CNN"><a href="#CNN" class="headerlink" title="CNN"></a>CNN</h1><p>充分利用空间特征。</p>
<ul>
<li><p>局部感受野<br>隐藏层中每个神经元链接到输入神经元的一个小区域。</p>
</li>
<li><p>共享权重和偏置<br>即是应用相同的特征检测器。因此也可以很好的适应图像的平移不变性。<br>我们有时候把从输入层到隐藏层的映射称为一个特征映射。我们把定义特征映射的权重称为共享权重。我们把以这种方式定义特征映射的偏置称为共享偏置。<br>共享权重和偏置经常被称为一个卷积核或者滤波器。</p>
</li>
<li><p>混合层（pooling layers）<br>混合层通常紧接着在卷积层之后使用。用于简化从卷积层输出的信息。</p>
</li>
</ul>
<p>一些常用的混合程序：</p>
<ul>
<li>最大值混合（max-pooling）：取最大值</li>
<li>L2混合（L2-pooling）：取激活值平方和的平方根</li>
</ul>
<p>一个使用5×5局部感受野和3个特征映射，以及应用于2×2区域的最大值混合层如下所示：</p>
<p><img src="http://i4.buimg.com/567571/3dae256ad7f99167.png" alt></p>
<p>[TBC]卷积网络中的反向传播？</p>
<h1 id="CNN实践"><a href="#CNN实践" class="headerlink" title="CNN实践"></a>CNN实践</h1><ul>
<li><p>加入第二个卷积-混合层<br>地一个卷积-混合层的输出是经过抽象和凝缩过的版本，但仍然有大量的空间结构。因此使用第二个卷积-混合层是有意义的。<br>以上例来说，输出是3×12×12,那么第二个卷积层的局部感受野应该是3×k×k。也就是说使用前面所有混合层数据。这是一个需要注意的地方。</p>
</li>
<li><p>切换激活函数<br>tanh，relu等。</p>
</li>
<li><p>扩展训练数据</p>
</li>
<li><p>插入额外的全连接层</p>
</li>
<li><p>使用组合网络<br>训练多个网络进行投票</p>
</li>
<li><p>对全连接层进行dropout<br>卷积层先天可以抵抗过拟合。原因是共享权重意味着卷积滤波器被强制从整个图像中学习。这使他们不太可能去选择在训练数据中的局部特质。于是就很少有必要来应用其他规范化，例如弃权。</p>
</li>
</ul>
<h1 id="为什么我们能够训练"><a href="#为什么我们能够训练" class="headerlink" title="为什么我们能够训练"></a>为什么我们能够训练</h1><p>我们是如何克服不稳定的梯度问题的？</p>
<ul>
<li>使用卷积层极大的减少了这些层中的参数的数目，使学习的问题更容易</li>
<li>使用更多强有力的规范化技术（尤其是弃权和卷积层）来减少过度拟合</li>
<li>使用修正线性单元而不是S型神经元，来加速训练 [TBC]?</li>
<li>使用GPU并愿意长时间训练</li>
</ul>
<p>以及其他技巧：</p>
<ul>
<li>使用充分大的数据集（避免过拟合）</li>
<li>使用正确的代价函数（避免学习减速）</li>
<li>使用好的权重初始化（避免神经元饱和和其引起的学习减速）</li>
</ul>
<h1 id="TBC-一些问题"><a href="#TBC-一些问题" class="headerlink" title="[TBC]一些问题"></a>[TBC]一些问题</h1><ul>
<li><p>对relu使用sigmoid函数的初始化方法会怎么样？</p>
</li>
<li><p>前面不稳定的梯度问题是针对sigmoid函数讨论的，对relu，会有什么差异？</p>
</li>
</ul>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/13/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><span class="page-number current">14</span><a class="page-number" href="/page/15/">15</a><span class="space">&hellip;</span><a class="page-number" href="/page/18/">18</a><a class="extend next" rel="next" href="/page/15/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">muzhen</p>
              <div class="site-description motion-element" itemprop="description"></div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">358</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">37</span>
                    <span class="site-state-item-name">categories</span>
                  
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">113</span>
                    <span class="site-state-item-name">tags</span>
                  
                </div>
              
            </nav>
          

          

          

          

          
          

          
            
          
          

        </div>
      </div>

      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">muzhen</span>

  

  
</div>


  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.0.1</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=7.0.1"></script>

  <script src="/js/src/motion.js?v=7.0.1"></script>



  
  


  <script src="/js/src/schemes/muse.js?v=7.0.1"></script>



  

  


  <script src="/js/src/next-boot.js?v=7.0.1"></script>


  
  



  




  

  

  
  

  
  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  


  

  

  

  

  

  

  

  

  

  

</body>
</html>
