<!DOCTYPE html>












  


<html class="theme-next muse use-motion" lang="en">
<head><meta name="generator" content="Hexo 3.8.0">
  <meta charset="UTF-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">


























<link rel="stylesheet" href="/lib/font-awesome/css/font-awesome.min.css?v=4.6.2">

<link rel="stylesheet" href="/css/main.css?v=7.0.1">


  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png?v=7.0.1">


  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png?v=7.0.1">


  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png?v=7.0.1">


  <link rel="mask-icon" href="/images/logo.svg?v=7.0.1" color="#222">







<script id="hexo.configurations">
  var NexT = window.NexT || {};
  var CONFIG = {
    root: '/',
    scheme: 'Muse',
    version: '7.0.1',
    sidebar: {"position":"left","display":"post","offset":12,"b2t":false,"scrollpercent":false,"onmobile":false,"dimmer":false},
    fancybox: false,
    fastclick: false,
    lazyload: false,
    tabs: true,
    motion: {"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}},
    algolia: {
      applicationID: '',
      apiKey: '',
      indexName: '',
      hits: {"per_page":10},
      labels: {"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}
    }
  };
</script>


  




  <meta property="og:type" content="website">
<meta property="og:title" content="the Home of MuZhen">
<meta property="og:url" content="http://www.muzhen.tk/page/15/index.html">
<meta property="og:site_name" content="the Home of MuZhen">
<meta property="og:locale" content="en">
<meta name="twitter:card" content="summary">
<meta name="twitter:title" content="the Home of MuZhen">






  <link rel="canonical" href="http://www.muzhen.tk/page/15/">



<script id="page.configurations">
  CONFIG.page = {
    sidebar: "",
  };
</script>

  <title>the Home of MuZhen</title>
  












  <noscript>
  <style>
  .use-motion .motion-element,
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-title { opacity: initial; }

  .use-motion .logo,
  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage" lang="en">

  
  
    
  

  <div class="container sidebar-position-left 
  page-home">
    <div class="headband"></div>

    <header id="header" class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-wrapper">
  <div class="site-meta">
    

    <div class="custom-logo-site-title">
      <a href="/" class="brand" rel="start">
        <span class="logo-line-before"><i></i></span>
        <span class="site-title">the Home of MuZhen</span>
        <span class="logo-line-after"><i></i></span>
      </a>
    </div>
    
    
  </div>

  <div class="site-nav-toggle">
    <button aria-label="Toggle navigation bar">
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
      <span class="btn-bar"></span>
    </button>
  </div>
</div>



<nav class="site-nav">
  
    <ul id="menu" class="menu">
      
        
        
        
          
          <li class="menu-item menu-item-home">

    
    
    
      
    

    

    <a href="/" rel="section"><i class="menu-item-icon fa fa-fw fa-home"></i> <br>Home</a>

  </li>
        
        
        
          
          <li class="menu-item menu-item-archives">

    
    
    
      
    

    

    <a href="/archives/" rel="section"><i class="menu-item-icon fa fa-fw fa-archive"></i> <br>Archives</a>

  </li>

      
      
    </ul>
  

  
    

  

  
</nav>



  



</div>
    </header>

    


    <main id="main" class="main">
      <div class="main-inner">
        <div class="content-wrap">
          
          <div id="content" class="content">
            
  <section id="posts" class="posts-expand">
    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2017/03/23/machine learning/machine learning/分类和回归/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/03/23/machine learning/machine learning/分类和回归/" class="post-title-link" itemprop="url">分类和回归</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-03-23 16:55:41" itemprop="dateCreated datePublished" datetime="2017-03-23T16:55:41+08:00">2017-03-23</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p>
<h1 id="分类和回归区别"><a href="#分类和回归区别" class="headerlink" title="分类和回归区别"></a>分类和回归区别</h1><ul>
<li><p>从输出上说，分类输出是离散值，回归输出是连续值。</p>
</li>
<li><p>从预测过程上说，分类是找寻分离超平面，回归是找寻一个超平面使得目标值是样本点到超平面距离？<br>显然不能如此理解，因为线性回归就不符合。</p>
</li>
</ul>
<h1 id="模型比较"><a href="#模型比较" class="headerlink" title="模型比较"></a>模型比较</h1><ul>
<li><p>CART<br>作为回归树，分类结果是叶节点y值均值。<br>作为分类树，分类结果是叶节点类别众数。  </p>
</li>
<li><p>线性回归<br>对于经典线性回归，是线性拟合结果。<br>对于logistic回归，是线性拟合结果的S型变换。</p>
</li>
<li><p>支持向量机<br>[TBC]SVR。<br>SVM则是将该距离使用符号函数改为分类结果。</p>
</li>
</ul>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2017/03/23/machine learning/optimization/EM算法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/03/23/machine learning/optimization/EM算法/" class="post-title-link" itemprop="url">EM算法</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-03-23 16:51:54" itemprop="dateCreated datePublished" datetime="2017-03-23T16:51:54+08:00">2017-03-23</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/optimization/" itemprop="url" rel="index"><span itemprop="name">optimization</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p>
<h1 id="实际问题"><a href="#实际问题" class="headerlink" title="实际问题"></a>实际问题</h1><p>假设我们有一个样本集$\{x_1,x_2,…,x_n\}$，各样本独立。但是，各样本对应的分布$z_i$未知。换言之，该样本集并不是从同一分布中取出，而是从多个分布中的某一个取出。$z$ 是隐含变量，表示服从哪个分布。<br>在此种情况下，使用极大似然法，求使得似然概率最大的参数$\theta$（没有先验，也就谈不上后验概率）。</p>
<h1 id="极大似然法"><a href="#极大似然法" class="headerlink" title="极大似然法"></a>极大似然法</h1><p>依旧希望似然概率最大，即：<br>$$\arg \max_\theta \sum_{x_i} \log p(x_i;\theta)$$</p>
<blockquote>
<p><strong>由于无法直接求出似然函数解析解，EM算法通过不断寻找似然函数的紧密下界，然后寻找使下界最优的$\theta$（由于紧密，往往也可以优化似然函数），如此不断迭代至似然函数最优。正是因为这个想法，才会想到使用Jensen不等式去找寻下界。</strong></p>
</blockquote>
<p>其中,根据Jensen不等式可推得：<br>\begin{eqnarray} l(\theta) = \sum_{x_i} \log p(x_i;\theta) &amp;&amp; = \sum_{x_i} \log \sum_{z_j} p(x_i,z_j;\theta)  \tag{1} \newline<br>&amp;&amp; = \sum_{x_i} \log \sum_{z_j} p(z_j) \frac{p(x_i,z_j;\theta)}{p(z_j)} \tag{2} \newline<br>&amp;&amp; \geq \sum_{x_i} \sum_{z_j} p(z_j) \log \frac{p(x_i,z_j;\theta)}{p(z_j)} \tag{3}\end{eqnarray}</p>
<p>因此，可以通过不断优化（3）式来不断提高似然函数下界，最终使之收敛到最优$\theta$。  </p>
<blockquote>
<p>这里需要说明，$p(z)$是为了数学优化目的而出现的一个概率分布列，它和隐变量分布的参数并不等同。事实上，它是给定参数(包括隐变量参数和各分布参数)和样本情况下的后验概率。</p>
</blockquote>
<h1 id="EM算法"><a href="#EM算法" class="headerlink" title="EM算法"></a>EM算法</h1><p><img src="http://p1.bqimg.com/519918/5b876e7671bd61e7.png" alt><br>(图中Q(z)即指p(z))</p>
<blockquote>
<ol>
<li>首先固定$\theta$，寻找合适的$p(z)$，使（3）成为似然函数的合宜下界。显然，在（1）=（3）时最为合宜。因为合宜，故而在此时，我们可以理解为（3）和（1）有相近的性质;</li>
<li>然后固定求得的$p(z)$,求使得（3）式达到最大值的$\theta$。因为（3）和（1）有相近的性质，因此使得（3）式达到最大值的$\theta$应该可以让似然函数变大;</li>
<li>不断迭代最终得到最优解。<br>（严格证明见下一部分）</li>
</ol>
</blockquote>
<p>由于$\log x$严格凹，$\lambda_i &gt; 0$,当且仅当自变量为常数时取等号:<br>\begin{split}&amp;&amp; \sum_i \log \lambda_i x_i \geq \sum_i \lambda_i \log x_i \newline<br>&amp;&amp; \text{equality is true,if and only if } \quad x_i = c,i=1,2,3,…<br>\end{split}</p>
<p>因此，当（1）=（3）时：<br>\begin{split}\frac{p(x_i,z_j;\theta)}{p(z_j)} = c,i=1,2,3,…\end{split}<br>由于多个等式分子分母相加不变：<br>\begin{split}c = \frac{\sum_j p(x_i,z_j;\theta)}{\sum_j p(z_j)} = \sum_j p(x_i,z_j;\theta) = p(x_i;\theta)\end{split}<br>因此：<br>\begin{eqnarray} p(z_j) &amp;&amp;= \frac{p(x_i,z_j;\theta)}{c} \tag{4} \newline<br>&amp;&amp;= \frac{p(x_i,z_j;\theta)}{\sum_j p(x_i,z_j;\theta)} \tag{5} \newline<br>&amp;&amp;= \frac{p(x_i,z_j;\theta)}{p(x_i;\theta)}  \tag{6} \newline<br>&amp;&amp;= p(z_j|x_i;\theta) \tag{7} \end{eqnarray}</p>
<p>另一方面：<br>\begin{eqnarray} p(x_i;\theta) = \sum_j p(x_i,z_j;\theta) = \sum_j p(z_j;\theta) p(x_i|z_j;\theta) \tag{8} \end{eqnarray}</p>
<blockquote>
<p>这里需要作出一些重要说明，我所看到的部分EM讲解（references中已列出部分）将z的角标也用成i，我在这里区分为j。同时，它们往往给出（7）式就结束，并没有明确说明如何求出p(z)。我在这里说明下我对其求法的理解:</p>
<ol>
<li>$p(z)$一般是多项分布的概率分布列，z取值个数已知（[TBC]如果z是连续值呢？）;</li>
<li>$p(z_j)$应该是通过（6）式即贝叶斯公式求得。其中分母利用（8）式求得;</li>
<li>显见的，对于每一个$x_i$，都可以求出整体$p(z_j),j=1,2,3,…$,标记为$p_{x_i}(z_j)$。故而，最终应该有:<br>\begin{eqnarray} p(z_j) = \sum_{x_i} p_{x_i}(z_j),j = 1,2,3,… \tag{9}\end{eqnarray}<br>正是因为（9），E步叫做求期望。（我是这样理解的～但貌似不对？） </li>
</ol>
</blockquote>
<p>EM算法整体步骤：  </p>
<ol>
<li>给定参数初值(包括隐变量参数和各分布参数)，</li>
<li>（E步，求期望）根据上一步给定的$\theta$,计算后验概率$p(z)$  </li>
<li>（M步，最大化）根据计算出的$p(z)$，带回（3）式，求使之最大的$\theta$<br>循环往复，直至收敛。</li>
</ol>
<p>期望-最大算法不是指最大化期望！只是两个步骤而已。</p>
<h1 id="收敛性证明"><a href="#收敛性证明" class="headerlink" title="收敛性证明"></a>收敛性证明</h1><p>首先明确顺序问题：初始状态，给定初始$\theta^{(1)}$和$p^{(0)}(z)$;第一次迭代，E步求出$p^{(1)}(z)$，然后M步求出$\theta^{(2)}$;第t次迭代，给定$\theta^{(t)}$，E步求出$p^{(t)}(z)$，然后M步求出$\theta^{(t+1)}$。</p>
<p>那么，因为在t时刻是根据（1）=（3）求出$p^{(t)}(z)$，那么有：<br>$$ l(\theta^{(t)}) = \sum_{x_i} \sum_{z_j} p^{(t)}(z_j) \log \frac{p(x_i,z_j;\theta^{(t)})}{p^{(t)}(z_j)}$$<br>而$\theta^{(t+1)}$是对上式右边求最大值得到的，因此显然有：<br>$$\sum_{x_i} \sum_{z_j} p^{(t)}(z_j) \log \frac{p(x_i,z_j;\theta^{(t+1)})}{p^{(t)}(z_j)} \geq \sum_{x_i} \sum_{z_j} p^{(t)}(z_j) \log \frac{p(x_i,z_j;\theta^{(t)})}{p^{(t)}(z_j)} = l(\theta^{(t)})$$<br>另一方面：<br>\begin{eqnarray} l(\theta^{(t+1)}) = \sum_{x_i} \log p(x_i;\theta^{(t+1)}) &amp;&amp; = \sum_{x_i} \log \sum_{z_j} p(x_i,z_j;\theta^{(t+1)}) \newline<br>&amp;&amp; = \sum_{x_i} \log \sum_{z_j} p(z_j) \frac{p(x_i,z_j;\theta^{(t+1)})}{p(z_j)} \newline<br>&amp;&amp; \geq \sum_{x_i} \sum_{z_j} p(z_j) \log \frac{p(x_i,z_j;\theta^{(t+1)})}{p(z_j)} \newline<br>&amp;&amp; \geq \sum_{x_i} \sum_{z_j} p^{(t)}(z_j) \log \frac{p(x_i,z_j;\theta^{(t)})}{p^{(t)}(z_j)} \newline<br>&amp;&amp; = l(\theta^{(t)})\end{eqnarray}<br>如此就可以证明迭代过程中似然函数的单调增性。单调有界故而收敛。<br>似然函数是凸函数才能收敛到全局最优值。</p>
<h1 id="实例：三硬币模型"><a href="#实例：三硬币模型" class="headerlink" title="实例：三硬币模型"></a>实例：三硬币模型</h1><p>参 李航《统计学习方法——EM算法》</p>
<h1 id="PRML角度"><a href="#PRML角度" class="headerlink" title="PRML角度"></a>PRML角度</h1><p>[TBC]PRML角度<br><a href="https://mqshen.gitbooks.io/prml/Chapter9/general_em.html?q=" target="_blank" rel="noopener">PRML EM</a></p>
<h1 id="EM-amp-K-means"><a href="#EM-amp-K-means" class="headerlink" title="EM &amp; K-means"></a>EM &amp; K-means</h1><p>K-means的损失函数可以写成：<br>$$J(C,\mu) = \sum_i || x_i - \mu_C||^2$$<br>其中，C是类分配，$\mu$是类重心。因此可以将K-means视为坐标上升或者EM算法。</p>
<h1 id="高斯混合模型"><a href="#高斯混合模型" class="headerlink" title="高斯混合模型"></a>高斯混合模型</h1><p>[TBC]</p>
<h1 id="references"><a href="#references" class="headerlink" title="references"></a>references</h1><p><a href="http://blog.csdn.net/zouxy09/article/details/8537620" target="_blank" rel="noopener">从最大似然到EM算法浅解</a></p>
<p><a href="http://www.cnblogs.com/jerrylead/archive/2011/04/06/2006936.html" target="_blank" rel="noopener">The EM Algorithm</a></p>
<p>李航《统计学习方法》</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2017/03/23/development/datacenter/Detecting Large-Scale System Problems by Mining Console Logs/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/03/23/development/datacenter/Detecting Large-Scale System Problems by Mining Console Logs/" class="post-title-link" itemprop="url">Detecting Large-Scale System Problems by Mining Console Logs</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-03-23 14:49:20" itemprop="dateCreated datePublished" datetime="2017-03-23T14:49:20+08:00">2017-03-23</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/datacenter/" itemprop="url" rel="index"><span itemprop="name">datacenter</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p>
<h1 id="原文链接"><a href="#原文链接" class="headerlink" title="原文链接"></a>原文链接</h1><p><a href="http://iiis.tsinghua.edu.cn/~weixu/files/sosp09.pdf" target="_blank" rel="noopener">Detecting Large-Scale System Problems by Mining Console Logs</a></p>
<h1 id="摘要"><a href="#摘要" class="headerlink" title="摘要"></a>摘要</h1><p>console log是被独立开发者所写的许多软件反馈信息的混合。惊奇的是，log很少能够帮助操作者侦测大规模数据中心服务器的问题。我们提出了一个一般方法论去通过这里丰富的信息源去自动侦测系统运行问题。<br>首先，我们结合源码分析和信息检索去解析log，以此来创造合成特征。然后，我们使用机器学习去分析这些特征来侦测系统问题。由于我们的方法具有较高的创造合成特征的能力，因此它可以分析出那些先前方法分析不出的问题。<br>我们同样展示了怎样提炼我们的分析结果到一个操作友好的单页决策树去列出一些对于所侦测问题至关重要的信息。我们使用Darkstar在线游戏服务和Hadoop文件系统数据集验证了我们的方法，它们以高精度和低假阳性率侦测出了许多真实问题。<br>在Hadoop案例中，我们能够在3min中内分析24百万条log。我们的方法可以工作于任意大小的log源数据上，不需要对服务软件进行修改，不需要人为输入，也不需要关于软件核心的知识。</p>
<h1 id="1-介绍"><a href="#1-介绍" class="headerlink" title="1. 介绍"></a>1. 介绍</h1><p>当一个由成百上千个运行着成百上千软件的电脑组成的大规模数据中心出现问题时，运维人员需要使用各种工具去侦测系统问题。<br>讽刺的是，有一个信息源，它囊括了几乎每种软件的细节信息，但却被经常忽略，它就是精简的console log。那些细节信息反映了软件原始开发者对于显著和不寻常活动的想法。</p>
<p>由于不同软件log形式不一致，而且由于软件的频繁修订和更新，导致log很混乱。我们的目标就是提供更好的工具去从log中提取价值。</p>
<p>由于log数据太庞大以至于不能人工检查，同时太不结构化以至于无法自动分析，因此运维人员往往通过搜索‘eroor’，’critical’之类关键词的方式来提取信息，但这已被表明对于侦测问题而言是不充分的。<br>而基于规则的处理过程是一个改进，但由于缺乏关于特殊软件成分的细节性知识，以及被各成分的交互所影响，依旧难以写出规则，去检出最相关的log。<br>代替要求用户去检测这一方式，我们提供了工具去自动发现有效的log信息。</p>
<p>由于不寻常的log信息通常表明了问题的根源，将之格式化机器学习中的异常检测问题是合理的。但是，用现有的方法并不能很好的解决这个问题，他是一个在不同种类log信息及其关系中进行异常检测的问题。<br>因此，相较于分析log中的字词，我们创造特征去精确捕获log之间各种各样的相关关系，然后通过这些特征去进行异常检测。创造这些特征需要利用源码信息去扩充log解析过程，这正是我们方法贡献的一部分。</p>
<p>我们研究很多应用于网络服务上的流行软件系统的log和源码，观察到console log比他看起来的更有结构性：‘schema’的定义隐含在log中，并可以从程序源码中发现。这个发现是我们log解析方法的关键，将可以导出细节和精确的特征。<br>我们相信开源软件开源代码的获取并不是我们方法的实践缺点。</p>
<p>我们的贡献是一个一般的四步法，它允许机器学习和信息检索技术被用于大海捞针，发现系统问题，而不需要人工输入。特别的，我们的方法包含了以下四个贡献：</p>
<ol>
<li>通过分析源码来发现log中隐含结构的技术;</li>
<li>log中信息的识别和特征的创造生成;</li>
<li>从大数据集中有效侦测异常的机器学习和信息检索方法论的证明;</li>
<li>异常侦测结果的一个合适、自动、用户友好的可视化构建方法。</li>
</ol>
<p>我们对log的分析可以深入细节层，可以并行化。</p>
<p>我们使用两个数据集做了验证。在Darkstar中，我们可以即时检测行为异常并提供异常原因的线索。在Hadoop中，我们可以探测运行时间异常这一经常被忽视的问题，并将结果可视化。</p>
<p>第二部分提供了我们方法的概要，第三部分从细节层面描述了我们的log解析技术，第四和第五部分阐明了特征创造和异常检测的结果，<br>第六部分评估了我们的方法并讨论了可视化技术，第七部分讨论了一些扩展和提出了改进log质量的建议，第八部分总结了相关工作，第九部分描述了一些结论。</p>
<h1 id="2-方法概览"><a href="#2-方法概览" class="headerlink" title="2. 方法概览"></a>2. 方法概览</h1><h2 id="2-1-信息被埋藏在log之中"><a href="#2-1-信息被埋藏在log之中" class="headerlink" title="2.1 信息被埋藏在log之中"></a>2.1 信息被埋藏在log之中</h2><p>重要信息被埋藏在大量log之中。为了自动分析log，我们需要创造高质量特征，实现log信息的数值表示，以便用于机器学习算法。以下三个关键观察导致了我们对这个问题的解决方法。</p>
<p><strong>源码是log的‘schema’。</strong> 尽管log可以以任意的文本格式出现，但实际上他们相当结构化，因为他们被系统中相对较小的log打印陈述规则集合所生成。</p>
<p>考虑图1中所展示的简单的log摘录和产生他们的源码。直觉上，使用源码信息去发现log的隐藏‘schema’是容易的。我们利用源码分析去发现log的固有结构。<br>我们方法的最显著优点是能够精确解析所有可能的log信息，即使是很少出现的log信息。另外，我们可以利用存在的方法去删除大部分启发式和猜测式的log解析。</p>
<p><img src="http://i1.piimg.com/567571/e2199d624f5440ef.png" alt></p>
<p><strong>通用的log结构导致有用的特征。</strong> 在这篇论文中，我们将log的常量部分称作<em>信息类型（message types）</em>，变量部分称作<em>信息变量（meassge variables）</em>。</p>
<p>我们仅仅将常量字符串标记为信息类型，完全忽视它们的语义。</p>
<p>信息变量也包含了至关重要的信息。我们识别了两个重要的信息变量：时间戳和各种各样的计数。</p>
<p><em>识别器（identifiers）</em>是用于识别客体的变量。而<em>状态变量（state vars）</em>是用于列举客体有的一系列可能状态的标签。我们可以基于频率判别一个给定的变量是识别器还是状态变量。表1给出了例子。直觉上，状态变量会有相对小的不同值，而识别器会有相对大的不同值（细节在第四部分）。</p>
<p><img src="http://i4.buimg.com/567571/3bca69ffecc22d0c.png" alt></p>
<p>信息类型和变量包含了重要的信息。但是，缺少工具去抽取这些结构。操作者要么忽视他们，要么手工花时间梳理他们。</p>
<p>我们的log解析方法允许我们使用结构化信息，例如信息类型和变量，去自动创造特征捕获log信息。据我们所知，这是工作是首次从log中抽取信息到这个粒度水平。</p>
<p><strong>信息强相关。</strong> 当log被合适的分组，组内信息具有强和稳定的相关性。例如，包含了特定文件名称的信息是高度相关的，因为它们很可能来自于系统中逻辑相关的操作步骤。</p>
<p>一组相关信息往往比起个体信息对问题具有更好的指示作用。 许多异常仅仅被不完全的信息片段所指出。例如，一个文件写入操作悄然失败（或许是因为开发者没有正确的处理报错机制），并没有一个单个的错误信息可以指出故障。但是，通过相同文件的相关信息，我们可以通过观察到相应的关闭文件信息缺失来侦测写入故障。先前的研究仅仅利用时间窗口进行log分组，并且侦测精度会遭受噪音影响。我们基于更为精确的信息去进行log分组，例如使用上面提到的信息变量。在这种情况下，相关性更加强大更具可读性，因此异常相关更容易被侦测。</p>
<h2 id="我们方法的工作流程"><a href="#我们方法的工作流程" class="headerlink" title="我们方法的工作流程"></a>我们方法的工作流程</h2><p><img src="http://i1.piimg.com/567571/696a19a03ea6c0f4.png" alt></p>
<p>图2展示了我们挖掘工作通用框架的四个步骤。</p>
<ol>
<li><p>log解析。<br>我们首先将log从非结构化文本转化为（信息类别，一系列信息变量）的数据结构。我们从源码中得到了所有可能的log信息模板字符串，并将之与log匹配，从而发现log的结构。我们的实验表明我们可以在真实系统中获得高精度的解析。<br>有系统使用了结构化追踪，例如BerkelyDB。既然这样，由于log已经被结构化了，我们可以跳过这个步骤，直接使用我们的特征创造和异常检测方法。注意这些结构化log仍然包含了识别器和状态变量。</p>
</li>
<li><p>特征创造。<br>下一步，我们通过对相关信息分组，并选择合适的变量，来构造特征向量。在这篇论文中，我们集中于构造状态比率向量和信息计数向量特征，这些在先前的研究中没有被探索。在我们对两个大规模真实统的实验中，这些特征都带来了很好的侦测结果。</p>
</li>
<li><p>异常检测。<br>然后，我们使用异常检测方法去挖掘特征向量，给每个特征向量打上正常或不正常的标签。我们发现基于主成分分析的异常侦测方法在这些特征上已经可以做的非常好。这是一个非监督学习算法，可以排除来自运维人员的先验需求，直接让所有的参数被自动选择或者轻松的进行调整。尽管我们使用了这一特殊的机器学习算法，它并不是我们方法的本质所在，利用不同抽取特征的不同算法可以轻易对我们的框架进行拓展。</p>
</li>
<li><p>可视化。<br>最后，为了让运维人员更好的理解侦测结果，我们利用决策树对结果做了可视化。相较于PCA侦测器，决策树以类似于活动处理规则的形式，提供了问题是如何被侦测到的更为细节的解释。</p>
</li>
</ol>
<h2 id="2-3-案例研究与数据收集"><a href="#2-3-案例研究与数据收集" class="headerlink" title="2.3 案例研究与数据收集"></a>2.3 案例研究与数据收集</h2><p>我们研究了22个被广泛部署的开源系统的源码和log。表2总结了成果。尽管这些系统本质上不同，它们在不同的时间被不同的开发者用不同的语言开发，其中20个系统还使用任意的log文本格式，我们基于源码的log解析应用到了全部20个系统上。有趣的是，</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2017/03/23/machine learning/machine learning/回归树/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/03/23/machine learning/machine learning/回归树/" class="post-title-link" itemprop="url">回归树</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-03-23 14:41:15" itemprop="dateCreated datePublished" datetime="2017-03-23T14:41:15+08:00">2017-03-23</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p>
<h1 id="CART"><a href="#CART" class="headerlink" title="CART"></a>CART</h1><p>ID3,C4.5都只能做分类，不能做回归。CART则采用了二叉树的形式，使用Gini系数构建分类树，使用均方误差构建回归树。</p>
<p>在构建回归树时，根据分裂后各子节点均方误差之和与父节点均方误差的差值来选择分裂条件进行分裂。</p>
<p>构建好后，新的测试样本的预测值是其所在叶节点中训练样本label的均值。<br>如果真是这样，那么回归树是没有办法进行拓展的，换言之对于不在训练集范围的y值它无法预测到。例如，他做不到线性回归的拓展性。</p>
<p>生成回归树之后，在各叶节点上建立线性回归模型，产生最终预测结果。换言之，可以理解为先通过分类选择临近样本点，利用临近样本点去进行回归学习。<br>也就是先分类缩小范围再回归。<strong>局部加权回归思想的一种变体</strong>。</p>
<p>[TBC]<br><a href="http://www.stat.wisc.edu/~loh/treeprogs/guide/wires11.pdf" target="_blank" rel="noopener">回归树</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2017/03/23/machine learning/Information theory/熵相关/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/03/23/machine learning/Information theory/熵相关/" class="post-title-link" itemprop="url">熵相关</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-03-23 13:43:11" itemprop="dateCreated datePublished" datetime="2017-03-23T13:43:11+08:00">2017-03-23</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/Information-theory/" itemprop="url" rel="index"><span itemprop="name">Information theory</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p>
<h1 id="信息熵（entropy）"><a href="#信息熵（entropy）" class="headerlink" title="信息熵（entropy）"></a>信息熵（entropy）</h1><ul>
<li><p>自信息量<br>概率越大，不确定越小：<br>$$I(X=k) = \log \frac{1}{p(X=k)}$$</p>
</li>
<li><p>信息熵<br>描述变量的不确定性,是符号自信息量的数学期望：<br>$$H(X) = \sum_{k=1}^K p(X=k) \log \frac{1}{p(X=k)}$$<br>信息量的单位：bit（以2为底），Nat（以e为底），Det（以10为底）</p>
</li>
</ul>
<h1 id="条件熵"><a href="#条件熵" class="headerlink" title="条件熵"></a>条件熵</h1><p>条件自信息量的数学期望：<br>$$I(x_i | y_i) = \log \frac{1}{p(x_i | y_i)}$$<br>$$H(X|Y) = \sum_{x,y} p(xy) \log \frac{1}{p(x|y)}$$<br><strong><em>条件熵是用联合概率加权。</em></strong></p>
<h1 id="联合熵"><a href="#联合熵" class="headerlink" title="联合熵"></a>联合熵</h1><p>联合自信息量的数学期望：<br>$$I(x_i y_i) = \log \frac{1}{p(x_i y_i)}$$<br>$$H(XY) = \sum_{x,y} p (xy) \log \frac{1}{p(xy)}$$</p>
<h1 id="相互关系"><a href="#相互关系" class="headerlink" title="相互关系"></a>相互关系</h1><p>\begin{split} H(XY) &amp; = \sum_{x,y} p (xy) \log \frac{1}{p(xy)} \newline<br>&amp; = - \sum_{x,y} p(xy) (\log p(x) + \log p(y|x)) \newline<br>&amp; = - \sum_x p(x) \log p(x) \sum_y p(y|x) \quad - \sum_{x,y} p(xy) \log p(y|x) \newline<br>&amp; = H(X) + H(Y|X) \newline<br>&amp;s.t. \quad \sum_y p(y|x) = 1 \end{split}</p>
<h1 id="互信息"><a href="#互信息" class="headerlink" title="互信息"></a>互信息</h1><p><strong><em>事件互信息</em></strong> 是衡量两个事件集合之间的相关性,$Y=y_i$对$X=x_i$的互信息量定义为：<br>$$I(X=x_i;Y=y_i) = \log \frac{p(X=x_i|Y=y_i)}{p(X=x_i)}$$<br>$Y=y_i$对$X=x_i$的互信息量就是X的后验概率与先验概率比值的对数。显然有对称性：<br>$$I(X=x_i;Y=y_i) = I(Y=y_i;X=x_i) = \log \frac{p(X=x_i,Y=y_i)}{p(X=x_i)p(Y=y_i)}$$</p>
<p><strong><em>变量互信息</em></strong> 是衡量两个变量之间的相关性，定义为：<br>$$I(X;Y) = \sum_{x,y} p(x,y) \log \frac{p(x,y)}{p(x)p(y)}$$</p>
<h1 id="交叉熵"><a href="#交叉熵" class="headerlink" title="交叉熵"></a>交叉熵</h1><p>定义为：<br>$$H(P,Q) = \sum_x P(x) \log Q(x)$$</p>
<h1 id="相对熵（KL散度）"><a href="#相对熵（KL散度）" class="headerlink" title="相对熵（KL散度）"></a>相对熵（KL散度）</h1><p>用于度量两个概率分布之间的差别，非对称，即：<br>$$D(P||Q) \neq D(Q||P)$$<br>对于不等式左边，P是真实分布，Q是P的拟合分布</p>
<p>定义：<br>\begin{split} D_{KL} (P||Q) &amp; = \sum_x P(x) \log \frac{P(x)}{Q(x)} \newline<br>&amp; = \sum_x P(x) \log P(x) - \sum_x P(x) \log Q(x) \newline<br>&amp; = -H(P) + H(P,Q) \end{split}</p>
<p>由于对数函数是上凸函数，且 $\sum_x P(x) =1,P(x) \geq 0$ 为凸组合,用Jensen不等式可知：<br>\begin{split} D_{KL} (P||Q) &amp; = \sum_x P(x) \log \frac{P(x)}{Q(x)} \newline<br>&amp; = - \sum_x P(x) \log \frac{Q(x)}{P(x)} \newline<br>&amp; \geq - \log \sum_x P(x) \frac{Q(x)}{P(x)} \newline<br>&amp; \geq 0 \end{split}<br>KL散度大于等于0,当且仅当两分布相同时，等于0。  </p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2017/03/22/machine learning/machine learning/cost function/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/03/22/machine learning/machine learning/cost function/" class="post-title-link" itemprop="url">cost function</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-03-22 14:14:48" itemprop="dateCreated datePublished" datetime="2017-03-22T14:14:48+08:00">2017-03-22</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p>
<h1 id="hinge-loss"><a href="#hinge-loss" class="headerlink" title="hinge loss"></a>hinge loss</h1><p>对于非连续可微损失函数，如何优化？以hinge loss为例</p>
<p><a href>Pegasos: primal estimated sub-gradient solver for SVM</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2017/03/21/machine learning/machine learning/生成模型与判别模型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/03/21/machine learning/machine learning/生成模型与判别模型/" class="post-title-link" itemprop="url">生成模型与判别模型</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-03-21 10:21:52" itemprop="dateCreated datePublished" datetime="2017-03-21T10:21:52+08:00">2017-03-21</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>
          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2017/03/21/python/修改文件内容和修改时间/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/03/21/python/修改文件内容和修改时间/" class="post-title-link" itemprop="url">修改文件内容和修改时间</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-03-21 09:52:38" itemprop="dateCreated datePublished" datetime="2017-03-21T09:52:38+08:00">2017-03-21</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/python/" itemprop="url" rel="index"><span itemprop="name">python</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="文件管理"><a href="#文件管理" class="headerlink" title="文件管理"></a>文件管理</h1><ul>
<li>查找路径下全部文件全路径</li>
<li>修改文件内容</li>
<li>修改文件创建时间和修改时间</li>
</ul>
<figure class="highlight lua"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line">import <span class="built_in">time</span></span><br><span class="line">import <span class="built_in">os</span></span><br><span class="line">import re</span><br><span class="line"></span><br><span class="line">root_path = <span class="string">'/home/linker-will/Dropbox/_posts'</span></span><br><span class="line">time_list = []</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> root,dirs,files <span class="keyword">in</span> <span class="built_in">os</span>.walk(root_path):        </span><br><span class="line">    <span class="keyword">for</span> filespath <span class="keyword">in</span> files:</span><br><span class="line">        f_path = <span class="built_in">os</span>.<span class="built_in">path</span>.join(root,filespath)</span><br><span class="line">        categories = <span class="built_in">os</span>.<span class="built_in">path</span>.basename(<span class="built_in">os</span>.<span class="built_in">path</span>.dirname(f_path))</span><br><span class="line">        f_name = <span class="built_in">os</span>.<span class="built_in">path</span>.basename(f_path)[:<span class="number">-3</span>]</span><br><span class="line">        mtime = <span class="built_in">os</span>.<span class="built_in">path</span>.getmtime(f_path)</span><br><span class="line">        ctime = <span class="built_in">os</span>.<span class="built_in">path</span>.getctime(f_path)</span><br><span class="line">        local_time = <span class="built_in">time</span>.localtime(<span class="built_in">os</span>.<span class="built_in">path</span>.getmtime(f_path))</span><br><span class="line">        f_mtime = <span class="built_in">time</span>.strftime(<span class="string">'%Y-%m-%d %H:%M:%S'</span>,local_time)</span><br><span class="line">        </span><br><span class="line">        f = <span class="built_in">open</span>(f_path,<span class="string">'r'</span>)</span><br><span class="line">        old = f.<span class="built_in">read</span>()</span><br><span class="line">        <span class="keyword">if</span> categories == <span class="string">'_posts'</span>:</span><br><span class="line">            <span class="built_in">print</span>(filespath,<span class="string">"don't have dirname"</span>)</span><br><span class="line">            pass</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(re.findall(<span class="string">'title:.*\ndate:.*\ncategories:.*'</span>,old)) == <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(filespath,<span class="string">"don't have title format"</span>)</span><br><span class="line">            pass</span><br><span class="line">        <span class="keyword">if</span> <span class="built_in">len</span>(re.findall(<span class="string">'\[TBC\]'</span>,old)) != <span class="number">0</span>:</span><br><span class="line">            <span class="built_in">print</span>(filespath,<span class="string">"[TBC]"</span>)</span><br><span class="line">        </span><br><span class="line">        data = re.<span class="built_in">sub</span>(<span class="string">'title:.*\ndate:.*\ncategories:.*'</span>,\</span><br><span class="line">               <span class="string">'title: %s\ndate: %s\ncategories: %s'</span>%(f_name,f_mtime,categories),old)</span><br><span class="line">        f.<span class="built_in">close</span>()</span><br><span class="line">        f = <span class="built_in">open</span>(f_path,<span class="string">'w'</span>)</span><br><span class="line">        f.<span class="built_in">write</span>(data)</span><br><span class="line">        f.<span class="built_in">close</span>()</span><br><span class="line">        <span class="built_in">os</span>.utime(f_path,times=(ctime,mtime))</span><br></pre></td></tr></table></figure>
          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2017/03/20/machine learning/statistics/参数估计方法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/03/20/machine learning/statistics/参数估计方法/" class="post-title-link" itemprop="url">参数估计方法</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-03-20 19:55:32" itemprop="dateCreated datePublished" datetime="2017-03-20T19:55:32+08:00">2017-03-20</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/statistics/" itemprop="url" rel="index"><span itemprop="name">statistics</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p>
<h1 id="极大似然估计（MLE，Maximum-Likelihood）"><a href="#极大似然估计（MLE，Maximum-Likelihood）" class="headerlink" title="极大似然估计（MLE，Maximum Likelihood）"></a>极大似然估计（MLE，Maximum Likelihood）</h1><ol>
<li><p>模型确定，参数未知（参数是固定的未知值，而非随机数）</p>
</li>
<li><p>$x_i$ 为iid样本(独立同分布)</p>
</li>
<li><p>使得样本出现概率（即似然函数）最大的参数最有可能是真实参数</p>
</li>
</ol>
<p>即是求<br>$$\arg\max_\mu p(\mathbf{X};\mu)$$<br>其中：<br>$$p(\mathbf{x};\mu) = \prod_{i} p(x_i;\mu)$$<br>实际计算中，常常等价地处理对数似然函数。</p>
<h1 id="最大后验估计（MAP-maximum-a-posteriori）"><a href="#最大后验估计（MAP-maximum-a-posteriori）" class="headerlink" title="最大后验估计（MAP,maximum a posteriori）"></a>最大后验估计（MAP,maximum a posteriori）</h1><ul>
<li>参数具有先验概率$p(\mu)$</li>
</ul>
<p>因此，问题转化为求给定观测值情况下使后验概率最大的$\mu$：<br>\begin{split} \hat{\mu_{MAP}} &amp;= \arg\max_{\mu} p(\mu|\mathbf{X}) \newline<br>&amp;= \arg\max_{\mu} \frac{p(\mathbf{X}|\mu)p(\mu)}{p(\mathbf{X})} \newline<br>&amp;=\arg\max_{\mu} p(\mathbf{X}|\mu)p(\mu) \end{split}</p>
<h1 id="MAP与Bayes估计-联系与区别"><a href="#MAP与Bayes估计-联系与区别" class="headerlink" title="MAP与Bayes估计 联系与区别"></a>MAP与Bayes估计 联系与区别</h1><p>[TBC]</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2017/03/20/machine learning/math/Jensen不等式/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/03/20/machine learning/math/Jensen不等式/" class="post-title-link" itemprop="url">Jensen不等式</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-03-20 19:55:29" itemprop="dateCreated datePublished" datetime="2017-03-20T19:55:29+08:00">2017-03-20</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/math/" itemprop="url" rel="index"><span itemprop="name">math</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p>
<p>#凸函数定义与性质</p>
<p>#Jensen不等式证明</p>
<p>[TBC]</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2017/03/20/machine learning/machine learning/广义线性模型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/03/20/machine learning/machine learning/广义线性模型/" class="post-title-link" itemprop="url">广义线性模型</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-03-20 19:55:21" itemprop="dateCreated datePublished" datetime="2017-03-20T19:55:21+08:00">2017-03-20</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>[TBC]</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2017/03/20/machine learning/machine learning/局部加权回归/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/03/20/machine learning/machine learning/局部加权回归/" class="post-title-link" itemprop="url">局部加权回归</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-03-20 16:47:58" itemprop="dateCreated datePublished" datetime="2017-03-20T16:47:58+08:00">2017-03-20</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/machine-learning/" itemprop="url" rel="index"><span itemprop="name">machine learning</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p>
<h1 id="非参数学习方法"><a href="#非参数学习方法" class="headerlink" title="非参数学习方法"></a>非参数学习方法</h1><p><strong>参数学习方法</strong>：在训练完成所有数据后得到一系列训练参数，然后根据训练参数来预测新样本的值，不再依赖之前的训练数据，参数值是确定的。</p>
<p><strong>非参数学习方法</strong>：在预测新样本值时候每次都会重新训练数据得到新的参数值，也就是说每次预测新样本都会依赖训练数据集合，所以每次得到的参数值是不确定的。</p>
<h1 id="局部加权回归"><a href="#局部加权回归" class="headerlink" title="局部加权回归"></a>局部加权回归</h1><p>在普通的回归问题中，均方误差损失：<br>$$J(\theta) = \frac{1}{n} \sum_i (h_\theta(x_i) - y_i)^2$$</p>
<p>局部加权回归中，均方误差损失：<br>$$J(\theta) = \frac{1}{n} \sum_i \omega_i (h_\theta(x_i) - y_i)^2$$<br>$$\omega_i = \exp(- \frac{(x_i - x)^2}{2 \tau^2})$$<br>其中，$x$是待预测的样本点，按待预测点与样本点距离分配权重。<br>显然，对于每一个需要预测的样本点，都需要重新建立模型去预测，计算量会更大，但欠拟合问题会被削弱。</p>
<h1 id="references"><a href="#references" class="headerlink" title="references"></a>references</h1><p><a href="http://blog.csdn.net/acdreamers/article/details/44662753" target="_blank" rel="noopener">局部加权回归</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2017/03/20/machine learning/preprocessing/连续性变量转化为离散型/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/03/20/machine learning/preprocessing/连续性变量转化为离散型/" class="post-title-link" itemprop="url">连续性变量转化为离散型</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-03-20 13:59:26" itemprop="dateCreated datePublished" datetime="2017-03-20T13:59:26+08:00">2017-03-20</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/preprocessing/" itemprop="url" rel="index"><span itemprop="name">preprocessing</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2017/03/19/machine learning/preprocessing/归一化的意义与方法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/03/19/machine learning/preprocessing/归一化的意义与方法/" class="post-title-link" itemprop="url">归一化的意义与方法</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-03-19 11:36:50" itemprop="dateCreated datePublished" datetime="2017-03-19T11:36:50+08:00">2017-03-19</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/preprocessing/" itemprop="url" rel="index"><span itemprop="name">preprocessing</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p>
<h1 id="标准化定义"><a href="#标准化定义" class="headerlink" title="标准化定义"></a>标准化定义</h1><p>标准化就是要把你需要处理的数据经过处理后（通过某种算法）限制在你需要的一定范围内.</p>
<h1 id="标准化原因"><a href="#标准化原因" class="headerlink" title="标准化原因"></a>标准化原因</h1><ol>
<li><p>为了后面数据处理的方便，把不同量纲的东西放在同一量纲下比较，即把不同来源的数据统一到一个参考系下，这样比较起来才有意义。</p>
</li>
<li><p>保正程序运行时收敛加快，大部分模型标准化后收敛速度会加快。例如，下面的例子，房间数和面积数不在一个量纲上，面积数值太小，房间数太大，成椭圆状，按照梯度收敛速度会慢，理想的是数据类似圆圈的形状，经过有限几个步骤则收敛了。</p>
</li>
<li><p>要注意的是，有的模型在标准化之后会影响效果，有的模型则不会。对于标准化是否影响模型效果，主要看模型是否具有伸缩不变性。<br>有些模型在各个维度进行不均匀伸缩后，最优解与原来不等价，例如SVM。对于这样的模型，除非本来各维数据的分布范围就比较接近，否则必须进行标准化，以免模型参数被分布范围较大或较小的数据dominate。<br>有些模型在各个维度进行不均匀伸缩后，最优解与原来等价，例如标准的logistic regression 和linear regression（加正则项后，正则项可能不具备伸缩不变性），简单的树模型（各个节点各算个的切分点）。对于这样的模型，是否标准化理论上不会改变最优解。但是，由于实际求解往往使用迭代算法，如果目标函数的形状太“扁”，迭代算法可能收敛得很慢甚至不收敛。所以对于具有伸缩不变性的模型，最好也进行数据标准化。但SVM则必须进行标准化。同的模型对特征的分布假设是不一样的。比如SVM 用高斯核的时候，所有维度共用一个方差，这不就假设特征分布是圆的么，输入椭圆的就坑了人家。<br>首先，对于gradient descent算法来说，learning rate的大小对其收敛速度至关重要。如果feature的scale不同，理论上不同的feature就需要设置不同的learning rate，但是gradient descent只有一个learning rate，这就导致不同feature的收敛效果不同，从而影响总体的收敛效果。所以在求解模型之前标准化不同feature的scale，可以有效提高gradient descent的收敛速度。<br>除此之外，如果feature的scale相差很大，则会出现scale越大的feature，对模型的影响越大。比如对于multivariate regression, 极端情况下, 有一个特征的值特别特别大，其他特征的值都特别特别小，那么cost function就被这个特别大的特征主导，甚至退化为univariate。即feature scale相差很大，线性回归模型得优化结果也会受到影响。</p>
</li>
</ol>
<p>也需要注意的是，各维分别做标准化会丢失各维方差这一信息，但各维之间的相关系数可以保留</p>
<h1 id="标准化方法"><a href="#标准化方法" class="headerlink" title="标准化方法"></a>标准化方法</h1><p>具体问题具体分析，可以先通过抽样实验的方式，去选择最好效果的标准化方法。</p>
<p>常用的方法有：</p>
<ul>
<li><p>0-1标准化：<br>$$y_i = \frac{x_i-\min x}{\max x - \min x}$$</p>
</li>
<li><p>对数标准化,可以对x做处理使之大于等于1（大于0必须，大于1个人认为不必须）,也可以取对数之后再除以最大对数值进行进一步缩放：<br>$$y = \log x$$</p>
</li>
<li><p>反正切标准化：<br>$$y = \frac{2}{\pi} \arctan x$$</p>
</li>
<li><p>z-score标准化：<br>$$y = \frac{x_i - \overline{x}}{s};\overline{x} = \frac{\sum\limits_i^n x}{n};s = \sqrt{\frac{\sum (x-\overline{x})^2}{n-1}}$$</p>
</li>
<li><p>归一化：<br>$$y = \frac{x_i}{\sum x}$$</p>
</li>
</ul>
<h1 id="references"><a href="#references" class="headerlink" title="references"></a>references</h1><p><a href="http://blog.csdn.net/resourse_sharing/article/details/51979494" target="_blank" rel="noopener">为什么要特征标准化及特征标准化方法</a></p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2017/03/19/machine learning/python-sklearn/the abstract of sklearn.ensemble/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/03/19/machine learning/python-sklearn/the abstract of sklearn.ensemble/" class="post-title-link" itemprop="url">the abstract of sklearn.ensemble</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-03-19 10:10:07" itemprop="dateCreated datePublished" datetime="2017-03-19T10:10:07+08:00">2017-03-19</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/python-sklearn/" itemprop="url" rel="index"><span itemprop="name">python-sklearn</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="sklearn-ensemble-AdaBoostClassifier-sklearn-ensemble-AdaBoostRegressor"><a href="#sklearn-ensemble-AdaBoostClassifier-sklearn-ensemble-AdaBoostRegressor" class="headerlink" title="sklearn.ensemble.AdaBoostClassifier/sklearn.ensemble.AdaBoostRegressor"></a>sklearn.ensemble.AdaBoostClassifier/sklearn.ensemble.AdaBoostRegressor</h1><p>Adaboost分类器是一个超估计。它通过在原始数据集上拟合一个分类器开始，然后在相同的数据集上调整误分类样本的权重，并用其再次拟合相同分类器，因此接下来的分类器会更关注于难以正确分类的样本。</p>
<h1 id="sklearn-ensemble-BaggingClassifier-sklearn-ensemble-BaggingRegressor"><a href="#sklearn-ensemble-BaggingClassifier-sklearn-ensemble-BaggingRegressor" class="headerlink" title="sklearn.ensemble.BaggingClassifier/sklearn.ensemble.BaggingRegressor"></a>sklearn.ensemble.BaggingClassifier/sklearn.ensemble.BaggingRegressor</h1><p>一个bagging分类器。</p>
<p>bagging分类器是一种集成meta-估计器。<br>它用原始数据集的随机子集去拟合基分类器，并聚合各自的预测结果（投票或者平均）去形成一个最终结果。<br>这样一个meta-估计器典型的被用于<strong>降低</strong>黑箱估计器（例如决策树）的<strong>方差</strong>，它通过在结构化过程中引入随机性并对结果进行集成来实现。</p>
<p>这个算法包括了过往的一些研究。当数据集中的随机子集被抽取出来作为样本的随机子集，这个算法就是Pasting。<br>如果样本的抽取是有放回的，这个方法就是Bagging。当特征被随机抽取，这个方法就是Random Subspaces。<br>最后，如果基分类器是建立在随机的样本和特征之上，这个方法被叫做Random Patches。</p>
<h1 id="sklearn-ensemble-ExtraTreesClassifier-sklearn-ensemble-ExtraTreesRegressor"><a href="#sklearn-ensemble-ExtraTreesClassifier-sklearn-ensemble-ExtraTreesRegressor" class="headerlink" title="sklearn.ensemble.ExtraTreesClassifier/sklearn.ensemble.ExtraTreesRegressor"></a>sklearn.ensemble.ExtraTreesClassifier/sklearn.ensemble.ExtraTreesRegressor</h1><p>这个算法通过拟合许多基于各种各样数据集子样本的随机决策树并平均结果来改善预测精度和控制过拟合。</p>
<p>该方法只对样本而不对特征进行抽样。但是，它通过max_features这样一个参数去控制每次分裂。<br>每次分类，只随机选择max_features个特征，从中选出最优分裂。但如果随机出的特征都不支持有效分裂，<br>算法会从剩下的特征中继续搜索出一个合宜的分裂。分裂的终止有其他相关树特征决定。<br>在sklearn中，<strong>ExtraTreesClassifier可以设置class_weight参数</strong>，这非常好！</p>
<p>我查阅资料并结合参数，猜测sklearn中ET树原理：<br>对于样本并不进行随机抽样（并没有max_samples参数），或者说用的是全样本。<br>每次分类时，随机选择max_features特征。对于这些特征，随机选择分割点。（正常二叉树情况下，需要计算特征各种可能分裂方式并寻找出最优。但这里仅仅是随机选择出特征分裂方式）<br>然后比较各特征分裂的信息增益，选择最优者进行树的生长。因此在这种情况下所有样本也都是oob（out-of-bag）样本。<br>bootstrap参数只能理解为是针对特征了。</p>
<h1 id="sklearn-ensemble-GradientBoostingClassifier-sklearn-ensemble-GradientBoostingRegressor"><a href="#sklearn-ensemble-GradientBoostingClassifier-sklearn-ensemble-GradientBoostingRegressor" class="headerlink" title="sklearn.ensemble.GradientBoostingClassifier/sklearn.ensemble.GradientBoostingRegressor"></a>sklearn.ensemble.GradientBoostingClassifier/sklearn.ensemble.GradientBoostingRegressor</h1><p>GB建立了一个前向加法模型，它允许优化任意不同的损失函数。<br>对于分类情况，在每一步，n_classes_（类数）个回归树被建立去拟合logloss。二分类是一个特殊情况，只使用一颗回归树。<br>对于回归情况，使用一棵回归树去拟合任意损失函数。</p>
<h1 id="sklearn-ensemble-IsolationForest"><a href="#sklearn-ensemble-IsolationForest" class="headerlink" title="sklearn.ensemble.IsolationForest"></a>sklearn.ensemble.IsolationForest</h1><p>使用IsolationForest算法对每个样本回归一个异常得分。</p>
<p>IsolationForest通过随机选择一个特征并在该特征最大最小值之间随机选择一个分裂值去孤立观察值。</p>
<p>由于树结构可以表示递归分裂，因此孤立一个样本所需要的分裂次数等价于从根节点到终端节点的路径长度。</p>
<p>路径长度，通过对森林中随机树长度进行平均得到，作为一种正常性度量。</p>
<p>随机分裂过程对于异常值有显著更短的路径。因此，当随机树组成的森林对于特殊样本产生了更短路径，它们更可能是异常。</p>
<h1 id="sklearn-ensemble-RandomForestClassifier-sklearn-ensemble-RandomForestRegressor"><a href="#sklearn-ensemble-RandomForestClassifier-sklearn-ensemble-RandomForestRegressor" class="headerlink" title="sklearn.ensemble.RandomForestClassifier/sklearn.ensemble.RandomForestRegressor"></a>sklearn.ensemble.RandomForestClassifier/sklearn.ensemble.RandomForestRegressor</h1><p>这个算法通过拟合许多基于各种各样数据集子样本的随机决策树并平均结果来改善预测精度和控制过拟合。<br>子样本大小总是和原始输入样本尺寸相同，但是当bootstrap=True时样本有放回抽取。</p>
<h1 id="sklearn-ensemble-RandomForestEmbedding"><a href="#sklearn-ensemble-RandomForestEmbedding" class="headerlink" title="sklearn.ensemble.RandomForestEmbedding"></a>sklearn.ensemble.RandomForestEmbedding</h1><p>一种无监督变换，将数据集变换到高维稀疏空间。数据点根据每颗树叶子进行编码。使用one-hot编码方式，这将导致二元编码。</p>
<h1 id="sklearn-ensemble-VotingClassifier"><a href="#sklearn-ensemble-VotingClassifier" class="headerlink" title="sklearn.ensemble.VotingClassifier"></a>sklearn.ensemble.VotingClassifier</h1><p>用于对未集成的估计器列表进行投票集成。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2017/03/17/machine learning/NN/《神经网络和深度学习》第四章——神经网络可以计算任何函数的可视化证明/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/03/17/machine learning/NN/《神经网络和深度学习》第四章——神经网络可以计算任何函数的可视化证明/" class="post-title-link" itemprop="url">《神经网络和深度学习》第四章——神经网络可以计算任何函数的可视化证明</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-03-17 17:14:51" itemprop="dateCreated datePublished" datetime="2017-03-17T17:14:51+08:00">2017-03-17</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/NN/" itemprop="url" rel="index"><span itemprop="name">NN</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script><br>神经网络一个最显著的事实就是它可以计算任何的函数。它具有一种<em>普遍性</em>！</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2017/03/17/machine learning/NN/《神经网络和深度学习》第三章——改进神经网络的学习方法/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/03/17/machine learning/NN/《神经网络和深度学习》第三章——改进神经网络的学习方法/" class="post-title-link" itemprop="url">《神经网络和深度学习》第三章——改进神经网络的学习方法</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-03-17 16:12:16" itemprop="dateCreated datePublished" datetime="2017-03-17T16:12:16+08:00">2017-03-17</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/NN/" itemprop="url" rel="index"><span itemprop="name">NN</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p>
<h1 id="损失函数选择"><a href="#损失函数选择" class="headerlink" title="损失函数选择"></a>损失函数选择</h1><p>当均方误差作为损失函数，S型曲线作为激活函数时，梯度和激活函数导数相关。<br>因此当严重误判时（i.e.真值为0预测输出接近1），由于此时S型激活函数在两侧导数都非常小，会出现犯了明显错误但学习却非常缓慢的情况。<br>而交叉熵和S型输出层组合并不会出现这个问题，因为经过计算可知梯度和预测错误相关（预测错误 = 预测值-真值），但仅仅限于输出神经元上。但遗憾的是，<br>交叉熵情况下，对权重梯度跟输入\(a_j^l\)相关，因此当\(a_j^l\)趋向于0时，权重梯度会很小，学习缓慢。  </p>
<p>回顾logistic回归，交叉熵可以由极大似然估计推出。</p>
<p>softmax输出层和对数似然代价组合是另一种可能。</p>
<p><em>交叉熵代价在变大，而分类准确率在上升，这是可能的。合理的选用代价函数（便于优化），以及合理的调参（提高分类准确率）。</em></p>
<h1 id="防止过拟合"><a href="#防止过拟合" class="headerlink" title="防止过拟合"></a>防止过拟合</h1><ol>
<li><p>增加样本</p>
</li>
<li><p>L2规范化/权重衰减<br>规范化项里面不包含偏置。因此，对于偏置梯度不变。对于权重梯度：<br>$$\frac{\partial C}{\partial \omega} = \frac{\partial C_0}{\partial \omega} + \frac{\lambda}{n}\omega$$<br>$$\frac{\partial C}{\partial b} = \frac{\partial C_0}{\partial b}$$<br>其中\(C_0\)是原始代价函数：<br>$$C = C_0 + \frac{\lambda}{2n}\sum_\omega\omega^2$$<br>因此，权重学习规则就变成：<br>\begin{split}\omega \rightarrow \omega - \eta\frac{\partial C_0}{\partial \omega} - \frac{\eta\lambda}{n}\omega\newline<br>= (1 - \frac{\eta\lambda}{n})\omega - \eta\frac{\partial C_0}{\partial \omega}\end{split}<br>故而它又被称作权重衰减。<br><em>L2规范化还可以帮助逃离局部最优值。</em><br>规范化更倾向于小权重，对于输入噪音不会过度敏感。<br>不对偏置进行规范化，是因为：</p>
<ul>
<li>实践看来改善不明显</li>
<li>大的偏置并不会对噪音敏感</li>
<li>大的偏置让网络更加灵活，让神经元更加容易饱和  </li>
</ul>
</li>
</ol>
<p>为什么L2规范化往往比不做规范化有更好的结果？<br>仅是实践经验，并不是因为“奥卡姆剃刀”。因为简单更好本身就无法证明，何况也无法严格判定在他同等效果下哪个模型更简单（对效果除权很难）。<br>对模型真正的测试不是简单性，而是在新场景中新活动的预测能力。</p>
<ol start="3">
<li><p>L1规范化<br>代价函数变为：<br>$$C = C_0 + \frac{\lambda}{2n}\sum_\omega|\omega|$$<br>求导得：<br>$$\frac{\partial C}{\partial \omega} = \frac{\partial C_0}{\partial \omega} + \frac{\lambda}{n}sgn(\omega)$$<br>因此更新规则为：<br>$$\omega \rightarrow \omega - \frac{\lambda}{n}sgn(\omega) - \frac{\partial C_0}{\partial \omega}$$<br>在L2规范化中，权重通过一个和$\omega$成比例的量进行缩小。在L1中，权重通过一个常量向0缩小。<br>所以，当权重绝对值很大时，L1规范化的权重缩小原比L2规范化要小的多。相反，当一个特定的权重绝对值很小时，L1规范化的权重缩小要比L2规范化大的多。<br>最终结果就是，L1规范化倾向于聚集网络权重在相对少量的高重要度连接上，而其他权重就会被驱使向0接近。</p>
</li>
<li><p>弃权（dropout）<br>在训练过程中通过概率使得部分隐藏神经元不参与计算和更新，不断重复这个过程使得权重选择性更新。最终得到的结果就可以视为bagging。<br>这样理解好象是错的？</p>
</li>
<li><p>人为扩展训练数据<br>就MNIST而言，可以通过对图像进行小幅旋转来认为产生新数据。<br>在语音识别中，还可以通过增加背景噪声来扩展训练数据，从而更加关注普适特征，训练出更具范化能力的模型。</p>
</li>
</ol>
<h1 id="权重初始化"><a href="#权重初始化" class="headerlink" title="权重初始化"></a>权重初始化</h1><p>一般采用标准正态分布来进行权重和偏置初始化。<br>考虑一个极端情况，一共n个输入神经元，输入值都为1。那么加权和就会服从均值0,方差n+1的正态分布，形如：<br><img src="http://p1.bpimg.com/567571/d42d2eff91ab2285.png" alt><br>就会造成带权和很大，从而相对应的隐藏神经元饱和，激活值接近0或1。从而梯度下降学习将会非常缓慢。<br>因此，我们可以选择均值0,方差1/n的正态分布来进行权重初始化，对于偏置习惯上依旧使用标准正态分布。</p>
<h1 id="其他参数"><a href="#其他参数" class="headerlink" title="其他参数"></a>其他参数</h1><ul>
<li>学习率</li>
<li>正则项系数</li>
<li>minibatch数量</li>
<li>迭代次数</li>
<li>隐藏神经元数量</li>
<li>层数</li>
<li>输出编码方式</li>
</ul>
<h1 id="其他技术"><a href="#其他技术" class="headerlink" title="其他技术"></a>其他技术</h1><ol>
<li><p>Hessian优化（泰勒展开与二阶近似）</p>
</li>
<li><p>基于momentum的梯度下降</p>
</li>
</ol>
<h1 id="其他激活函数"><a href="#其他激活函数" class="headerlink" title="其他激活函数"></a>其他激活函数</h1><ul>
<li><p>tanh<br><img src="http://i1.piimg.com/567571/32cda276ddd538ca.png" alt>  </p>
</li>
<li><p>relu<br><img src="http://i1.piimg.com/567571/c10b3dae75ed663c.png" alt></p>
</li>
</ul>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2017/03/17/machine learning/NN/《神经网络和深度学习》第二章——反向传播算法如何工作/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/03/17/machine learning/NN/《神经网络和深度学习》第二章——反向传播算法如何工作/" class="post-title-link" itemprop="url">《神经网络和深度学习》第二章——反向传播算法如何工作</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-03-17 15:31:14" itemprop="dateCreated datePublished" datetime="2017-03-17T15:31:14+08:00">2017-03-17</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/NN/" itemprop="url" rel="index"><span itemprop="name">NN</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script></p>
<h1 id="反向传播算法"><a href="#反向传播算法" class="headerlink" title="反向传播算法"></a>反向传播算法</h1><p>反向传播算法通过<em>链式法则</em>来简化计算<em>损失函数对各权重和偏置的梯度</em>。</p>
<h2 id="符号说明、一个定义、四个关键方程："><a href="#符号说明、一个定义、四个关键方程：" class="headerlink" title="符号说明、一个定义、四个关键方程："></a>符号说明、一个定义、四个关键方程：</h2><ol>
<li><p>符号说明：<br>\(l-1\)层\(k\)神经元与\(l\)层\(j\)神经元连接权重： \(\omega_{jk}^l\)<br>带权和： \(z_j^{l} = \sum\limits_{k}{\omega_{jk}^{l}a_k^{l-1}} + b_j^l\)<br>激活值： \(a_j^l = \sigma(z_j^l)\)</p>
</li>
<li><p>定义\(l\)层\(j\)神经元误差：<br>$$\delta_j^l = \frac{\partial C}{\partial z_j^l}$$<br>最优化时，梯度应当为0,因此可以理解为梯度绝对值越大，离最优化状态越远，误差越大。之所以对z而不是a求梯度，则是计算方便的考虑。</p>
</li>
<li><p>输出层误差方程：<br>$$\delta_j^L = \frac{\partial C}{\partial a_j^L}\sigma^{‘}(z_j^L)$$<br>矩阵形式：<br>$$\delta^L = \nabla_aC \odot \sigma^{‘}(z^L)$$</p>
</li>
<li><p>使用下一层的误差\(\delta^{l+1}\)来表示当前层的误差\(\delta^l\)：<br>$$\delta_j^l = \sum_k{\omega_{kj}^{l+1}\delta_k^{l+1}\sigma^{‘}(z_j^l)}$$<br>矩阵形式：<br>$$\delta^l = ((\omega^{l+1})^T\delta^{l+1}) \odot \sigma^{‘}(z^l)$$</p>
</li>
<li><p>损失函数对偏置的导数：<br>$$\frac{\partial C}{\partial b_j^l} = \delta_j^l$$</p>
</li>
<li><p>损失函数对权重的导数：<br>$$\frac{\partial C}{\partial \omega_{jk}^l} = a_k^{l-1}\sigma_j^l$$</p>
</li>
</ol>
<h2 id="伪代码如下："><a href="#伪代码如下：" class="headerlink" title="伪代码如下："></a>伪代码如下：</h2><ol>
<li><p>计算并记录前向传播过程中<em>带权和</em>和<em>激活值</em>;</p>
</li>
<li><p>利用公式计算代价函数对参数梯度</p>
</li>
</ol>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2017/03/10/machine learning/NN/Neural Networks for Applied Sciences and Engineering--Chapter 8/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2017/03/10/machine learning/NN/Neural Networks for Applied Sciences and Engineering--Chapter 8/" class="post-title-link" itemprop="url">Neural Networks for Applied Sciences and Engineering--Chapter 8</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2017-03-10 18:00:24" itemprop="dateCreated datePublished" datetime="2017-03-10T18:00:24+08:00">2017-03-10</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/NN/" itemprop="url" rel="index"><span itemprop="name">NN</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <h1 id="Chapter-8-Discovering-Unknown-Clusters-in-Data-with-Self-Organizing-Maps"><a href="#Chapter-8-Discovering-Unknown-Clusters-in-Data-with-Self-Organizing-Maps" class="headerlink" title="Chapter 8 Discovering Unknown Clusters in Data with Self-Organizing Maps"></a>Chapter 8 Discovering Unknown Clusters in Data with Self-Organizing Maps</h1><script type="text/javascript" src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=default"></script>

<h2 id="8-1-Introduction-and-Overview"><a href="#8-1-Introduction-and-Overview" class="headerlink" title="8.1 Introduction and Overview"></a>8.1 Introduction and Overview</h2><h2 id="8-2-Structure-of-Unsupervised-Networks"><a href="#8-2-Structure-of-Unsupervised-Networks" class="headerlink" title="8.2 Structure of Unsupervised Networks"></a>8.2 Structure of Unsupervised Networks</h2><h2 id="8-3-Learning-in-Unsupervised-Networks"><a href="#8-3-Learning-in-Unsupervised-Networks" class="headerlink" title="8.3 Learning in Unsupervised Networks"></a>8.3 Learning in Unsupervised Networks</h2><blockquote>
<p>Rosenblatt proposed a model of competitive learning between neurons.In his model that<br> attempts to mimic this brain function,neurons inhibit each other<br> by sending their activation as inhibitory signals,the goal being to<br> win a competition for the maximum activation corresponding to an input pattern.<br> <strong>The neuron with the maximum activation then represents the input pattern</strong> that<br> led to its activation. This neuron alone becomes the winner and is allowed to<br> <strong>adjust its weight vector by moving it closer to that input vector</strong>;however,the neurons that<br> lose the competition by succumbing to the inhibition are not allowed to change their weights.</p>
</blockquote>
<h2 id="8-4-Implementation-of-Competitive-Learning"><a href="#8-4-Implementation-of-Competitive-Learning" class="headerlink" title="8.4 Implementation of Competitive Learning"></a>8.4 Implementation of Competitive Learning</h2><blockquote>
<p>In many cases,the number of data clusters is unknown.When there is uncertainty,<br> it is better to <strong>have a larger number</strong> of output neurons than the possible number of clusters<br> <strong>because redundant neurons can be eliminated</strong>.<br> After the number of input variables and output neurons has been set,the next step is to<br> <strong>initialize the weights</strong>.These may be set to <strong>small random values</strong>,as was done in the MLP networks.<br> Another possibility is to <strong>randomly choose some input vectors and use their values for the weights</strong>.<br> This has the potential to speed up learning.</p>
</blockquote>
<h3 id="8-4-1-Winner-Selection-Based-on-Neuron-activation"><a href="#8-4-1-Winner-Selection-Based-on-Neuron-activation" class="headerlink" title="8.4.1 Winner Selection Based on Neuron activation"></a>8.4.1 Winner Selection Based on Neuron activation</h3><blockquote>
<p>Once each output neuron has computed its activation,competition can begin.There are several ways<br> this can happen;a simple way is for each neuron to <strong>send its signal</strong> in an inhibitory manner,<br> with <strong>an opposite sign to other neurons</strong>.Once each neuron has received signals from the others,<br> each neuron can compute its <strong>net activation</strong> by simply summing the incoming inhibitory signals and<br> its own activation.If the activation drops below a threshold(or zero),that neuron drops out of the competition.<br> As long as more than one neuron remians,the cycle of inhibition continues until one winner emerges;<br> its output is set to one.This neuron is declared the winner because it has the highest activation<br> and it alone represents the input vector.</p>
</blockquote>
<p><span style="color:blue"><em>Is the opposite sign only the sign,not the opposite activation?</em></span></p>
<h3 id="8-4-2-Winner-Selection-Based-on-Distance-to-Input-vector"><a href="#8-4-2-Winner-Selection-Based-on-Distance-to-Input-vector" class="headerlink" title="8.4.2 Winner Selection Based on Distance to Input vector"></a>8.4.2 Winner Selection Based on Distance to Input vector</h3><blockquote>
<p>Once the distance between an input vector and all the weights has been found,<br> the neuron with <strong>the smallest distance</strong> to the input vector is chosen as the winner,<br> and its weights are updated so that it <strong>moves closer to the input vector</strong>,as<br> \(\Delta\omega_j = \beta(x - \omega_j) = \beta{}d_j\).<br><img src="http://omdhuynsr.bkt.clouddn.com/17-3-6/13980399-file_1488786922883_a440.png" alt title="weight update"></p>
</blockquote>
<h4 id="8-4-2-1-Other-Distance-Measures"><a href="#8-4-2-1-Other-Distance-Measures" class="headerlink" title="8.4.2.1 Other Distance Measures"></a>8.4.2.1 Other Distance Measures</h4><h3 id="8-4-3-Competitive-Learning-Example"><a href="#8-4-3-Competitive-Learning-Example" class="headerlink" title="8.4.3 Competitive Learning Example"></a>8.4.3 Competitive Learning Example</h3><h4 id="8-4-3-1-Recursive-Versus-Batch-Learning"><a href="#8-4-3-1-Recursive-Versus-Batch-Learning" class="headerlink" title="8.4.3.1 Recursive Versus Batch Learning"></a>8.4.3.1 Recursive Versus Batch Learning</h4><blockquote>
<p>In the batch learning,the weight update for each input vector is noted,<br> but the weights are not changed until all the input patterns have been presented.<br> Training terminates when the mean distance between the winning neurons and<br> the inputs they repersent is at a minimum across the entire set of clusters,<br> or when this distance stops changing.</p>
</blockquote>
<h4 id="8-4-3-2-Illustration-of-the-Calculations-Involved-in-Winner-Selection"><a href="#8-4-3-2-Illustration-of-the-Calculations-Involved-in-Winner-Selection" class="headerlink" title="8.4.3.2 Illustration of the Calculations Involved in Winner Selection"></a>8.4.3.2 Illustration of the Calculations Involved in Winner Selection</h4><blockquote>
<p>The training criterion is the mean distance(the sum of the squared distance)<br> between all the inputs and their respective winning neuron weights which<br> represent the cluster centers.<br> The objective of training is to <strong>minimize the mean distance</strong> over iterations.<br> The mean distance \(D\) can be expressed as<br> $$D = \sum_{i=0}^k \sum_{n\in C_i}(x^n - \omega_i)^2$$</p>
</blockquote>
<h4 id="8-4-3-3-Network-Training"><a href="#8-4-3-3-Network-Training" class="headerlink" title="8.4.3.3 Network Training"></a>8.4.3.3 Network Training</h4><h2 id="8-5-Self-Organizing-Feature-Maps"><a href="#8-5-Self-Organizing-Feature-Maps" class="headerlink" title="8.5 Self-Organizing Feature Maps"></a>8.5 Self-Organizing Feature Maps</h2><blockquote>
<p>In SOMs,not only the winner neuron but also neurons in <strong>the neighborhood</strong> of the winner<br> <strong>adjust</strong> their weights together so that a neighborhood of neurons becomes sensitive to a specific input.<br> This neighborhood feature helps to preserve <strong>topological characteristics of inputs</strong>.<br> Therefore,inputs that are spatially closer together must be represented in close proximity<br> in the output layer or map of a network.</p>
</blockquote>
<h3 id="8-5-1-Learning-in-Self-Organizing-Map-Networks"><a href="#8-5-1-Learning-in-Self-Organizing-Map-Networks" class="headerlink" title="8.5.1 Learning in Self-Organizing Map Networks"></a>8.5.1 Learning in Self-Organizing Map Networks</h3><h4 id="8-5-1-1-Selection-of-Neighborhood-Geometry"><a href="#8-5-1-1-Selection-of-Neighborhood-Geometry" class="headerlink" title="8.5.1.1 Selection of Neighborhood Geometry"></a>8.5.1.1 Selection of Neighborhood Geometry</h4><blockquote>
<p>There are several ways to define a neighborhood.<br> <img src="http://omdhuynsr.bkt.clouddn.com/17-3-6/15425123-file_1488797380352_ba79.png" alt><br> If only the most immediate neighbors of the winer are considered,the distance,<br> also called <strong>radius r</strong>,is 1.If two levels of adjacent neighbors are considered,then the radius is 2.</p>
</blockquote>
<h4 id="8-5-1-2-Training-of-Self-Organizing-Maps"><a href="#8-5-1-2-Training-of-Self-Organizing-Maps" class="headerlink" title="8.5.1.2 Training of Self-Organizing Maps"></a>8.5.1.2 Training of Self-Organizing Maps</h4><blockquote>
<p>$$\omega_j^{‘} = \omega_j + \beta NS<em>[x - \omega_j]$$<br> where \(NS\) is the <em>*neighbor strength</em></em> that varies with the distance to a neighbor neuron from the winner.<br> Neighbor strengh defines the strength of weight adjustment of the neighbors with respect to that of the winner.</p>
</blockquote>
<h4 id="8-5-1-3-Neighbor-Strength"><a href="#8-5-1-3-Neighbor-Strength" class="headerlink" title="8.5.1.3 Neighbor Strength"></a>8.5.1.3 Neighbor Strength</h4><blockquote>
<p>The winning neuron update is the most pronounced and the farther away a neighbor neuron is,<br> the less its weight update.The \(NS\) function determines how the weight adjustment<br> <strong>decays</strong> with distance from the winner.There are several possibilities for this function and<br> some commonly usedd functions are <strong>linear,Gaussian,and exponential</strong>.<br> The Gaussian form of the \(NS\) function makes the weight adjustments decay smoothly with distance,<br> and is given by \(NS = Exp[\frac{-d_{i,j}^2}{2\delta^2}]\)<br> The exponential decay \(NS\) function is given by \(NS = Exp[-kd_{i,j}]\)</p>
</blockquote>
<h4 id="8-5-1-4-Example-Training-Self-Organizing-Networks-with-a-Neighbor-Feature"><a href="#8-5-1-4-Example-Training-Self-Organizing-Networks-with-a-Neighbor-Feature" class="headerlink" title="8.5.1.4 Example:Training Self-Organizing Networks with a Neighbor Feature"></a>8.5.1.4 Example:Training Self-Organizing Networks with a Neighbor Feature</h4><h4 id="8-5-1-5-Neighbor-Matrix-and-Distance-to-Neighbors-from-the-Winner"><a href="#8-5-1-5-Neighbor-Matrix-and-Distance-to-Neighbors-from-the-Winner" class="headerlink" title="8.5.1.5 Neighbor Matrix and Distance to Neighbors from the Winner"></a>8.5.1.5 Neighbor Matrix and Distance to Neighbors from the Winner</h4><blockquote>
<p>When the map is large,an efficient method is required to determine the distance of a neighbor<br> from the winner to compute neighor strength.<br> Use a neighbor matrix(\(NM\),also called distance matrix) for a two-dimensional map as a example.<br> For a map of 12 neurons arranged in three rows and four columns,the neighbor matrix for a<br> rectangular neighborhood is<br> $$NM = \begin{bmatrix}<br> 3 &amp; 2 &amp; 2 &amp; 2 &amp; 2 &amp; 2 &amp; 3 \\\\<br> 3 &amp; 2 &amp; 1 &amp; 1 &amp; 1 &amp; 2 &amp; 3 \\\\<br> 3 &amp; 2 &amp; 1 &amp; 0 &amp; 1 &amp; 2 &amp; 3 \\\\<br> 3 &amp; 2 &amp; 1 &amp; 1 &amp; 1 &amp; 2 &amp; 3 \\\\<br> 3 &amp; 2 &amp; 2 &amp; 2 &amp; 2 &amp; 2 &amp; 3<br> \end{bmatrix}_{5\times7}$$<br> Suppose that the horizontal and vertical coordinates of the winner neuron on the two-dimensional map<br> are indicated by (\(i_{win},j_{win})\).Then the distance between the winner and<br> any neighbor neuron at position \((i,j)\) is<br> $$d = NM[\begin{bmatrix} c_1-i_{win}+i,c_2-j_{win}+j \end{bmatrix}]$$<br> where \({c_1,c_2}\) is the position of the winner in the neighbor matrix \(NM\).For this case,<br> \(c_1 = 3\) and \(c_2 = 4\)</p>
</blockquote>
<h4 id="8-5-1-6-Shrinking-Neighborhood-Size-with-iterations"><a href="#8-5-1-6-Shrinking-Neighborhood-Size-with-iterations" class="headerlink" title="8.5.1.6 Shrinking Neighborhood Size with iterations"></a>8.5.1.6 Shrinking Neighborhood Size with iterations</h4><blockquote>
<p><strong>A larger initial neighborhood is necessay because smaller initial neighborhoods can lead to<br> metastable states corresponding to local minima</strong>.However,subsequent <strong>shrinking</strong> of neighborhood<br> is required to further <strong>refine</strong> the representation of the input probability distribution by the map.<br> The equation below shows a linear function commnonly used for this purpose:$$\delta_t = \delta_0(1-t/T)$$<br> Exponential decay is another form used for adjusting neighborhood size with iterations,as given by<br> $$\delta_t = \delta_0Exp[-t/T]$$<br> And the decay in neighborhood size is integrated into the NS function as<br> $$NS(d,t) = Exp[-d_{i,j}^2/2\delta_t^2] = Exp[-d_{i,j}^2/2\\{\delta_0Exp(-t/T)\\}^2]$$<br> where \(T\) is a constant that allows the decay function to decay to zero with iterations.<br> A recommendation is  that the neighborhood size should initially cover almost all neurons in the network<br> when centered on a winning neuron and then shrink slowly with iterations.</p>
</blockquote>
<h4 id="8-5-1-7-Learning-Rate-Decay"><a href="#8-5-1-7-Learning-Rate-Decay" class="headerlink" title="8.5.1.7 Learning Rate Decay"></a>8.5.1.7 Learning Rate Decay</h4><blockquote>
<p>The step length,or the learning rate \(\beta\),is also reduced with iterations in<br> self-organizing learning and a common form of this function is the linear decay,given by<br> $$\beta_t = \beta_0(1-t/T)$$<br> Another form is the exponential decay of the learning rate given by<br> $$\beta_t = \beta_0Exp[-t/T]$$<br> where \(T\) is a time constant that brings the learning rate to a very small value with iterations.<br> A general guide is to start with a relatively high learning rate and let it decrease gradually but<br> remain above 0.01.</p>
</blockquote>
<h4 id="8-5-1-8-Weight-Update-Incorporating-Learning-Rate-and-Neighborhood-Decay"><a href="#8-5-1-8-Weight-Update-Incorporating-Learning-Rate-and-Neighborhood-Decay" class="headerlink" title="8.5.1.8 Weight Update Incorporating Learning Rate and Neighborhood Decay"></a>8.5.1.8 Weight Update Incorporating Learning Rate and Neighborhood Decay</h4><blockquote>
<p>Thus,the weight update after presenting an input vector \(\mathbf{x}\) to a SOM incorporating both<br> neighborhood size and learning rate that decrease with the number of iterations can be expressed as<br> $$\omega_j(t) = \omega_j(t-1) + \beta(t)NS(d,t)[\mathbf{x}(t)-\omega_j(t-1)]$$</p>
</blockquote>
<h4 id="8-5-1-9-Recursive-and-Batch-Training-and-Relation-to-K-Means-Clustering"><a href="#8-5-1-9-Recursive-and-Batch-Training-and-Relation-to-K-Means-Clustering" class="headerlink" title="8.5.1.9 Recursive and Batch Training and Relation to K-Means Clustering"></a>8.5.1.9 Recursive and Batch Training and Relation to K-Means Clustering</h4><blockquote>
<p>In batch mode,the unsupervised algorithm without neighbor feature becomes <strong>equivalent to</strong> K-means clustering.<br> When the neighbor feature is incorporated,it allows <strong>nonlinear projection</strong> of the data as well as<br> the very attractive feature of <strong>topology preservation</strong>,by which regions closer in input space are<br> represented by neurons that are closer in the map.<strong>For this reason it is called a feature map.</strong></p>
</blockquote>
<h4 id="8-5-1-10-Two-Phases-of-Self-Organizing-Map-Training"><a href="#8-5-1-10-Two-Phases-of-Self-Organizing-Map-Training" class="headerlink" title="8.5.1.10 Two Phases of Self-Organizing Map Training"></a>8.5.1.10 Two Phases of Self-Organizing Map Training</h4><blockquote>
<p>Training is usually performed in two phases:ordering and convergence.<br> In the ordering phase,learning rate and neighborhood size are reduces with iterations until<br> the winner or a few neighbors around the winner remain.<br> In the convergence phase,the feature map is fine tuned with the shrunk neighborhood so that<br> it produces an accurate representation of the input space.<br> In this phase,<strong>learning rate is maintained at a small value</strong>,on the order of 0.01,<br> to achieve convergence with good statistical accuracy.Haykin states that the learning rate<br> must not become zero because the network can get stuck in a metastable state that<br> corresponds to a feature map configuration with a topological defect.The \(NS\) function should<br> <strong>contain only the nearest neighbors</strong> of the winning neuron and may <strong>slowly reduce to one or zero neighbors</strong>(i.e.,only the winner remains).</p>
</blockquote>
<h4 id="8-5-1-11-Example-Illustrating-Self-Organizing-Map-Learning-with-a-Hand-Calculations"><a href="#8-5-1-11-Example-Illustrating-Self-Organizing-Map-Learning-with-a-Hand-Calculations" class="headerlink" title="8.5.1.11 Example:Illustrating Self-Organizing Map Learning with a Hand Calculations"></a>8.5.1.11 Example:Illustrating Self-Organizing Map Learning with a Hand Calculations</h4><h4 id="8-5-1-12-SOM-Case-Study-Determination-of-Mastitis-Health-Status-of-Dairy-Herd-from-Combined-Milk-Traits"><a href="#8-5-1-12-SOM-Case-Study-Determination-of-Mastitis-Health-Status-of-Dairy-Herd-from-Combined-Milk-Traits" class="headerlink" title="8.5.1.12 SOM Case Study:Determination of Mastitis Health Status of Dairy Herd from Combined Milk Traits"></a>8.5.1.12 SOM Case Study:Determination of Mastitis Health Status of Dairy Herd from Combined Milk Traits</h4><h3 id="8-5-2-Example-of-Two-Dimensional-Self-Organizing-Maps-Clustering-Canadian-and-Alaskan-Salmon-Based-on-the-Diameter-of-Growth-Rings-of-the-Scales"><a href="#8-5-2-Example-of-Two-Dimensional-Self-Organizing-Maps-Clustering-Canadian-and-Alaskan-Salmon-Based-on-the-Diameter-of-Growth-Rings-of-the-Scales" class="headerlink" title="8.5.2 Example of Two-Dimensional Self-Organizing Maps:Clustering Canadian and Alaskan Salmon Based on the Diameter of Growth Rings of the Scales"></a>8.5.2 Example of Two-Dimensional Self-Organizing Maps:Clustering Canadian and Alaskan Salmon Based on the Diameter of Growth Rings of the Scales</h3><h4 id="8-5-2-1-Map-Structure-and-Initialization"><a href="#8-5-2-1-Map-Structure-and-Initialization" class="headerlink" title="8.5.2.1 Map Structure and Initialization"></a>8.5.2.1 Map Structure and Initialization</h4><h4 id="8-5-2-2-Map-Training"><a href="#8-5-2-2-Map-Training" class="headerlink" title="8.5.2.2 Map Training"></a>8.5.2.2 Map Training</h4><blockquote>
<p>For example,the map was trained using a square neighborhood with learning rate \(\beta\) expressed as<br> $$\beta = \left\\{\begin{array}{ll}<br>    0.01&amp;{t &lt; 5} \\\\<br>    \frac{2}{3+t}&amp;{t &gt; 5}\end{array}\right.$$<br> Learning rate is a <strong>samll constant value</strong> in the first four iterations so that the codebook vectors<br> <strong>find a good orientation</strong>(this is not always done).<br> The neighbor strength function used was<br> $$NS = \left\\{\begin{array}{ll}<br>        Exp[-0.1d]&amp;if \quad t &lt; 5 \\\\<br>        Exp[-\frac{(t-4)}{10}d]&amp;otherwise\end{array}\right.$$<br> During the first four iterations,<strong>all neurons on the map are neighbors</strong> of a winning neuron and<br> <strong>all neighbors are <em>strongly</em> influenced</strong>.The stronger influence on the neighbors in the initial iterations<br> makes the network conform to a nice structure and avoids knots.<br> <strong><em>Ordering phase</em></strong>.The map was trained using <strong>recursive update</strong>.<br> <strong><em>Convergence phase</em></strong>.To finetune and make sure that the map has converged,the trained map was trained further in <strong>batch mode</strong>.<br> Because the network in this case appears to have approached <strong>convergence</strong>,the learning rate<br> has been <strong>set to 1.0</strong> because there will be only small or straightforward adjustments to<br> the position of the codebook vectors with further training.The neighbor strength is <strong>limited to<br> the winning neuron</strong>.If,however,the network has <strong>not approached convergence</strong> in the ordering phase,<br> further training with a smaller constant learning rate on <strong>the order of 0.01</strong> may be appropriate.<br> The neighbor strength then may be <strong>limited to a few neighbors</strong> and decrease to the winner<br> or the nearest neighbors towards the end of training.<br> The final map has reached more data in the outlying regions compared to the map formed<br> at the end of the ordering phase,is expressed as below figure:<br> <img src="http://omdhuynsr.bkt.clouddn.com/17-3-7/10981036-file_1488895472789_17ff2.png" alt><br> <strong>We can set larger number of codebook vectors than the number of classes.</strong><br> So,a cluster of codebook vectors,not a single vector,defines each class.<strong>This gives the map its<br> ability to form nonlinear cluster boundaries.</strong>This cluster structure can be used to<br> discover unknown clusters in data.The map can also be used for subsequent supervised classification.<br> For example,when class labels are known,the codebook vectors that represent corresponding<br> input vectors can <strong>be used as input</strong> to train a feedforward classification network to obtain<br> the calss to which a particular unknown input vector belongs.This is called <strong>learning vector quantization</strong>.<br> Each codebook vector represents the center of gravity of a cluster of inputs that it represents and<br> therefore approximates <strong>the average or point density</strong> of the original distribution in a small cluster region.<br> <strong>Therefore,the magnitude(length) of the codebook vectors should reflect this.</strong> A properly ordered map<br> should show evenly varying length of the codebook vectors on the map.</p>
</blockquote>
<p><span style="color:red">It is a good example of model designment and parameters adjustment.</span><br><span style="color:blue">why the magnitude could relect the point density of the original distribution?</span></p>
<h4 id="8-5-2-3-U-Matrix"><a href="#8-5-2-3-U-Matrix" class="headerlink" title="8.5.2.3 U-Matrix"></a>8.5.2.3 U-Matrix</h4><blockquote>
<p>The distance between the neighboring codebook vectors can highligh different cluster regions<br> in the map and can be a useful visualization tool.The average of the distance to the nearest neighbors<br> is called unified distance,and the matrix of these values for all neurons is called the U-matrix.<br> Thus the map has not only orientated itself in the principal directions of the data,but has also<br> learned to represent the density distribution of the input data,like the figure below:<br> <img src="http://omdhuynsr.bkt.clouddn.com/17-3-8/37014593-file_1488941605615_b781.png" alt></p>
</blockquote>
<p><span style="color:blue">I can understand the figure,but I can’t understand why the distance is average<br>and how to plot the figure.Does the average of the distance mean the average of all input in related cluster regions?</span></p>
<h3 id="8-5-3-Map-Initialization"><a href="#8-5-3-Map-Initialization" class="headerlink" title="8.5.3 Map Initialization"></a>8.5.3 Map Initialization</h3><blockquote>
<p><strong>Random initialization</strong> clusters the initial vectors near the center of gravity of inputs and assigns<br> random values in this cener region.<br> <strong>Deterministic initialization</strong> is another approach,where some input vectors from the dataset<br> are used as initial vectors.This can accelerate map training.<br> Yet another approach is to train a map with random initialization for a few iterations and<br> use the resulting vectors as initial vectors(<strong>random-derministic</strong>).<br> Another possible approach to initialization is to find the first two<br> <strong>principal directions</strong> of data using principal component analysis and<br> use these two directions for map directions.</p>
</blockquote>
<h3 id="8-5-4-Example-Training-Two-Demensional-Maps-on-Multidimensional-Data"><a href="#8-5-4-Example-Training-Two-Demensional-Maps-on-Multidimensional-Data" class="headerlink" title="8.5.4 Example:Training Two-Demensional Maps on Multidimensional Data"></a>8.5.4 Example:Training Two-Demensional Maps on Multidimensional Data</h3><blockquote>
<p>The SOMs can be used not only to cluster input data,but also to <strong>explore the relationship<br> between different attributes of input data.</strong></p>
</blockquote>
<h4 id="8-5-4-1-Data-Visualization"><a href="#8-5-4-1-Data-Visualization" class="headerlink" title="8.5.4.1 Data Visualization"></a>8.5.4.1 Data Visualization</h4><blockquote>
<p>It is a good example for EDA with iris datasets:<br> <img src="http://omdhuynsr.bkt.clouddn.com/17-3-8/91990443-file_1488956403117_5fd3.png" alt><br> Where use color distinct different clusters.</p>
</blockquote>
<p><span style="color:red">It is a good example for EDA.</span></p>
<h4 id="8-5-4-2-Map-Structure-and-Training"><a href="#8-5-4-2-Map-Structure-and-Training" class="headerlink" title="8.5.4.2 Map Structure and Training"></a>8.5.4.2 Map Structure and Training</h4><blockquote>
<p><img src="http://omdhuynsr.bkt.clouddn.com/17-3-8/3745649-file_1488961989125_16d9f.png" alt><br> <img src="http://omdhuynsr.bkt.clouddn.com/17-3-8/88751501-file_1488962053638_139d5.png" alt><br> <img src="http://omdhuynsr.bkt.clouddn.com/17-3-8/50439332-file_1488962099950_d395.png" alt></p>
</blockquote>
<h4 id="8-5-4-3-U-matrix"><a href="#8-5-4-3-U-matrix" class="headerlink" title="8.5.4.3 U-matrix"></a>8.5.4.3 U-matrix</h4><p><span style="color:blue">8.5.4 should be inspection again.It provides many tricks of EDA.</span></p>
<h4 id="8-5-4-4-Point-Estimates-of-Probability-Density-of-Inputs-Caotured-by-the-Map"><a href="#8-5-4-4-Point-Estimates-of-Probability-Density-of-Inputs-Caotured-by-the-Map" class="headerlink" title="8.5.4.4 Point Estimates of Probability Density of Inputs Caotured by the Map"></a>8.5.4.4 Point Estimates of Probability Density of Inputs Caotured by the Map</h4><blockquote>
<p>From the trained map,we can also determine the number of input vectors represented by each neuron.<br> Each neuron represents the local probability density of inputs.In the below figure,the lighter the color,<br> the larger the number of inputs falling onto thar neuron.<br> <img src="http://omdhuynsr.bkt.clouddn.com/17-3-8/29639643-file_1488963168903_8ffa.png" alt></p>
</blockquote>
<h4 id="8-5-4-5-Quantization-Error"><a href="#8-5-4-5-Quantization-Error" class="headerlink" title="8.5.4.5 Quantization Error"></a>8.5.4.5 Quantization Error</h4><blockquote>
<p>Quantization error is a measure of the distance between codebook vectors and inputs.<br> If for an input vector \(\mathbf{x}\),the winner’s weights vector is \(\pmb{\omega}_c\),<br> then the quantization error can be described as a distortion error,\(e\),expressed as<br> $$e = d(\mathbf{x},\pmb{\omega}_c)$$</p>
</blockquote>
<blockquote>
<p>which is the distance from the input to the closet codebook vector.<br> It may be more appropriate to define the distortion error in terms of neighborhood function<br> because the neighbor featuer is central to SOM.With the neighbor feature,<br> the distortion error of the map for an input vector \(\mathbf{x}\) becomes<br> $$e = \sum_{i}NS_{ci}d(\mathbf{x},\pmb{\omega_i})$$<br> where \(NS_{ci}\) is the neighbor strength,\(c\) is the index of the winning neuron closest to input vector \(\mathbf{x}\),<br> and \(i\) is any neuron in the neighborhood of the winner,including the winner.<br> Computing the distortion measure for all input vectors in the input space,the average distortion error \(E\)<br> for the map can be calculated from<br> $$E = \frac{1}{N}\sum_n\sum_iNS_{ci}d(\mathbf{x}^n,\pmb{\omega}_i)$$<br> When the neighbor feature is not used,equation above simplifies to<br> $$E = \frac{1}{N}\sum_nd(\mathbf{x}^n,\pmb{\omega}_i)$$<br> Thus the goal of SOM can alternatively be expressed as finding the set of codebook vectors \(\pmb{\omega}_i\)<br> that <strong>globally minimizes the average map distortion error \(E\)</strong>.<br> The information from figure below can be used to refine the map to obtain a more uniform distortion error measure<br> if a more faithful reproduction of the input distribution from the map is desired.<br> <img src="http://omdhuynsr.bkt.clouddn.com/17-3-8/32153453-file_1488971907071_18164.png" alt></p>
</blockquote>
<p><span style="color:blue">The format of Latex here has some problems,so I split the words to two parts.How could fix it?</span></p>
<h4 id="8-5-4-6-Accuracy-of-Retrieval-of-Input-Data-from-the-Map"><a href="#8-5-4-6-Accuracy-of-Retrieval-of-Input-Data-from-the-Map" class="headerlink" title="8.5.4.6 Accuracy of Retrieval of Input Data from the Map"></a>8.5.4.6 Accuracy of Retrieval of Input Data from the Map</h4><blockquote>
<p>If the dataset is sent through the map,it identifies the best matching codebook vector.The resulting codebook vector<br> can be thought of as the retrieved input because it is the closest to that input.<br> If a neighborhood of neurons is used in the retrieval,more than one codebook vector can be activated and<br> these codebook vectors can be interpolated to obtain a recalled match of the input to the map.Then the retrieved inputs<br> are not the codebook vectors,but <strong>fall between them due to <em>interpolation</em></strong>.<br> The retrieval error is the average distance between the actual data vectors and their corresponding interpolated<br> codebook vectors defining the best position for those input vectors in the trained map.Thus,a neighborhood provides<br> a <strong>better approximation</strong> to this input distribution than a single codebook vector.</p>
</blockquote>
<h3 id="8-5-5-Forming-Clusters-on-the-Map"><a href="#8-5-5-Forming-Clusters-on-the-Map" class="headerlink" title="8.5.5 Forming Clusters on the Map"></a>8.5.5 Forming Clusters on the Map</h3><h4 id="8-5-5-1-Approaches-to-Clustering"><a href="#8-5-5-1-Approaches-to-Clustering" class="headerlink" title="8.5.5.1 Approaches to Clustering"></a>8.5.5.1 Approaches to Clustering</h4><h4 id="8-5-5-2-Example-Illustrating-Clustering-on-a-Trained-Map"><a href="#8-5-5-2-Example-Illustrating-Clustering-on-a-Trained-Map" class="headerlink" title="8.5.5.2 Example Illustrating Clustering on a Trained Map"></a>8.5.5.2 Example Illustrating Clustering on a Trained Map</h4><h3 id="8-5-6-Validation-of-a-Trained-Map"><a href="#8-5-6-Validation-of-a-Trained-Map" class="headerlink" title="8.5.6 Validation of a Trained Map"></a>8.5.6 Validation of a Trained Map</h3><h4 id="8-5-6-1-n-Fold-Cross-Validation"><a href="#8-5-6-1-n-Fold-Cross-Validation" class="headerlink" title="8.5.6.1 n-Fold Cross Validation"></a>8.5.6.1 n-Fold Cross Validation</h4><h2 id="8-6-Evolving-Self-Organizing-Maps"><a href="#8-6-Evolving-Self-Organizing-Maps" class="headerlink" title="8.6 Evolving Self-Organizing Maps"></a>8.6 Evolving Self-Organizing Maps</h2>
          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
      

  

  
  
  

  

  <article class="post post-type-normal" itemscope itemtype="http://schema.org/Article">
  
  
  
  <div class="post-block">
    <link itemprop="mainEntityOfPage" href="http://www.muzhen.tk/2014/11/12/pansee/essay/我当做选择/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="name" content="muzhen">
      <meta itemprop="description" content>
      <meta itemprop="image" content="/images/avatar.gif">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="the Home of MuZhen">
    </span>

    
      <header class="post-header">

        
        
          <h1 class="post-title" itemprop="name headline">
                
                
                <a href="/2014/11/12/pansee/essay/我当做选择/" class="post-title-link" itemprop="url">我当做选择</a>
              
            
          </h1>
        

        <div class="post-meta">
          <span class="post-time">

            
            
            

            
              <span class="post-meta-item-icon">
                <i class="fa fa-calendar-o"></i>
              </span>
              
                <span class="post-meta-item-text">Posted on</span>
              

              
                
              

              <time title="Created: 2014-11-12 10:48:23" itemprop="dateCreated datePublished" datetime="2014-11-12T10:48:23+08:00">2014-11-12</time>
            

            
              

              
                
                <span class="post-meta-divider">|</span>
                

                <span class="post-meta-item-icon">
                  <i class="fa fa-calendar-check-o"></i>
                </span>
                
                  <span class="post-meta-item-text">Edited on</span>
                
                <time title="Modified: 2019-02-28 20:45:06" itemprop="dateModified" datetime="2019-02-28T20:45:06+08:00">2019-02-28</time>
              
            
          </span>

          
            <span class="post-category">
            
              <span class="post-meta-divider">|</span>
            
              <span class="post-meta-item-icon">
                <i class="fa fa-folder-o"></i>
              </span>
              
                <span class="post-meta-item-text">In</span>
              
              
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing"><a href="/categories/essay/" itemprop="url" rel="index"><span itemprop="name">essay</span></a></span>

                
                
              
            </span>
          

          
            
            
          

          
          

          

          

          

        </div>
      </header>
    

    
    
    
    <div class="post-body" itemprop="articleBody">

      
      

      
        
          
            <p>我当做选择</p>
<p>哪怕不选择也是一种选择，选择的是相信自我。故而，现阶段我所认可的选择乃是基督教</p>
<p>我当奉行这一信仰所当行之事</p>
<p>选择并不是口头空话，应当奉行所当行之事。至于这样奉行是否会导致如吸毒一样而影响清醒的判断这一疑虑不过是“相信自我”这一被弃选择的余孽作祟</p>
<p>我不能承诺一辈子的信仰</p>
<p>未来并不由我掌控（这实际上是相信人心的可变与有限）。故而我并无必要根本上是没有能力做出这样的承诺</p>
<p>不管真理如何，不管谬误是否终将显明，我并不知我能否掌控。唯一能做的便是选择最合宜的选择。</p>
<p>是否可以承负迅速的信仰变换所带来的非议？理性上讲这一问题无需考虑，但实际生活上却有考虑之必要。</p>

          
        
      
    </div>

    

    
    
    

    

    
      
    
    

    

    <footer class="post-footer">
      

      

      

      
      
        <div class="post-eof"></div>
      
    </footer>
  </div>
  
  
  
  </article>


    
  </section>

  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/page/14/"><i class="fa fa-angle-left" aria-label="Previous page"></i></a><a class="page-number" href="/">1</a><span class="space">&hellip;</span><a class="page-number" href="/page/14/">14</a><span class="page-number current">15</span><a class="page-number" href="/page/16/">16</a><span class="space">&hellip;</span><a class="page-number" href="/page/18/">18</a><a class="extend next" rel="next" href="/page/16/"><i class="fa fa-angle-right" aria-label="Next page"></i></a>
  </nav>



          </div>
          

        </div>
        
          
  
  <div class="sidebar-toggle">
    <div class="sidebar-toggle-line-wrap">
      <span class="sidebar-toggle-line sidebar-toggle-line-first"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-middle"></span>
      <span class="sidebar-toggle-line sidebar-toggle-line-last"></span>
    </div>
  </div>

  <aside id="sidebar" class="sidebar">
    <div class="sidebar-inner">

      

      

      <div class="site-overview-wrap sidebar-panel sidebar-panel-active">
        <div class="site-overview">
          <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
            
              <p class="site-author-name" itemprop="name">muzhen</p>
              <div class="site-description motion-element" itemprop="description"></div>
          </div>

          
            <nav class="site-state motion-element">
              
                <div class="site-state-item site-state-posts">
                
                  <a href="/archives/">
                
                    <span class="site-state-item-count">358</span>
                    <span class="site-state-item-name">posts</span>
                  </a>
                </div>
              

              
                
                
                <div class="site-state-item site-state-categories">
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">37</span>
                    <span class="site-state-item-name">categories</span>
                  
                </div>
              

              
                
                
                <div class="site-state-item site-state-tags">
                  
                    
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                      
                    
                    <span class="site-state-item-count">113</span>
                    <span class="site-state-item-name">tags</span>
                  
                </div>
              
            </nav>
          

          

          

          

          
          

          
            
          
          

        </div>
      </div>

      

      

    </div>
  </aside>
  


        
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="footer-inner">
        <div class="copyright">&copy; <span itemprop="copyrightYear">2019</span>
  <span class="with-love" id="animate">
    <i class="fa fa-user"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">muzhen</span>

  

  
</div>


  <div class="powered-by">Powered by <a href="https://hexo.io" class="theme-link" rel="noopener" target="_blank">Hexo</a> v3.8.0</div>



  <span class="post-meta-divider">|</span>



  <div class="theme-info">Theme – <a href="https://theme-next.org" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> v7.0.1</div>




        








        
      </div>
    </footer>

    
      <div class="back-to-top">
        <i class="fa fa-arrow-up"></i>
        
      </div>
    

    

    

    
  </div>

  

<script>
  if (Object.prototype.toString.call(window.Promise) !== '[object Function]') {
    window.Promise = null;
  }
</script>


























  
  <script src="/lib/jquery/index.js?v=2.1.3"></script>

  
  <script src="/lib/velocity/velocity.min.js?v=1.2.1"></script>

  
  <script src="/lib/velocity/velocity.ui.min.js?v=1.2.1"></script>


  


  <script src="/js/src/utils.js?v=7.0.1"></script>

  <script src="/js/src/motion.js?v=7.0.1"></script>



  
  


  <script src="/js/src/schemes/muse.js?v=7.0.1"></script>



  

  


  <script src="/js/src/next-boot.js?v=7.0.1"></script>


  
  



  




  

  

  
  

  
  
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
      
    
  

  


  

  

  

  

  

  

  

  

  

  

</body>
</html>
